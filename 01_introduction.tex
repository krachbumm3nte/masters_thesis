
\chapter{Introduction}



\section{Motivation}

The outstanding learning capabilities of the human brain have been found to be elusive and as yet impossible to fully
explain or replicate in silicio. While in recent years the power of classical machine learning solutions  has improved
even beyond human capabilities for some tasks, their underlying algorithms cannot serve as a model of human cognition.
Some reasons why brains and machines appear irreconcilable relate to questions about network structure and neuron
models. Yet more pressingly, almost all the most powerful artificial neural networks are trained with the
Backpropagation of errors algorithm, which has long been considered to be impossible for neurons to implement. Hence,
Neuroscience has dismissed this algorithm in an almost dogmatic way for many years after its development, stating that
the brain must employ a different mechanism to learn.

Yet in recent years, there has been a resurgence of research by neuroscientists towards reconciling biological and
artificial neural networks in spite of these concerns. This led to a number of experimental results indicating that
brains might be capable of performing something very similar to Backpropagation after all. Furthermore, despite rigorous
efforts, no unifying alternative to this learning principle was found which performs well enough to account for the
brain's unmatched capabilities.

Hence, there now exists a vibrant community developing alternative ways to implement this algorithm - or some
approximation of it. These novel approaches are capable of replicating an increasing number of properties of biological
brains. Nevertheless, many issues remain unsolved, and a lot of neuronal features remain unaccounted for in brain models
that are capable of any kind of learning. It is this open problem, to which I want to dedicate my efforts in this
thesis. After reviewing the existing literature, I have selected a promising model of learning in cortical circuits.
This model uses multi-compartment neuron models and local plasticity rules to implement a variant of Backpropagation. In
this project, I will investigate and attempt to further improve its concordance with data on the human neocortex. I will
use the approach of computationally modelling the model while progressively adding biological features, attempting to
retain learning performance in the process.


\section{The Backpropagation of errors algorithm}

The Backpropagation of errors algorithm (\textit{Backprop}) \citep{Schmidhuber2014} is the workhorse of modern machine
learning and is able to outperform humans on a growing number of tasks \citep{LeCun2015}. Particularly for training deep
neural networks it has remained popular and largely unchanged since its initial development. Its learning potential
stems from its unique capability to attribute errors in the output of a network to activations of specific neurons and
connections within its hidden layers. This property also forms the basis of the algorithm's name; After an initial
forward pass to form a prediction about the nature of a given input, a separate backward pass propagates the arising
error through all layers in reverse order. During this second network traversal, local error gradients dictate to what
extent a given weight needs to be altered so that the next presentation of the same sample would elicit a lower error in
the output layer. It has been argued that through this mechanism, Backprop solves the \textit{credit assignment problem}
- i.e.\ the question to what degree a parameter contributes to an error signal - optimally \citep{Lillicrap2020}. With
this critical information in hand, computing parameter changes that decrease error becomes almost trivial. As biological
neural networks are likewise subject to the credit assignment problem, finding a general solution to it promises to be
invaluable to neuroscience. For a long time Backprop was believed to be unsuitable for networks of biological neurons
for several reasons.


\section{Concerns over biological plausibility}

While Backprop continues to prove exceptionally useful in conventional machine learning systems, it is viewed critically
by many neuroscientists. For one, it relies on a slow adaptation of synaptic weights, and therefore requires a large
amount of examples to learn rather simple input-output mappings. In this particular way, its performance is far inferior
to the powerful one-shot learning exhibited by humans \citep{Brea2016}. Yet more importantly, no plausible mechanisms
have yet been found by which biological neural networks could implement the algorithm. In fact, Backprop as a way by
which brains may learn has been dismissed entirely by much of the neuroscience community for decades
\citep{Grossberg1987,Crick1989,Mazzoni1991,OReilly1996}. This dismissal is often focussed on three mechanisms that are
instrumental for the algorithm \citep{whittington2019theories,Bengio2015,Liao2016}:



\subsection{Local error representation}

Neuron-specific errors in Backprop are computed and propagated by a mechanism that is completely detached from the
network itself, which requires access to the entirety of the network state. In order to compute the weight changes for a
given layer, the algorithm takes as an input the activation and synaptic weights of all downstream neurons. In contrast,
plasticity in biological neurons is largely considered to be primarily dependent on factors that are local to the
synapse \citep{Abbott2000,magee2020synaptic,urbanczik2014learning}. While neuromodulators are known to influence
synaptic plasticity, their dispersion is too wide to communicate neuron-specific errors. Thus, biologically plausible
Backprop would require a method for encoding errors locally, i.e.\ close to the neurons to which they relate. This has
been perhaps the strongest criticism of Backprop in the brain, as many questions regarding mechanisms for both computing
and storing these errors remain unanswered as yet.

\subsection{The weight transport problem}

During the weight update stage of Backprop, errors are transmitted between layers with the same weights that are used in
the forward pass. In other words, the magnitude of a neuron-specific error that is back-propagated through a given
connection should be proportional to its impact on output loss during the forward pass. To replicate this, a neuronal
network implementing Backprop would require feedback connections that mirror both the precise connectivity and synaptic
weights of the forward connections. Bidirectional connections that could theoretically back-propagate errors are common
in the cortex, yet it is unclear by which mechanism pairs of synapses would be able to align. This issue becomes
particularly apparent when considering long-range pyramidal projections. In these, the feedforward and feedback synapses
which need to be aligned would potentially be separated by a considerable distance.

\subsection{Neuron models}

Finally, the types of artificial neurons typically used in Backprop transmit a continuous scalar activation at all
times, instead of discrete spikes. In theory, these activations correspond to the firing rate of a spiking neuron,
giving this class of models the title \textit{rate neurons}. Yet handling spike based communication requires more
sophisticated neuron models than are typically employed in Backprop networks. Additionally, plasticity rules for rate
neurons do not necessarily have an easily derived counterpart for spiking neurons. A notable example for this issue is
Backprop itself; The local error gradient of a neuron is not trivial to compute for spiking neural networks (SNN), as a
spiketrain has no natural derivative. Furthermore, a given neuron's activation in classical Backprop is computed from a
simple weighted sum of all inputs. This fails to capture the complex nonlinearities of dendritic integration that are
fundamental to cortical neurons (cf. Section \ref{sec-dendrites}). Finally, these abstract neurons - at least in
classical Backprop - have no persistence through time. Thus, their activation is dictated strictly by instantaneous
presynaptic activity, in contrast to the leaky membrane dynamics exhibited by biological neurons.


\section{Overcoming biological implausibility}

Backprop has remained the gold standard against which most attempts at modelling learning in the brain eventually are
compared. Also, despite its apparent biological implausibility, it does share some notable parallels to learning in the
brain. Artificial neural networks (ANN) trained with Backprop have been shown to develop similar representations to
those found in brain areas responsible for comparable tasks
\citep{Yamins2016,Whittington2018,KhalighRazavi2014,Kubilius2016}. Thus, numerous attempts have been made to define more
biologically plausible learning rules which approximate Backprop to some degree. A full review of the available
literature would be out of scope for this thesis, so only a few examples will be discussed in this section. \newline


\noindent One approach to solve the issues around local error representations is, to drive synaptic plasticity through a
global error signal \citep{potjans2011imperfect,mozafari2018combining,sutton2018reinforcement}. The appeal of this
solution is that such signalling could be plausibly performed by neuromodulators like dopamine
\citep{Mazzoni1991,Seung2003,izhikevich2007solving}. These types solutions to not approximate Backprop, but instead lead
to a kind of reinforcement learning. While some consider this the most plausible way for brains to learn
\citep{sutton2018reinforcement}, performance of global error/reward signalling stays far behind that of the credit
assignment performed by Backprop. Additionally, this class of algorithms requires even more examples of a training
dataset, and was shown to scale poorly with network size \citep{Werfel2003}. 

Two prominent classes of Backprop approximations have been developed, which are capable of locally representing errors.
These algorithms encode errors in either activation changes over time or local membrane potentials. They will be
discussed further in Section \ref{sec-model-selection}.\newline 


\noindent The weight transport problem was successfully addressed by a mechanism called \textit{Feedback Alignment} (FA)
\citep{Lillicrap2014}. This seminal paper shows that Backprop can still learn successfully when feedback weights are
random. In addition to learning to represent an input-output mapping in forward weights, the network is trained to
extract useful information from randomly weighted instructive pathways. The authors call this process \textit{learning
to learn}, and show that performance is even superior to classical Backprop for some tasks. This mechanism was further
expanded to show that the principles of FA perform very well when biologically plausible plasticity rules are employed
\citep{Liao2016,Zenke2018}. Another popular line of thought is - instead of computing local errors - to compute optimal
activations for hidden layer neurons using autoencoders \citep{Bengio2014,Lee2015,Ahmad2020}. Approaches derived from
this do not suffer from the weight transport problem, and by design does not require local error representations. While
these solutions (summized as \textit{Target propagation} algorithms) solve the weight transport problem, they fall far
behind traditional Backprop on more complex benchmark datasets like
\textit{\href{https://www.cs.toronto.edu/~kriz/cifar.html}{CIFAR}} and
\textit{\href{https://www.image-net.org/index.php}{ImageNet}} \citep{Bartunov2018}.\newline

\noindent Numerous approaches for implementing Backprop with more plausible neuron models exist, most of which employ variants of
the \textit{Leaky Integrate-and-fire} (LIF) neuron \citep{Sporea2013,Lee2016,Bengio2017,Lee2020}. The aforementioned
issue of computing the derivative over spiketrains has been solved in several ways, with the most prominent variant
perhaps being \textit{SuperSpike} \citep{Zenke2018}. One might therefore view this as the weakest criticism aimed at
Backprop. Yet none of the employed neuron models come close to portraying the intricacies of biological neurons, and
thus fail to provide explanations for their complexity. One aspect of this will be discussed in the upcoming
section.\newline

\noindent All of these studies successfully solve one or more concerns of biological plausibility, while still
approximating Backprop to some degree. Yet none of them are able to solve all three simultaneously, and some of them
introduce novel mechanisms that are themselves biologically questionable. It further appears that in all but a few
cases, an increase in biological plausibility leads to a decrease in performance. Thus, whether Backprop could be
implemented or approximate by biological neurons remains an open question.

\subsection{Dendrites as computational elements}\label{sec-dendrites}

The issue of oversimplified neuron models is by far the most frequent to be omitted from explanations of the biological
implausibility of Backprop (See for example \citep{Meulemans2020,Lillicrap2014}). This disregard might stem from the
fact that rate-based point neurons are employed in many of the most powerful artificial neural networks. This fact might
be taken as an argument that the simple summation of synaptic inputs is sufficient for powerful and generalized
learning. Modelling neurons more closely to biology would by this view only increase mathematical complexity and
computational cost without practical benefit. Another hypothesis states that the dominance of point neurons stems from a
"somato-centric perspective" within neuroscience \citep{Larkum2018}, which stems from the technical challenges inherent
to studying dendrites in vivo. The vastly different amount of available data regarding these two neuronal components
might have induced a bias in how neurons are modelled computationally. Some researchers have even questioned whether
dendrites should be seen as more of a 'bug' than a 'feature' \citep{Haeusser2003}, i.e.\ a biological necessity which
needs to be overcome and compensated for.

Yet in recent years, with novel mechanisms of dendritic computation being discovered, interest in researching and
explicitly modelling dendrites has increased. Particularly the vast dendritic branches of pyramidal neurons found in the
cerebral cortex, hippocampus and amygdala, were shown to perform complex integrations of their synaptic inputs
\citep{spruston2008pyramidal}. These dendritic trees are capable of performing coincidence- \citep{Larkum1999} and
sequence detection \citep{Branco2010}. The size of dendritic trees is also known to discriminate regular spiking from
burst firing pyramidal neurons \citep{Elburg2010}. Furthermore, pyramidal neuron dendrites are capable of performing
computations, which were previously assumed to require multi-layer neural networks \citep{Schiess2016,Gidon2020}. See
\citep{Larkum2022} and \citep{Poirazi2020} for extensive reviews. 

These neuroscientific insights have  sparked hope that modelling dendritic compartments explicitly might aid machine
learning in terms in both learning performance and energy efficiency
\citep{Chavlis2021,guerguiev2017towards,Richards2019,Eyal2018}. It appears that, if not for computational gains,
dendrites should be considered essential for any model attempting to explain the power of human learning. While the
network discussed in this thesis includes very simple multi-compartment models, the choice of model was
strongly influenced by the fact that segregated dendrites were considered at all.




\section{Cortical microcircuits}

Another feature of the brain which is often not considered in (biologically plausible) machine learning models is its
intricate connectivity. This is quite understandable, as there is still some uncertainty about which brain areas would
be involved in Backprop-like learning. It is also unclear, to what level of detail these areas would need to be modeled.
It has been shown that the connectivity patterns of cortical circuits are superior to amorphous networks in some cases
\citep{haeusler2007statistical}, so there might be a computational gain from modeling network structure closer to
biology. The question over network structure goes hand in hand with the choice of neuron models, as synaptic connections
arrive at specific points of pyramidal neuron dendrites, depending on the origin of the connection
\citep{felleman1991distributed,Ishizuka1995,Larkum2018}.

Several theories of cortical funcition focus more on reinforcement \citep{Legenstein2008} or unsupervised learning
\citep{George2009,hausler2017inhibitory}. Without dismissing these theories, this thesis will adopt the viewpoint that
human brains require a form of gradient descent to successfully adapt to their ever-changing environments. Furthermore,
we share the hypothesis that this kind of learning occurs predominantly in the neocortex \citep{Marblestone2016}.

The literature on the subject of learning historically appears to be somewhat split (although several important
exceptions have been published recently). On the one hand, the "machine-learning" point of view largely considers the
utility of network changes first, with considerations of biology appearing as an afterthought\citeme. On the other hand,
intricate models of cortical circuits exist, which can so far not be trained to perform tasks
\cite{potjans2014cell,schmidt2018multi,van2022bringing}. Within this thesis, I hope to contribute to the body of
literature between those extremes. For this, my approach will be to select a learning model that is already highly
biologically plausible, and to attempt to improve its plausibility - without breaking the learning rule.

\section{Model selection}\label{sec-model-selection}

The model selection progress was strongly influenced by a review article on biologically plausible approximations of
Backprop \citep{whittington2019theories}. The authors narrow the wide range of proposed solutions down to four
algorithms that are both highly performant and largely biologically plausible. Due to impact of the paper on this
thesis, their model comparison is depicted in Supplementary Table \ref{tab-wb-models}. The algorithms were in part
selected for requiring minimal external control during training, as well as by the fact that they can all be described
within a common framework of energy minimization \citep{Scellier2017}. The first two models are Contrastive learning
\citep{OReilly1996}, and its extension to time-continuous updates \citep{Bengio2017}. Both of these encode
neuron-specific errors in the change of neural activity over time. One of their appeals is the fact that they rely on
Hebbian (and Anti-Hebbian) plasticity, which are highly regarded in the neuroscience literature
\citep{magee2020synaptic,Brea2016}. Yet in the plasticity rule also lies their greatest weakness, as synapses need to
switch between the two opposing mechanisms once the target for a given stimulus is provided. This switch requires a
global signal that communicates the change in state to all neurons in the network simultaneously.

The second class of models was more appealing to me, as both variants are based on the predictive coding account in
Neuroscience \citep{rao1999predictive}, which deserves its own introduction.

\subsection{Predictive coding}

In this seminal model of processing in the visual cortex, each level of the visual hierarchy represents the outside
world at some level of abstraction. Recurrent connections then serve communicate prediction errors and predictions up
and down the hierarchy respectively, which the network attempts to reconcile. The authors show that through rather
simple computations, these prediction errors can be minimized to obtain useful representations at each level of the
hierarchy. They further show that a predictive coding network trained on natural images exhibits end-stopping properties
previously found in mammalian visual cortex neurons. This work was instrumental to shaping the modern neuroscientific
perspective of perception being largely driven by cortico-cortical feedback connections in addition to the feedforward
processes. The extension of predictive coding principles from visual processing to the entire living system is promising
to revolutionize neuroscience under the name of \textit{Active inference} \citep{Friston2008,Friston2009,Adams2015}. By
this view, the entire brain aims to minimize prediction errors with respect to an internal (generative) model of the
world. A noteworthy property of this hypothesis is that it implies an agents action in the world as 'just another' way
in which it can decrease discrepancies between its beliefs and sensory information. In a seminal paper, a model of the
cortical microcircuit \citep{haeusler2007statistical} was shown to have a plausible way for performing the computations
required by predictive coding \citep{bastos2012canonical}.

While predictive coding was originally described as a mechanism for unsupervised learning, through a slight modification
it is also capable of performing Backprop-like supervised learning \citep{Whittington2017}. This is the third model
considered in the review paper, in which values (i.e.\ predictions) and errors of a layer are encoded in separate,
recurrently connected neuron populations. By employing only local Hebbian plasticity, this network is capable of
approximating Backprop in multilayer perceptrons while conforming to the principles of predictive coding. The constraint
on network topology was later relaxed by showing that the model is capable of approximating Backprop for arbitrary
computation graphs \citep{Millidge2022}. The neuron-based predictive coding network was therefore an important
contribution towards unifying the fields of Active inference and machine learning research. As noted in a recent review
article:

\begin{quotation}
  \noindent``Since predictive coding is largely biologically plausible, and has many potentially plausible process
  theories, this close link between the theories provides a potential route to the development of a biologically
  plausible alternative to backprop, which may be implemented in the brain. Additionally, since predictive coding can be
  derived as a variational inference algorithm, it also provides a close and fascinating link between backpropagation of
  error and variational inference.`` \citep{millidge2021predictive} \end{quotation}

\noindent With this perspective in mind, we turn to the final model discussed in the review paper.

\subsection{The Dendritic error model}

The predictive coding network stores local prediction errors in nodes (i.e.\ neurons) close to the nodes to which these
errors relate. That errors may be represented within the activation of individual neurons is a promising hypothesis with
some advantages, as well as results backing it up \citep{Hertaeg2022}. Yet there is a competing view, by which errors
elicited by individual neurons may be represented by membrane potentials of their dendritic compartments
\citep{guerguiev2017towards}. The "Dendritic error model" \citep{sacramento2018dendritic} - as the name implies -
follows this line of thought. It contains a highly recurrent network of both pyramidal- and interneurons, in which
pyramidal neuron apical dendrites encode prediction errors. This view is supported by behavioral rodent experiments
which show that stimulation to pyramidal neuron apical tufts in cortical layer 1 controls learning \citep{Doron2020}.

For the errors to be encoded successfully, the model requires a symmetry between feedforward and feedback sets of
weights, which it has to learn prior to training. After that, apical compartments behave like the error nodes in a
predictive coding network. They are silent during a feedforward network pass, and encode local prediction errors in
their membrane potential when a target is applied to the output layer. Since they are a part of the pyramidal neuron,
only local information is required to minimize these prediction errors through a plasticity rule for multi-compartment
neurons \citep{urbanczik2014learning}. A critical observation made in \citep{whittington2019theories} is that the
dendritic error model is mathematically equivalent to their predictive coding network \todo{expand if I have time,
otherwise this will be a ref.}. All of these factors combined make the dendritic error model a promising model to help
us further understand both predictive coding and deep learning in cortical circuits. While both the employed neuron and
connectivity model are far behind some of the more rigorous cortical simulations, it is regarded in the
literature as an important step towards integrating deep learning and neuroscience.

Nevertheless, the model still suffers from some constraints with regard to its biological plausibility; Both the
predictive coding network and the dendritic error network require strongly constrained connectivity schemes, without
which they cannot learn. This kind of specificity (in particular one-to-one relationships between pairs of neurons) are
highly untypical for cortical connections \citep{Thomson2003}. Hence, their exact network architectures are unlikely to
be present in the cortex. The Dendritic error model additionally requires Pre-training to be capable of approximating
Backprop. Both of these issues will be discussed in this thesis. Yet the most salient improvement to the network's
biological plausibility is likely, to change neuron models from rate-based to spiking neurons. It has been shown that
the Plasticity rule employed by the network is capable of performing simple learning tasks when adapted to spiking
neurons \citep{Stapmanns2021}. Yet, (to the best of my knowledge) there are no studies investigating if this variant is
capable of learning more complex tasks on a network-level. A spiking implementation of the dendritic error network will
therefore be the starting point for this thesis, upon which further analysis shall build.


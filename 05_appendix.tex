
\chapter{Appendix}



\section{Somato-dendritic coupling}\label{sec-somato-dendr}

\cite{urbanczik2014learning} discuss a possible extension to their neuron- and plasticity model, in which the
dendro-somatic coupling transmits voltages in both directions. They show that the plasticity rule requires only minor
adaptations for successful learning under this paradigm. Yet, as described by passive cable theory, the flow between
neuronal compartments is dictated by their respective membrane capacitances. These are calculated from their membrane
areas, which vastly differ in the case of pyramidal neurons. \todo{find a nice citation for this}


15,006 458


will not be considered here. The motivation is, that dendritic membrane area is


\section{Integration of the spike-based Urbanczik-Senn plasticity}



Starting with the complete Integral from $t=0$.

\begin{align}
  \dot{W_{ij}}(t)    & = \eta (\phi(u_i) - \phi(\alpha v^{basal}_i(t))) \phi(u_j)                                                     \\
  \Delta W_{ij}(t,T) & = \int_t^T dt' \ \eta \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \  \phi(u_j^{t'})                         \\
  \Delta W_{ij}(t,T) & = \eta \int_t^T dt' \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \ \phi(u_j^{t'})                            \\
  V_i^*              & = \phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})                                                                    \\
  s_j^*              & = \kappa_s * s_j                                                                                               \\
  \Delta W_{ij}(0,t) & =\eta \int_0^t dt' \  \int_0^{t'} dt'' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'')                          \\
                     & = \eta \int_0^t dt'' \  \int_{t''}^{t} dt' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'')                      \\
                     & = \eta \int_0^t dt'' \  \left[ \tilde{\kappa}(t-t'') - \tilde{\kappa}(0) \right] V_i^\ast (t'') s_j^\ast (t'') \\
\end{align}

With $\tilde{\kappa}$ being the antiderivative of $\kappa$:

\begin{align}
  \kappa(t)         & = \frac{\delta}{\delta t} \tilde{\kappa}(t) \\
  \tilde{\kappa}(t) & = - e^{-\frac{t}{t_{\kappa}}}               \\
\end{align}

The above can be split up into two separate integrals:

Which implies the identities

\begin{align}
  I_1(t_1, t_2 + \Delta t) & = I_1 (t_1, t_2) + I_1 (t_2, t_2 + \Delta t)                                       \\
  I_2(t_1, t_2 + \Delta t) & = e^{- \frac{t_2 - t_1}{\tau_{\kappa}}} I_2 (t_1, t_2) + I_2 (t_2, t_2 + \Delta t)
\end{align}


\begin{align}
  I_2 (t_1, t_2 + \Delta t) & = -\int_{t_1}^{t_2 + \Delta t} dt' \ \tilde{\kappa} (t_2 + \Delta t - t') V_i^\ast (t') s_j^\ast (t')                                        \\
                            & = -\int_{t_1}^{t_2} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')
  -\int_{t_2}^{t_2 + \Delta t} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')                                             \\
                            & = -e^{- \frac{ \Delta t}{\tau_\kappa}} \int_{t_1}^{t_2} dt' \ \left[ -e^{- \frac{t_2 - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')
  -\int_{t_2}^{t_2 + \Delta t} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')
\end{align}


Using this we can rewrite the weight change from $t$ to $T$ as:


\begin{align}
  \Delta W_{ij}(t,T) & = \Delta W_{ij}(0,T) - \Delta W_{ij}(0,t)                                               \\
                     & = \eta [-I_2(0,T) + I_1(0,T) + I_2(0,t) - I_1(0,t)]                                     \\
                     & = \eta [I_1(t,T) - I_2(t,T) + I_2(0,t)\left( 1 - e^{- \frac{T-t}{\tau_\kappa}} \right)]
\end{align}

The simplified \cite{sacramento2018dendritic} case would be:

\begin{align}
  \frac{dW_{ij}}{dt} & = \eta (\phi(u_i) - \phi(\hat{v_i})) \phi(u_j)                                         \\
  \Delta W_{ij}(t,T) & = \int_t^T dt' \ \eta \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \  \phi(u_j^{t'}) \\
  \Delta W_{ij}(t,T) & = \eta \int_t^T dt' \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \ \phi(u_j^{t'})    \\
  V_i^*              & = \phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})                                            \\
  s_j^*              & = \kappa_s * s_j
\end{align}


Where $s_i$ is the postsynaptic spiketrain and $V_i^*$ is the error between dendritic prediction and somatic rate and
$h( u )$. The additional nonlinearity $h( u ) = \frac{d}{du} ln \  \phi(u)$ is ommited in our model \todo{should it
though?}.




Antiderivatives:

\begin{align}
  \int_{-\infty}^x H(t)dt = tH(t) = max(0,t)
\end{align}


\begin{align}
  \tau_l & = \frac{C_m}{g_L} = 10 \\
  \tau_s & = 3
\end{align}

Writing membrane potential to history (happens at every update step of the postsynaptic neuron):

\begin{lstlisting}[language=C++, directivestyle={\color{black}}
                   emph={int,char,double,float,unsigned,exp},
                   emphstyle={\color{blue}}]

UrbanczikArchivingNode< urbanczik_parameters >::write_urbanczik_history(Time t, double V_W, int n_spikes, int comp)
{
	double V_W_star = ( ( E_L * g_L + V_W * g_D ) / ( g_D + g_L ) );
	double dPI = ( n_spikes - phi( V_W_star ) * Time::get_resolution().get_ms() )
      * h( V_W_star );
}\end{lstlisting}

I interpret this as:


\begin{align}
  \int_{t_{ls}}^T dt' \ V_i^* & = \int_{t_{ls}}^T dt' \  (s_i - \phi(V_i )) h(V_i),               \\
  \int_{t_{ls}}^T dt' \ V_i^* & = \sum_{t=t_{ls}}^T \  (s_i(t) -  \phi(V_i^t ) \Delta t) h(V_i^t) \\
\end{align}

\begin{lstlisting}[language=C++, directivestyle={\color{black}}
                   emph={int,char,double,float,unsigned,exp},
                   emphstyle={\color{blue}}]
for (t = t_ls; t< T; t = t + delta_t)
{
   	minus_delta_t = t_ls - t;
    minus_t_down = t - T;
    PI = ( kappa_l * exp( minus_delta_t / tau_L ) - kappa_s * exp( minus_delta_t / tau_s ) ) * V_star(t);
    PI_integral_ += PI;
    dPI_exp_integral += exp( minus_t_down / tau_Delta_ ) * PI;
}  
// I_2 (t,T) = I_2(0,t) * exp(-(T-t)/tau) + I_2(t,T)
PI_exp_integral_ = (exp((t_ls-T)/tau_Delta_) * PI_exp_integral_ + dPI_exp_integral);
W_ji = PI_integral_ - PI_exp_integral_;
W_ji = init_weight_ + W_ji * 15.0 * C_m * tau_s * eta_ / ( g_L * ( tau_L - tau_s ) );    
  
kappa_l = kappa_l * exp((t_ls - T)/tau_L) + 1.0;
kappa_s = kappa_s * exp((t_ls - T)/tau_s) + 1.0;
  \end{lstlisting}


\begin{align}
  \int_{t_{ls}}^T dt' s_j^* & =  \tilde{\kappa_L}(t') * s_j -  \tilde{\kappa_s}(t') * s_j
\end{align}

$I_1$ in the code is computed as a sum:

\begin{align}
  I_1 (t,T) = \sum_{t'=t}^T \ (s_L^*(t') - s_s^*(t')) * V^*(t')
\end{align}



\section{Dendritic leakage conductance}\label{sec-gl-dend}

In order to match the dendritic potential of rate neurons  in the spiking neuron model, a suitable leakage conductance
for dendritic compartments was required. As described in Equation \ref{eq-spiking-basal-compartment}, a dendritic
compartment evolves according to:
\begin{align}
  C_m^{dend} \dot{v}_j^{dend} & = -g_l^{dend} \  v_j^{dend} + \sum_i W_{ji} \    \langle \textit{n}_i \rangle
\end{align}

Under the assumption that the activation of all presynaptic neurons $i$ remains static over time, we can replace the
spontaneous activation $s_i(t)$ with the expected number of spikes per simulation step $\langle \textit{n}_i \rangle =
r_i \ \Delta t$ (cf Equation \ref{eq-n-spikes}). Note that these values do not employ matrix notation, but concern
individual neurons. Next, in order to find the convergence point of the ODE, we set the left side of the equation to $0$
and to solve it:

\begin{align}
  0                        & = -g_l^{dend} \  v_j^{dend} + \sum_i W_{ji} \    r_i \ \Delta t \\
  g_l^{dend} \  v_j^{dend} & = W_{ji} \    r_i \ \Delta t
\end{align}

The desired dendritic potential of rate neurons is $v_j^{dend} = \sum_i W_{ji} \ r_i$, which occurs on both sides of the
above equation. Assuming that our dendritic model fulfills this equality, both terms to drop out from the equation.
Thus, the correct parametrization for the dendritic leakage conductance remains:
\begin{align}
  g_l^{dend} & = \Delta t
\end{align}

It was shown experimentally that for hight spike frequencies, this parameterization leads to an exact match of dendritic
potentials between the neuron models. It will therefore be assumed as the default throughout all experiments where
spiking neurons are used. \newline

In order to keep the NEST models as similar as possible, rate neurons evolve according to the same dynamics. Like in the
original implementation, dendrites of rate neurons are fully defined by their inputs at time $t$. This behaviour is
acheived by setting the leakage conductance to $1$ for all dendritic compartments. During network initialization,
dendritic leakage conductances are set to either one of these values depending on the type of neuron model employed.


\section{Plasticity in feedback connections}\label{sec-feedback-plast}

\todo{move to results}

Within the present model, Pyramidal-to-pyramidal feedback weights evolve according to:

\begin{align}
  \dot{w}_{l}^{down} & = \eta_l^{down} \ ( \phi(u_l^{P}) - \phi(w_l^{down} r_{l+1}^P) )\ \phi(u_{l+1}^{P})^T
\end{align}

The error term in this case differs slightly from the others, but could arguably still be implemented by biological
neurons. An intuitive way to interpret the error term is as the difference between somatic activity and the activity of
a distant apical compartment that is innervated only by superficial pyramidal neurons. Within the NEST implementation,
this distal compartment leaks into the proximal apical compartment ($v^{api}$) with a conductance of $g^{api,dist}=1$.
The separation of pyramidal neuron apical dendrites into a proximal and a distal tree is well documented \citeme. A
difference between plasticity mechanisms for synapses arriving at these two integration zones is plausible, although I
was unable to find prior research supporting this type of plasticity \citeme.  A more sophisticated model of the apical
tree which resembles pyramidal neurons more closely could be a desirable extension to the model.

While the plasticity was successfully implemented in all variants of the model, it did not prove useful for training the
networks during my tests. A strong indicator to the reason behind this is the fact, that the dendritic error for this 
rule is nonzero, even in the self-predicting state (cf. Figure \ref{fig-error-comp-le}). Making these connections non-plastic led to the best learning performance, and is therefore
assumed as the default for all training simulations. This matches the previous implementations of this network too,
which typically set learning rates of these connections to $0$ with the exception of a few experiments employing
steady-state approximations. Note that feedback information is transmitted through fixed weights in this case.
Feedforward weights in turn learn to match these, meaning that the network effectively implements a type of Feedback
alignment \cite{Lillicrap2014}.


\section{Presentation times and Latent Equilibrium}\label{sec-appendix-t-pres}

Exactly matching parameters and the training environment to those of existing implementations turned out to be a
significant challenge. Particularly the way NEST handles signal transmissions made and exact numerical replication of
results impossible, as discussed in Section \todo{talk about timing differences}. In order to validate, that



\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{fig_3_numpy}
  \caption{Replication of Figure \ref{fig-bars-le-snest} using a slightly modified version of the python code from
    \cite{Haider2021}. Resulting performance matches the original results closely, showing that this version can serve
    as a baseline for comparing performance of the NEST implementation to the original results.}
  \label{fig-bars-le-numpy}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{fig_3_rnest}
  \caption{Replication of Figure \ref{fig-bars-le-snest} using networks of rate neurons in the NEST simulator. A notable
    difference to the python implementation in Figure \ref{fig-bars-le-numpy} is, that this version does not handle very
    low presentation times as well. This can likely be traced back to the synaptic delay enforced by NEST, which imposes
    an upper bound on network relaxation time. Besides that, performance of the two variants is very similar.}
  \label{fig-bars-le-rnest}
\end{figure}



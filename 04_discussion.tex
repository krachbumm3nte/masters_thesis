
\chapter{Discussion}


\section{Contribution}




\subsection{criteria for biological plausibility}

In which we discuss, to what extent the network conforms to the criteria for biologically plausible learning rules
introduced by \cite{Whittington2017}:
\begin{enumerate}
      \item Local computation. A neuron performs computation only on the basis
            of the inputs it receives from other neurons weighted by the strengths
            of its synaptic connections.
      \item  Local plasticity. The amount of synaptic weight modification is dependent on only the activity of the two
            neurons the synapse connects (and possibly a neuromodulator).
      \item  Minimal external control. The neurons perform the computation autonomously with as little external control
            routing information in different ways at different times as possible.
      \item   Plausible architecture. The connectivity patterns in the model should
            be consistent with basic constraints of connectivity in neocortex.
\end{enumerate}








\section{Limitations}

Network needs to be reset between stimuli
original does not do that, in NEST it's kindof a big deal.

Most experiments require repeats to confirm results across with multiple independent runs.


\subsection*{Efficiency}
exposure time and training set still quite large


\subsection*{Neuron model}

non-resetting, non-refractory

\subsection*{Network}

Oversized networks have a higher tendency of failing on some tasks (results not shown). This issue is not consistent,
but plagued some of my experiments with no satisfactory explanation yet found.


 When injecting large currents (i.e. in func approx experiments), the network kinda shits itself. Somewhat
expected, somewhat annoying




\section{Future directions}

\subsection{Additional Backprop concerns}


from \citep{Marblestone2016} on one-shot learning

Additionally, the nervous system may have a way of quickly
storing and replaying sequences of events. This would allow
the brain to move an item from episodic memory into a long-
term memory stored in the weights of a cortical network (Ji and
Wilson, 2007), by replaying the memory over and over. This
solution effectively uses many iterations of weight updating to
fully learn a single item, even if one has only been exposed to
it once. Alternatively, the brain could rapidly store an episodic
memory and then retrieve it later without the need to perform
slow gradient updates, which has proven to be useful for
fast reinforcement learning in scenarios with limited available
data (Blundell et al., 2016)


Where do targets come from? \cite{Bengio2015}

\section{Should it be considered pre-training?}

\todo{someone said this network needs pre-training and that made me sad :(}

\section{Relation to energy minimization}


\section{Correspondence of the final model to cortical circuitry}

Cortical neurons depend on neuromodulation too \cite{Roelfsema2018}

Completely unclear whether cortical circuits perform supervised learning \citep{magee2020synaptic}


This would probs be super efficient on neuromorphics!

I am not going to try plasticity with spike-spike or spike-rate dendritic errors

Training on ImageNet

Making this shit faster

BPTT/time-continuous inputs


The dendritic error rule at the core of Urbanczik-Senn looks absurdly similar to superspike \citep{Zenke2018}.
Someone should look into that!

Different configurations for which synapses are plastic should be elaborated on.


\subsection{improvements to the neuron models}


\todo{talk about ALIF neurons \citep{Gerstner2009} also reffed in
e-prop\citep{bellec2019biologically,bellec2020solution}. look for sources}

pyr and intn are wayy to similar

LIF, membrane reset and $t_{ref}$

reward-modulated urbanczik-senn plasticity?

wtf is shunting inhibition?

Consider neurogenesis deeper. Any network that is plausible must be able to develop in a plausible fashion. Investigating
how the cortex develops might hold insights into plausible connectivity schemes. This does not necessarily compete or
conflict with looking at connectivity of developed brains


\subsubsection{improve prospective activity with regard to spike creation}

le does a lot, particularly at hidden layers. Yet response time under spiking paradigms is still
lackluster. In particular, prospective activation does next to nothing for these very low input time
constants. SNN performs best when $\tau_x$ is greater than $\Delta_t$ by roughly x10.









\section{Conclusion}
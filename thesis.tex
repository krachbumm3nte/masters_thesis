\documentclass[11pt,a4paper,titlepage]{report}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath{ {./images/} }
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[round]{natbib}
\bibliographystyle{apalike}
\usepackage[onehalfspacing]{setspace}
\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\par\singlespacing\small}
\usepackage[top=100pt,bottom=100pt,left=75pt,right=75pt]{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}

\DeclareSymbolFont{letters}{OML}{ztmcm}{m}{it}
\DeclareSymbolFontAlphabet{\mathnormal}{letters}
\pagestyle{headings}


\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue,
}


\newcommand*\ttvar[1]{\texttt{\expandafter\dottvar\detokenize{#1}\relax}}
\newcommand*\dottvar[1]{\ifx\relax#1\else
  \expandafter\ifx\string_#1\string_\allowbreak\else#1\fi
  \expandafter\dottvar\fi}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}



\newcommand{\what}[1] {\textcolor{red}{\textbf{#1} \addcontentsline{toc}{subsection}{\textcolor{orange}{#1}}}}

\newcommand{\todo}[1] {\textcolor{orange}{\textbf{TODO: #1}}}

\newcommand{\citeme}{\textcolor{orange}{\textbf{TODO: cite}}}

\newcommand{\phrasing}{\textcolor{green}{\textbf{phrasing}}}


\newcommand{\image}[3][1]
{
\begin{figure}
	\centerline{\includegraphics[width={1\linewidth}]{#1}}
	\caption{#2}
	\label{#3}
\end{figure}
}



\begin{document}

\title{\textbf{Philipps-Universität Marburg}}

\author{Johannes Gille}
\date{\parbox{\linewidth}{\centering%
    Fachbereich 17\endgraf
    AG Allgemeine und Biologische Psychologie\endgraf
    AE Theoretische Kognitionswissenschaft\endgraf
    \bigskip
    \bigskip
    Learning in cortical microcircuits with multi-compartment pyramidal neurons\endgraf
    \textbf{Supervisors:}\endgraf
    \bigskip
    Prof. Dr. Dominik Endres, Philipps-Universität Marburg\endgraf
    \bigskip
    Dr. Johan Kwisthout, Radboud University}}
\maketitle

\tableofcontents

\include{01_introduction.tex}
\include{02_methods.tex}
\include{03_results.tex}
\include{04_discussion.tex}
\include{05_appendix.tex}



\begin{enumerate}
  \item In the torch implementation, there no persistence between timesteps at all. Input is fed into the network and processed feedforward and feedback. Output is read and weights (+biases) are updated. Rinse and repeat.
  \item to what extent should dendritic and somatic compartments decay?
  \item Can (should) we transfer the learned bias from the torch model?
  \item I can "cheat" the apical voltage constraint for self prediction by increasing apical leakage conductance. How does this influence my model?
  \item Is there some analytical approach to identifying why synaptic weights deviate from their intended targets?
  \item I think that lambda needs to be scaled in dependence on $g_{lk}$, such that current inputs, spike inputs and leakage cancel each other out.

  \item How do we deal with population size dependence?

\end{enumerate}

\section{Parameter study}
\begin{itemize}

  \item Transfer function $\phi$
  \item interneuron mixing factor $\lambda$
  \item injected current $I_e$
  \item dendritic leakage conductance $g_{lk,d}$
  \item somatic leakage conductance $g_{lk,s}$
  \item Learning rate $\eta$
  \item synaptic time constants $\tau_{delta}$
  \item noise level $\sigma$
  \item Simulation time $\mathbb{T}$
  \item plasticity onset after the network relaxes
  \item compartment current decay $\tau_{syn}$

\end{itemize}


\subsection*{Observations}

\begin{itemize}
  \item In self-predicting paradigm, Apical errors stay constant, despite interneuron error steadily increasing.
  \item Interneuron error (between neuron pairs) is proportional to absolute somatic voltage in self-predicting paradigm.
  \item abs interneuron voltage is always higher than abs pyramidal voltage. This kind of makes sense, as interneurons receive direct current input proportional to pyramidal voltage in addition to feedforward input. This discrepancy disappears when setting $\lambda$ to 0 as expected.
  \item When plasticity is enabled from a random starting configuration, apical error \textbf{sometimes} converges to better values than can be achieved in both self-predicting paradigms. I believe this to be a huge issue: the self-predicting state does not cause minimal apical voltage, and completely decayed feedback weights are preferable to perfectly counteracting feedback weights.
  \item feedforward weights tend to increase absolutely, i.e. drift towards the closest extreme. \textit{This only happens since I re-implemented the second exponential term in the pyr\_synapse}. Yet they do not simply explode to the nearest extremum, but will traverse a zero weight to reach the maximum with equal sign as the weight they are supposed to match.
  \item feedback weights tend to decay to around zero. Yet they appear to remain close to zero in the direction they are supposed to be.
  \item Idea: I think that the somatic nudging is handled as straight currents being sent to the neuron, instead of the difference between actual and desired somatic voltage.
  \item In the paper and Mathematica code, Feedback learning rate is 2-5 times higher than feedforward lr. In my model, for learning to happen on similar time scales, feedback lr has to be ~100 times lower than feedforward lr. An indicator that my plasticity is messed up.
  \item The simulation is likely producing way too few spikes (5-20 per 1000ms iteration). Could adapting the activation function yield better results?
  \item In the Mathematica solution, leakage conductance is greater than 1! ($\delta U_i = -(g_L + g_D + g_{SI}) U_i + g_D V_{BI} + g_{SI} U_Y$) with $g_L + g_D + g_{SI} = 1.9$
\end{itemize}

\newpage

\chapter{Preliminary structural components}

\section{Synaptic delays}

Where I will inspect the implications of synaptic delays inherent to the NEST simulations on
the model and plasticity rule. In particular, I will look at the biological necessity for this
type of delay and discuss why any model attempting to replicate neuronal processes must be resilient
to these delays.


\section{Literature review - Backpropagation in SNN}

Where I will review other attempts at implementing biologically plausible Backpropagation
alternatives and contrast them to the current model.

\section{NEST Urbanczik-senn implementation}


\section{My neuron model}

\begin{itemize}
  \item Low pass filtering
  \item multi-compartment computation
  \item Imprecision of the ODE
  \item abuse of the somatic conductance
\end{itemize}

\subsection{NEST rate neuron shenanigans}

Given how long I worked on a rate neuron implementation in NEST, some pages should be devoted
to this effort.


\section{My synapse model}

Where I discuss the synapse implementation with regard to multi-compartment neurons,
urbanczik-archiving and in particular the issues with timing that arise from NEST delays.


\section{The relation between the pyramidal microcircuit and actual microcircuits}

Where I can finally use the shit that has been on my whiteboard for half a year...

This will also serve as valuable insight into how plausible this microcircuit actually is,
and might give some insight into possible model extensions.

\subsection{Interneurons and their jobs}

\section{Does it have to be backprop?}

Where I will explain my concerns regarding the usefulness of approximating backpropagation
in light of the substantial one-shot learning capability of the brain and the active inference
model.


\section{Discrepancies between mathematica and NEST model}

1. Weights deviate slightly. This difference can be alleviated by exposing a single stimulus for a longer duration before switching.

\section{Transfer functions}

Where I will discuss the sensitivity of this entire simulation to minor changes in the
parametrization and style of transfer function being used.

\chapter{The weight-leakage tradeoff}

Where I will discuss the issue, that decreasing both synaptic weights and dendritic leakage conductance
lead to more stability in the dendritic voltage, while at the same time requiring longer exposure
per iteration.


\section*{TODOs}

\begin{itemize}
  \item Prove that the network is stable in the self-predicting state and at the end of learning
  \item Show the limits of learning capability (i.e. how big of a network it can match)
  \item Test the network on a real-world dataset (mnist)
  \item prove/find literature on why the poisson process is a rate neuron in the limit.
  \item Does the network still learn when neurons have a refractory period?
  \item Comparison to other spiking backprops
  \item what can we learn from this? does it describe part of the brain
\end{itemize}

\newpage


\chapter{Open Questions}

\begin{itemize}
  \item Any tips for transitioning to large simulation? also regarding the threadripper
  \item Is refractoryness interesting to us or more of a sidenote?
  \item Neuron dropout?
  \item How does one prove that the network is converged and will not diverge again.
  \item randomized/longer synaptic delays?
  \item As a follow up of dropout, maybe even neurogenesis?
  \item Should I look at delaying injection of the target activation?
  \item more ways in which this is biologically implausible?
\end{itemize}

$t_{pres} 10 - 50 \tau$
\bibliography{bib/library.bib}

\end{document}
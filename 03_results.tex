
\chapter{Results}



\todo{Today, I changed the test criterion from the output neuron voltage at time $t$ to a mean over a sample of the
    last $20ms$, network performance improved tremendously. Intuitively makes sense, but in particular it makes NEST networks
    diverge less after peak performance is reached. Are fluctuations in output layer activity increasing during late stages
    of training?}

\todo{it might be worth experimenting with different synaptic delays in NEST in order to evaluate learning performance
    under biologically plausible transmission times. How easy this will \textbf{assumably} be to implement in NEST deserves
    note at this point.}

\todo{talk about the fact that NEST synapses are updated, and SpikeEvents stored to ring buffers to be integrated into
    $u_som$ after the synaptic delay. How much of physiological synaptic delays occurs pre- and postsynaptically in pyramidal
    neurons?}

\section{direct feedback connections to interneurons}\label{sec-electric-syns}

\cite{Vaughn2022,Mancilla2007}


\chapter{room for random observations}

\begin{itemize}
    \item When reducing the \texttt{weight\_scale} parameter weights converge to lower absolute levels. I.e. mean abs
weight drops from 0.5 to approx 0.1
\item When reducing the size of the bar dataset by just 1 element, much simpler networks are capable of learning the task 
at lower t\_pres. Failure to learn shows strange behaviour: the network fails to predict any sample of one group, with the
group which it fails to represent switching every now and then.
\end{itemize}
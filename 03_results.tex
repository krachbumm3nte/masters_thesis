
\chapter{Results}

The following results are exploratory in nature, and After some poor initial results the focus was laid on proving that
the network can perform at all, rather than fine-tuning hyperparameters towards optimal performance. This decision was
in part motivated by a priorization of gaining neuroscientific insights over proving high performance. Yet it should be
noted, that training the network is computationally quite costly (c.f. Section \ref{sec-benchmark}), which turned
parameter studies into a time-consuming process. The network is furthermore fairly sensitive to changes in
parametrization, hence many early experiments in this direction immediately caused a failure to learn. The search for
default parameters itself took some effort, as a certain heterogeneity exists in the two existing implementations
\citep{sacramento2018dendritic,Haider2021}, both in hyperparameters as in the simulation environment. This model
includes properties of both variants, while relying more strongly on the Latent Equilibrium implementation. Unless
stated otherwise, neurons employ prospective activation functions in all simulations. So far, no drawbacks to this
mechanism have presented themseleves, and learning speed can be increased drastically compared to the original
implementation. The full default parametrization is shown in Table \ref{tab-params}.

Since it was anticipated that the spiking implementation would perform worse than the rate-based variant, the first goal
was to measure how big this difference in performance is. Furthermore, a relevant question was, to what degree the
synaptic delays enforced by NEST would influence performance of the rate model. These questions will be answered in the
part of the upcoming sections.




\section{The self-predicting state}

As a first comparison between the three implmementations, the pre-training towards a self-predicting state (cf.
\cite{sacramento2018dendritic}[Fig. S1]) was performed. For this experiment, no target signal is provided at the output
layer, and the network is tasked with learning to self-predict top-down input. The network is initialized with fully
random weights and stimulated with random inputs from a uniform distribution between 0 and 1. A comparison of the four
error metrics between networks is shown in Fig. \ref{fig-self-pred}.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig_self_prediction}
    \caption{Different network types learn to predict self-generated activity in superficial layers. All networks were
        initialized with the same random weights for dimensions $[5, 8, 3]$, and stimulated with $5000$ samples of
        random input for $100ms$ each. As described in \cite{sacramento2018dendritic}, during this phase only
        Pyramidal-Interneuron and Interneuron-Pyramidal weights are plastic ($\eta^{pi}=0.05, \eta^{ip}=0.02375,
            \eta^{up}=\eta^{down}=0$). For spiking neurons, stability was increased by setting $\psi=150$.}
    \label{fig-self-pred}
\end{figure}

Both rate neuron implementations were able to reach comparable values for all error metrics after roughly the same time.
The exact values that errors converge on differs slightly between implementations, with no implementation being clearly
superior. This is an important result for upcoming experiments, as it indicates that the simulation environment
developed for the NEST version is an adequate replica of the original framework.

For the spiking variant, Interneuron- and its corresponding feedfworward weight error are comparable to the other
implementations. In fact these metrics appear to converge slightly faster to comparable values. The primary limitation
of this version are the apical error and the closely correlated feedback weight error. After appearing to converge very
quickly, the two metrics stagnate at very high levels. These high errors correlate with strong fluctuations of the
apical compartment. These fluctuations can likely at least in part be attributed to low spike frequencies. This was
confirmed by repeating the experiment with $\psi=1500$, which alleviated the issue to a degree (results not shown). Yet,
error values were still inferior to the rate models, and this change came at the cost of substantially increased
training time. Therefore, this approach was not pursued much further. A different possible approach is, to increase the
membrane capacitance of the apical compartment in order to smoothe out the error induced by individual spikes. This will
be discussed in Section \ref{sec-c-m-api}.

In most simulations in the literature, the network is initialized to an ideal self-predicting state. Furthermore
feedback weights are often non-plastic ($\eta^{pi}=\eta^{down}=0$). Therefore, a failure to reduce these errors further
might not be a substantial issue. For the time being, showing that the network approaches a self-predicting state was
deemed a sufficient result.


\section{Presentation times and latent equilibrium}\label{sec-le-tpres}

In order to validate the performance of the NEST implementations on a learning task, the parameter study from
\citep{Haider2021}[Fig. 3] was replicated. In this experiment, the network is trained different stimulus presentation
times $t_{pres} \in \{0.3,\ 500\}ms$. Performance of the original Dendritic error network is compared to the improved
model which employs Latent equilibrium. Due to the costly computation of the network under such long $t_{pres}$, a
simple artificial classification dataset was used. The \textit{Bars-dataset} is defined for $3\times3$ input- and $3$
output neurons. it consists of three horizontal, three vertical, and two diagonal bars in the $3\times3$ grid, which are
to be encoded in a 'one-hot-vector' at the output layer. In the experiment, networks of $9-30-3$ pyramidal neurons per
layer were trained for 1000 Epochs of 24 samples each. Networks were initialized to the self-predicting state and only
feedforward $Pyr\rightarrow Pyr$ and $Pyr \rightarrow Intn$ synapses were plastic. Learning rates scaled inversely with
presentation times: $\eta^{ip}_0 = \frac{0.2}{t_{pres}}, \eta^{up}_0 = \frac{0.5}{t_{pres}}, \eta^{up}_1 =
    \frac{0.1}{t_{pres}}$. The results for the spiking NEST network using spiking are shown in Fig. \ref{fig-bars-le-snest},
while the results for NumPy and Rate NEST variants are depicted in Supplementary Figures \ref{fig-bars-le-numpy} and
\ref{fig-bars-le-rnest}, respectively.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fig_3_snest}
    \caption{Replication of Fig. 3 from \cite{Haider2021} using networks of spiking neurons in the NEST simulator.
        \textbf{A:} Comparison between Original dendritic error network by and an identical network employing Latent
        equilibrium. Shown is the training of networks with 9-30-3 neurons on the Bars-dataset from with three different
        stimulus presentation times. \textbf{B:} Test performance after 1000 Epochs as a function of stimulus
        presentation time.}
    \label{fig-bars-le-snest}
\end{figure}

For the original dendritic error model, performance in all three implementations is close to being identical. This an
important finding as it answers two open questions: Changes made for a NEST-compatible implementation were adequate and
result in equal learning performance between rate-based implementations. Learning behaviour of the spiking model is the
same, confirming the hypothesis that the spike-based dendritic plasticity model is capable of more complex credit
assignment tasks than previously shown. In this regard, the implementation can be considered a success.

The results for the Latent equilibrium experiments are somewhat more interesting. For very long $t_{pres}$, both rate
implementations behave the same. yet the NEST implementation requires considerably more epochs for training, as
$t_{pres}$ is reduced. In the limit, the NEST implementation was expected to break due to the enforced synaptic delay.
The NumPy variant computes a full forward pass of the network during the first simulation step, as all layers are
processed in sequence. Only feedback signals from pyramidal neurons are delayed by one timestep, in this simulation
backend. In NEST, all connections have a minimum synaptic delay of $\Delta t$. Therefore, for very short presentation
times the NEST network can not be expected to perform well. It remains an open question whether this feature alone
explains the gradual decrease in performance, or if there is an undiscovered error within the novel neuron models or
simulation environment. Such short stimulus presentation times - while exceptionally beneficial to computational
efficiency - are themselves questionable  in terms of biological plausibility, as they are much lower than pyramidal
neuron time constants \citep{McCormick1985}. Thus, no attempts were made to improve performance for very low $t_{pres}$.

The spiking variant proved similarly sensitive to presentation times as the other NEST variant. While obtaining similar
final accuracy, it required - at best - twice as many stimulus presentations as its direct competitor. This is result,
while somewhat expected, shows that under this paradigm spiking communication is substantially inferior. The simulation
does show that the relaxation problem affects all communication schemes equally. While the difference is not as
pronounced here, Latent Equilibrium substantially improves performance and efficiency of the spiking variant. For this
reason, it will be assumed the default in all upcoming simulations.



\section{Apical compartment capacitance}\label{sec-c-m-api}

Next, an investigation was made into the apical- and feedback weight errors of the SNN. The hypothesis was that a
smoothing of apical compartment voltage would lead to a decrease in both errors. To test this, the Self-predicting
experiment was repeated with numerous values for apical compartment capacitance $C_m^{api} \in \{ 1, 250 \} pF$
(results not shown). The experiment was a success and showed that for $C_m^{api} = 50pF$, apical error is almost halved
($0.0034 \rightarrow 0.0019$), and FB error is $80\%$ lower ($0.15 \rightarrow 0.027$). These values are
still at least an order of magnitude higher than the rate implementations, but mark a substantial improvement. Further
increasing the parameter lead to a decreased apical error, but came at the cost of slower convergence. Higher
membrane capacitances must in general be expected to increase the relaxation period of the entire network. Thus, they
requiring a highly undesirable increase in $t_{pres}$ for successful learning.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fig_c_m_psi}
    \caption{Comparison of SNN performance along different values for $\psi$ and $C_m^{api}$.}
    \label{fig-c-m-psi}
\end{figure}

A secondary objective of experimenting with apical capacitance was to enable learning with lower $\psi$ and therefore
approach biologically plausible firing rates. To test this, training on the Bars dataset was performed under different
combinations of apical capacitance and $\psi$. \todo{describe params and results}



Hence, another tradeoff between performance and training duration is introduced by this parameter.


\section{Imperfect connectivity}

Connectivity within cortical circuits, while structured, appears to subject to a high degree of randomness
\citep{potjans2014cell} As noted before, one-to-one connections between pairs of neurons are therefore highly unlikely
\citep{whittington2019theories}. On the other hand, 'fully connected' populations of neurons likewise have not been
observed in electrophysiological \citep{thomson2002synaptic} and in-vitro \citep{binzegger2004quantitative} analyses of
cortical neurons. Therefore, any network proclaiming to model the cortex must invariably be capable of handling this
imperfect connectivity.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fig_dropout}
    \caption{Error terms after training a network with dimensions [8, 8, 8] towards the self-predicting state, with
        different percentages of synaptic connections randomly removed. Experiments were performed with the
        rate-based network in NEST, each network was trained for 2000 epochs of 50ms each. Errors are averaged over
        6 independent runs for each configuration.}
    \label{fig-dropout}
\end{figure}

To test if the dendritic error model fullfills this requirement, in a first step the self-prediction experiment was
repeated with neuron dropout. To simulate connection probabilities $p_{conn} \in {0.6, 1.0}$, an appropriate number of
synapses was deleted after network setup. To avoid completely separating two neuron populations, this deletion was
performed separately for each of the four synaptic populations.

Even with only 60\% of synaptic connections present, the network architecture still achieves competetive values for all
four error metrics. Weight errors, are calculated as mean squared errors over the two matrices, which requires matrices
to contain data at every cell. Thus, to compute these errors, weights of deleted synapses were set to zero in these
matrices. This choice was made under the assumption that a missing connection in an ideal self-predicting network would
be matched by a zero-weight - or likewise absent - synapse. These results indicate that the dendritic error rule is capable of
compensating for absent synapses by correctly identifying and depressing corresponding feedback connections. Through this 
mechanism, the network is able to retain its self-predicting properties in spite of physiological constraints.


The extent of this capability was confirmed in SNN, by comparing performance on the Bars dataset of a control network to
a \textit{dropout network}. Both networks were initialized with random synaptic weights, and trained with full
plasticity ($\eta^{ip}_0 = 0.004, \eta^{pi}_0 = 0.01, \eta^{up}_0 = 0.01, \eta^{up}_1 = 0.003$) to best enable the
dropout network to compensate for missing synapses. The dropout network was initialized with $40$ instead of $30$ hidden
layer pyramidal neurons to counteract the deletion of $15\%$ of synapses per synaptic population. Both networks
performed very similarly, with the control network reaching perfect accuracy slightly faster (Epoch 140 vs. Epoch 200),
while the dropout network exhibited slightly lower test loss at the end of training (results not shown). 

These results prove that the dendritic error model is capable of learning in spite of imperfect connectivity, whic must be
expected to occur in the cortex. This sets it apart from the previous implementation of a predictive coding network
\citep{Whittington2017}, and further supports its biological plausibility.


\subsection{Interneurons}

An easily overlooked connection of this network is the nudging signal from pyramidal neurons to their interneuron
sisters. These were deliberately not included in the previous dropout studies, as deleting any of them would break the
foundation of the network's learning scheme. If any interneuron was to not receive its nudging signal, its incoming
synapses would be unable to adapt their weights, as somatic activity would be exclusively driven by dendritic voltage.
As a result, both interneuron- and Feedforward weight error would fail to converge, in turn impeding apical error
reduction. These one-to- one connections can be considered the most important communication channels in the network. If
there is no redundancy in the neurons, the deletion of any of them breaks learning in this network.


Sacramento et al. proclaim that the interneurons of of the network resemble somatostatin-expressing (\textit{SST})
neurons. This is a reasonable statement, as SST cells are ubiqutous in the cortex and inhibit the same layers as
pyramidal neurons. Furthermore, they share dense and recurrent synaptic connections to these pyramidal neurons
\citep{urban2016somatostatin}. Furthermore they have been shown to receive top-down instructive signals, which have 
been hypothesized to transmit prediction errors \citep{Leinweber2017}. \todo{expand}


\section{Separation of synaptic polarity}


A dogma held in neuroscience for a long time now has been the notion that all neurons are either excitatory or
inhibitory, as dictated by their type (also known as Dale's law \citep{Kandel1968}). Several studies have since shown
that some neurons violate this law through co-transmission or specific release sites for different neurotransmitters
\citep{Svensson2019,Barranca2022}. Despite these findings, pyramidal neurons are still regarded to release exclusively
Glutamate, therefore being strictly excitatory \citep{gerfen2018long,spruston2008pyramidal,Eyal2018}.



A key limitation of the present network model is the
requirement that all synapses must be able to assume both positive and negative polarities. When restricting any
synaptic population in the network to just one polarity, the network is unable to reach the self-predicting state
\todo{expand?}. Thus, activity in any neuron must be able to have both excitatory and inhibitory postsynaptic effects
facilitated by appropriate synaptic weights. This requirement is at odds with biology, which dictates a singular
synaptic polarity for all outgoing connections of a neuron, determined by neuron type and its corresponding
neurotransmitter \citeme.


To investigate to what degree the plasticity rule can deal with this constraint, an experiment was conducted: A
population of pyramidal neurons $A$  was connected to another population $C$ with plastic synapses that were constrained
to positive weights. In order to facilitate the required depression, $A$ was also connected to a population of
inhibitory interneurons $B$ through excitatory synapses with random and non-plastic weights. The interneurons in turn
were connected to $C$ through plastic, inhibitory connections. All incoming synapses at $C$ targeted the same dendritic
compartment. When inducing a dendritic error in that compartment, all plastic synapses in the network collaborated in
order to minimize that error. When injecting a positive basal error for example, the inhibitory weights ($C \rightarrow
    B$) decayed, while excitatory synaptic weights ($A \rightarrow B$) increased. Flipping the sign of that error injection
had the opposite effect on weights, and likewise cancelled the artificial error. This shows that a separation of
synaptic polarity does not interfere with the principles of the Urbanczik-Senn plasticity when depression is facilitated
by interneurons.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.2\textwidth}
        \textbf{a)}\par\medskip
        \centering
        \includegraphics[width=0.9\textwidth]{fig_exc_inh_network}
    \end{minipage}\hfill
    \begin{minipage}{0.7\textwidth}
        \textbf{b)}\par\medskip
        \centering
        \includegraphics[width=0.9\textwidth]{fig_exc_inh_split}
    \end{minipage}
    \caption{Dendritic error minimization under biological constraints on synaptic polarity and network connectivity.
        \textbf{a)} Network architecture. An excitatory population $A$ connects to a dendrite of Neuron $C$ both
        directly and through inhibitory interneuron population $B$. Only synapses $A\rightarrow C$ and $B \rightarrow C$
        are plastic through dendritic error rules. Populations $A$ and $B$ are fully connected with random weights.
        \textbf{b)} \textit{Left:} All plastic synapses arrive at apical dendrites and evolve according to Equation
        \ref{eq-delta_w_pi}. \textit{Right:} Identical network setup, plasticity for synapses at basal dendrites
        (Equations \ref{eq-delta_w_up}, \ref{eq-delta_w_ip}). \textit{Top:} Dendritic error of a single target neuron.
        Errors of opposite signs are induced at $0$ and $500ms$ (vertical dashed line). \textit{Bottom:} Synaptic
        weights of incoming connections. All initial synaptic weights and input neuron activations were drawn from
        uniform distributions.}
    \label{fig-exc-inh-split}
\end{figure}

Yet, as criticised previously \citep{whittington2019theories}, the one-to-one connections between $A$ and $B$ are
untypical for biological neural networks \citeme. Hence, a second experiment was performed, in which $A$ and $B$ were
fully connected through static synapses with random positive weights. This decrease in specificity of the connections
did not hinder the error-correcting learning, as shown in Fig. \ref{fig-exc-inh-split}.

These results are useful, as they enable a biologically plausible way for excitatory long-range pyramidal projections to
connect to pyramidal neurons in another layer of the network (i.e. in a different part of the cortex). The steps
required to facilitate this type of network are rather simple; A pyramidal neuron projection could enter a distant
cortical area and spread its axonal tree \phrasing within a layer that contains both pyramidal neuron dendrites and
interneurons. If these interneurons themselves connect to the local pyramidal population, Dendritic errors with
arbitrary signs and magnitudes could be effectively minimized.

While error minimization is a funcamental feature of this network, it does not necessarily imply that synaptic credit
assignment is successful aswell. Numerous weight configurations are concievable which could silence dendritic errors,
but likely only a small subset of them is capable of transmitting useful information. To prove that this nonspecific
connectivity is compatible with learning of complex tasks, it was introduced into the dendritic error network. The
connection between Interneurons and Pyramidal neuron apical dendrites was chosen for the first test, as the employed
plasticity rule had proven most resilient to parameter imperfections previously. A network of rate neurons was
initialized with self-predicting weights as in Section \ref{sec-le-tpres}. The Weights $w^{pi}$ were redrawn and
restricted to postive values, and a secondary inhibitory interneuron population was created and fully connected to both
populations as described in Fig. \ref{fig-exc-inh-split}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{fig_exc_inh_training}
    \caption{Training progress of a network of rate neurons in NEST, in which hidden layer connections between
        interneurons and pyramidal neurons are unipolar and nonspecific.}
    \label{fig-exc-inh-training}
\end{figure}





\section{Performance of the different implementations}\label{sec-benchmark}

As stated in \cite{Haider2021}, simulating large dendritic error networks with the full leaky dynamics quickly becomes
unfeasable. While the NEST simulator can be regarded as rather fast \citep{albada2018performance}, simulations on it by
design cannot employ batched matrix multiplication, as is typical in machine learning. Thus, by computing neuron updates
individually even in highly structured networks like this one, NEST was expected to perform worse than previous
implementations using PyTorch and dedicated GPUs. Not only did the NEST implementations perform rather poorly in terms
of speed, the spiking variant was the worst performer across the board. To investigate the extent of this, as well as
possible causes, several benchmark experiments were performed. Unless specified otherwise, these tests were run on
an \textit{AMD Ryzen Threadripper 2990WX} using 8 cores at up to $3.0GHz$.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{fig_benchmark}
    \caption{Benchmark of the three different implementations using a network of $[9, n_{hidden}, 3]$ neurons per layer.
        $n_{hidden}=30$ was chosen as a baseline, as it is the default throughout all simulations on the Bars dataset.
        Networks were instantiated with the same synaptic weights and trained for a single epoch of 5 stimulus
        presentations of $100ms$ each.}
    \label{fig-benchmark}
\end{figure}


To compare how network size affects simulation time, all three implementations created for this project were trained
on the bars dataset for a single epoch (8 samples) with different network sizes.

The result of this comparison is shown in Fig. \ref{fig-benchmark}. The NumPy network is slow at baseline, which is
likely explained by the fact that it is the only variant which is running on a single thread. This is due to a
limitation of NumPy, and could likely be improved greatly by using batched matrix multiplications, as are provided for
example by \texttt{PyTorch}\footnote{It is also possible, that the network code surrounding the NumPy computations is
    less efficient than the one for the NEST network. As this implementation was needed primarily to prove that neuron
    dynamics and synaptic updates were ported correctly to NEST, efficiency was a minor concern here and this was not
    investigated further.}.  Notably, this variant exhibits very little slowdown in response to an increase in network size.
My assumption is, that the vectorization of synaptic updates on a single thread scales up better than the communication
between threads that is required by most events in the NEST simulations. The NEST implementation using rate neurons
performed best in terms of speed across the board. This result was slightly surprising, as the demand on the
communication interface between threads is very high, since all neurons transmit an event to each of their postsynaptic
targets at every time step.

Finally, the novel spiking variant of this model performed substantially worse than anticipated. Particularly in
comparison to the rate implementation, I initially expected substantial performance improvements. The Difference between
the two was even greater when simulating on an office-grade processor (Benchmark was also run on an \textit{Intel Core
    i5-9300H} at $2.40GHz$, results not shown). Three hints about the comparatively poor performance can be deduced: For
one, both the rate and the spiking neuron model employ almost identical neuron models, with minor changes to
parametrization and output generation. Thus, updates to the neuron state are unlikely to be responsible for the worse
performance. Secondly, the number of Events transmitted between neurons is much lower for the SNN compared to

the \textit{relative} performance decrease when increasing the number neurons by the same amout is much greater for the
spiking network. Thus, the most likely cause of slowdown are the updates at the synapses. This is supported by the fact,
that the number of synapses increases much faster for this kind of network than the number of neurons. For the given
network of $n_{x} = 9$ input neurons, $n_y = 3$ output neurons and $n_{h}$ neurons in the hidden layer $l$, the number
of total synapses in the network is given by

\begin{align}
    n_{synapses} & = |w_{l}^{up}| + |w_{l}^{pi}| + |w_{l}^{ip}| + |w_{l}^{down}| + |w_{y}^{up}| \\
                 & = n_h n_x + n_h n_y + n_y n_h  + n_h n_y + n_y  n_h                          \\
                 & = n_h (n_x + n_y^4)
\end{align}

with $|w|$ of a weight matrix $w$ in this case denoting the total number of elements in that matrix. By contrast, in a
strictly feedforwad network of three layers the number of synapses is given by $n_{synapses} = n_h (n_x + n_y)$. Thus,
the number of synapses in a network grows exponentially faster than for the dendritic error network, making 

availis given by the stark increase in

\section{Response to unpredicted stimuli}

One of the predictions about cortical activity made by predictive coding is an increased network activity in response to
senesory input violating the generative model. This behaviour is to be expected, as local prediction errors in these
networks are encoded in the activation of error neurons. A diverse set of studies has since independently reported
behaviour consistent with this hypothesis in primate cortical neurons (see \citep{bastos2012canonical}[Table 1] for an
extensive review). This property is also listed in Table \ref{tab-wb-models} in support of the predictive coding
network, but not the dendritic error model. As this is an experimentally testable hypothesis, a simple experiment was
conducted. A network with two hidden layers (dims=[9,30,10,3]) was presented with stimuli from the Bars-dataset. The
number of spikes were recorded for three paradigms: Fully random weights, self-predicting weights, and post-training
weights from a previous experiment. Results are shown in Figure \ref{fig-stimulus-response}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fig_unpredicted_stimulus}
    \caption{Comparison of neuron activation in response to stimuli based on current network weights.}
    \label{fig-stimulus-response}
\end{figure}

As the results show, the network does exhibit increased activation when stimuli are novel




<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,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);
}
h2.filename {
  font-family: monospace;
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.linenb {
  font-style: italic;
  color: lightgrey;
  width: 30pt;
  float: left;
  margin-top: 1pt;
  margin-bottom: 1pt;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.codeline {
  margin-left: -30pt;
  padding-left: 60pt;
  margin-top: 1pt;
  margin-bottom: 1pt;
}
.no-text {
  display: none;
}
.clear {
  clear: both;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your file(s) with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<h2 class="filename">03_results.tex</h2>

<p>Found 35 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Results}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">The following results are exploratory in nature, and are </div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline"><span class="keyword1">\section</span>{The self-predicting state}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">As a first comparison between the three <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [implementations] (136) [lt:en:MORFOLOGIK_RULE_EN_US]">implmementations</span>, the pre-training towards a self-predicting stat (cf.</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">\cite{sacramento2018dendritic} Figure S1) was performed with equal <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [parametrization] (247) [lt:en:MORFOLOGIK_RULE_EN_US]">parametrizations</span>, as shown in Figure</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">\ref{fig-self-pred}. For this experiment, no target signal is provided at the output layer, and the network was</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">stimulated with random inputs.</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_self_prediction}</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">    <span class="keyword1">\caption</span>{Different network types learn to predict self-generated activity in superficial layers. All networks were</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">        initialized with the same random weights for dimensions $[6, 10, 3]$, and stimulated with $5000$ samples of random input for $100ms$ each.</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">        As described <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite{</span>sacramento2018dendritic}, during only Pyramidal-Interneuron and Interneuron-Pyramidal</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">        weights are plastic ($\eta^{pi}=0.05, \eta^{ip}=0.02375, \eta^{up}=\eta^{down}=0$). All variants reach a</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">        self-predicting state within the first 1000 stimulus presentations and errors remain stable after that point.}</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">    <span class="keyword1">\label</span>{fig-self-pred}</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">All implementations were able to reach comparable values for the four error metrics after roughly the same time. The</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">exact values that errors converge on differs slightly between implementations, but generally is on the same order of</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">magnitude and thus does not hinder learning performance greatly (cf. Section \ref{sec-le-tpres}). A notable outlier is</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">the apical error of pyramidal neuron in the spike-based implementation. This can however be traced back to individual</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">spikes causing substantial deviations in apical potentials, and can therefore be alleviated by increasing the</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline"><span class="keyword1">\textit</span>{weight\_scale} parameter (results not shown) at the cost of increased training time. Alternatively, increasing</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">the membrane capacitance of the apical compartment also solves the issue as it <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [smoother, smoothed, smoothies, smoothest, soothes, smooths, smooches, smooth es, smoothen, smoothens] (1166) [lt:en:MORFOLOGIK_RULE_EN_US]">smoothes</span> out the effect of individual</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">spikes. Yet this solution also increases the relaxation period of the entire network, requiring a highly undesirable</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">increase in $t_{pres}$ for successful learning. Since weight errors converge to similar values as the rate-based</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">implementations, an increased absolute apical compartment voltage was deemed tolerable.</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline"><span class="keyword1">\section</span>{Apical compartment capacitance}</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{Presentation times with latent equilibrium}<span class="keyword1">\label</span>{sec-le-tpres}</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">In order to validate the performance of my implementations, I replicated a parameter study <span class="highlight-sh" title="Do not use 'from [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">from \cite{</span>Haider2021}[Fig.</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">    3]. The results for the NEST network using spiking neurons with default parameters \todo{elaborate on this} are</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">    shown in Fig. \ref{fig-bars-le-snest}. A</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_3_snest}</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">    <span class="keyword1">\caption</span>{Replication of <span class="highlight-sh" title="Do not refer to figures using hard-coded numbers. Use \ref instead. [sh:hcfig]">Figure 3</span> <span class="highlight-sh" title="Do not use 'from [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">from \cite{</span>Haider2021} using networks of spiking neurons in the NEST simulator.</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">        <span class="keyword1">\textbf</span>{A:} Comparison between Original pyramidal microcircuit network <span class="highlight-sh" title="Do not use 'by [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">by \cite{</span>sacramento2018dendritic} and</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">        Latent equilibrium variant <span class="highlight-sh" title="Do not use 'from [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">from \cite{</span>Haider2021}. Shown is the training of a network with 9-30-3 neurons on the</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">        'Bars' Dataset from \todo{describe it} with three different stimulus presentation times. <span class="keyword1">\textbf</span>{B:} Test</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">        performance after 1000 Epochs as a function of stimulus presentation time.}</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">    <span class="keyword1">\label</span>{fig-bars-le-snest}</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline"><span class="keyword1">\section</span>{Separation of synaptic polarity}</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">\todo{investigate dales law <span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\citep{</span>Barranca2022}}</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">A key limitation of the present network model is the requirement that all synapses must be able to assume both positive</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">and negative polarities. When restricting any synaptic population in the network to just one polarity, the network is</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">unable to reach the self-predicting state \todo{expand?}. Thus, activity in any neuron must be able to have both</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">excitatory and inhibitory postsynaptic effects facilitated by appropriate synaptic weights. This requirement is at odds</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">with biology, which dictates a singular synaptic polarity for all outgoing connections of a neuron, determined by neuron</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">type and its corresponding neurotransmitter \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (2528) [lt:en:MORFOLOGIK_RULE_EN_US]">citeme</span>.</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">To investigate to what degree the plasticity rule can deal with this constraint, an experiment was conducted: A</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline"> population of pyramidal neurons $A$  was connected to another population $C$ with plastic synapses that were</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline"> constrained to positive weights. In order to facilitate the required depression, $A$ was also connected to a population</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline"> of inhibitory interneurons $B$ through excitatory synapses with random and non-plastic weights. The interneurons in</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline"> turn were connected to $C$ through plastic, inhibitory connections. All incoming synapses at $C$ targeted the same</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline"> dendritic compartment. When inducing a dendritic error in that compartment, all plastic synapses in the network</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline"> collaborated in order to minimize that error. When injecting a positive basal error for example, the inhibitory weights</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline"> ($C \rightarrow B$) decayed, while excitatory synaptic weights ($A \rightarrow B$) increased. Flipping the sign of that</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline"> error injection had the opposite effect on weights, and likewise cancelled the artificial error. This shows that a</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline"> separation of synaptic polarity does not interfere with the principles of the <span class="highlight-spelling" title="Possible spelling mistake found. (3620) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity when depression</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"> is facilitated by interneurons.</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline"> <span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">    <span class="keyword2">\begin{minipage}</span>{0.2\textwidth}</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">        <span class="keyword1">\textbf</span>{a)}\par\medskip</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">        \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_exc_inh_network}</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">    <span class="keyword2">\end{minipage}</span>\hfill</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">    <span class="keyword2">\begin{minipage}</span>{0.7\textwidth}</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">        <span class="keyword1">\textbf</span>{b)}\par\medskip</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">        \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_exc_inh_split}</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">    <span class="keyword2">\end{minipage}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">    <span class="keyword1">\caption</span>{Dendritic error minimization under biological constraints on synaptic polarity and network connectivity.</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">        <span class="keyword1">\textbf</span>{a)} Network architecture. An excitatory population $A$ connects to a dendrite of Neuron $C$ both</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">        directly and through inhibitory interneuron population $B$. Only synapses $A\rightarrow C$ and $B \rightarrow C$</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">        are plastic through dendritic error rules. Populations $A$ and $B$ are fully connected with random weights.</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">        <span class="keyword1">\textbf</span>{b)} <span class="keyword1">\textit</span>{Left:} All plastic synapses arrive at apical dendrites and evolve according to Equation</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">        \ref{eq-delta_w_pi}. <span class="keyword1">\textit</span>{Right:} Identical network setup, plasticity for synapses at basal dendrites</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">        (Equations \ref{eq-delta_w_up}, \ref{eq-delta_w_ip}). <span class="keyword1">\textit</span>{Top:} Dendritic error of a single target neuron.</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">        Errors of opposite signs are induced at $0$ and $500ms$ (vertical dashed line). <span class="keyword1">\textit</span>{Bottom:} Synaptic</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">        weights of incoming connections. All initial synaptic weights and input neuron activations were drawn from</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">        uniform distributions.}</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">    <span class="keyword1">\label</span>{fig-exc-inh-split}</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">Yet, as <span class="highlight-spelling" title="Possible spelling mistake. 'criticised' is British English.. Suggestions: [criticized] (3705) [lt:en:MORFOLOGIK_RULE_EN_US]">criticised</span> previously \citep{whittington2019theories}, the one-to-one connections between $A$ and $B$ are</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">untypical for biological neural networks \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (3821) [lt:en:MORFOLOGIK_RULE_EN_US]">citeme</span>. Hence, a second experiment was performed, in which $A$ and $B$ were</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">fully connected through static synapses with random positive weights. This decrease in specificity of the connections</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">did not hinder the error-correcting learning, as shown in Fig. \ref{fig-exc-inh-split}.</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">These results are useful, as they enable a biologically plausible way for excitatory long-range pyramidal projections to</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">connect to pyramidal neurons in another layer (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>in a different part of the cortex). The steps required to facilitate</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">this type of network are rather simple; A pyramidal neuron projection could enter a distant cortical area and <span class="highlight" title="Possible typo: you repeated a word. Suggestions: [spread] (4432) [lt:en:ENGLISH_WORD_REPEAT_RULE]">s</span>pread</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">spread its <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [atonal] (4450) [lt:en:MORFOLOGIK_RULE_EN_US]">axonal</span> tree \phrasing within a layer that contains both pyramidal neuron dendrites and interneurons. If these</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">interneurons themselves connect to the local pyramidal population, Errors with arbitrary signs and magnitudes in those</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">dendrites could be effectively minimized by the described connectivity. While error minimization is a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [fundamental] (4781) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>funcamental</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">feature of this network, it does not necessarily imply that synaptic credit assignment is successful <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [as well, swell, Aspell, a swell] (4894) [lt:en:MORFOLOGIK_RULE_EN_US]">aswell</span>. To prove</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">that this nonspecific connectivity does not hinder learning, it was introduced into the dendritic error network. The</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">connection between Interneurons and Pyramidal neuron apical dendrites was chosen for the first test, as the employed</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">plasticity rule had proven most resilient to parameter imperfections previously. A network of rate neurons was</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">initialized with self-predicting weights as in Section \ref{sec-le-tpres}. The Weights $w^{pi}$ were redrawn and</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">restricted to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [positive, postie] (5359) [lt:en:MORFOLOGIK_RULE_EN_US]">postive</span> values, and a secondary inhibitory interneuron population was prepared and fully connected to both</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">populations as described in Fig. \ref{fig-exc-inh-split}. </div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.8\textwidth]{fig_exc_inh_training}</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">    <span class="keyword1">\caption</span>{Training progress of a network of rate neurons in NEST, in which hidden layer connections between</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">    interneurons and pyramidal neurons are unipolar and nonspecific.}</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">    <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-exc-inh-training is never referenced in the text [sh:figref]">fig-exc-inh-training</span>}</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline"><span class="keyword1">\section</span>{In search of plausible spike frequencies}</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline"> \todo{expand}</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline"><span class="keyword1">\section</span>{Resilience to imperfect connectivity}</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_dropout}</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">    <span class="keyword1">\caption</span>{Error terms after training a network with dimensions [8, 8, 8] towards the self predicting states, with</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline">    different percentages of synaptic connections randomly removed. To avoid completely separated layers, synapse</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">    deletion was performed per synaptic population, and the percentage is therefore only an approximation. Even with</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">    only 60\% of synaptic connections present, the network architecture still achieves competetive values for all four</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">    error metrics. For the weight errors, which is calculated with mean-squared-error over two matrices, missing</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">    synapses were set to $0$. This choice was made under the assumption that a missing connection in an ideal</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">    self-predicting network would be matched by a zero-weight - or likewise absent - synapse. Experiments were performed</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">    with the rate-based network in NEST, each network was trained fro 2000 epochs of 50ms each.}</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">    <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-dropout is never referenced in the text [sh:figref]">fig-dropout</span>}</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline"><span class="highlight-sh" title="A section title should start with a capital letter. [sh:001]"><span class="keyword1">\section</span>{direct feedback connections to interneurons}</span><span class="keyword1">\label</span>{sec-electric-syns}</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline">\cite{Vaughn2022,Mancilla2007}</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline"><span class="keyword1">\section</span>{Performance of the different implementations}</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">As stated <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite{</span>Haider2021}, simulating the present network with many neurons or more than one hidden layer quickly</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">becomes unfeasible when simulating the full leaky dynamics. To investigate how network size affects simulation time, all</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">three implementations created for this project were trained on the <span class="highlight" title="An apostrophe may be missing.. Suggestions: [bars', bar's] (5991) [lt:en:POSSESSIVE_APOSTROPHE]">bars</span> dataset for a single epoch with different</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">network sizes for a single epoch, in order to assess efficiency.</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.8\textwidth]{fig_benchmark}</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline">    <span class="keyword1">\caption</span>{Benchmark of the three different implementations using a network of $[9, n_{hidden}, 3]$ neurons per layer.</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">        $n_{hidden}=30$ was chosen as a baseline, as it is the default throughout all simulations on the Bars dataset.</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">        Networks were instantiated with the same synaptic weights and trained for a single epoch of 5 stimulus</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">        presentations of $100ms$ each. Simulations were performed on an <span class="keyword1">\textit</span>{AMD Ryzen Threadripper 2990WX} using 8</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">        cores for the NEST simulations at up to $3.0GHz$.}</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">    <span class="keyword1">\label</span>{fig-benchmark}</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">The result of this comparison is shown in Fig. \ref{fig-benchmark}. The NumPy network is slow at baseline, which is</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">likely explained by the fact that it is the only variant which is running on a single thread. This is due to a</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline">limitation of NumPy, and could likely be improved greatly by using batched matrix multiplications, as are provided for</div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">example by \texttt{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PyTorch It] (6447) [lt:en:MORFOLOGIK_RULE_EN_US]">PyTorch}\footnote{It</span> is also possible, that the network code surrounding the NumPy computations is</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">less efficient than the one for the NEST network. As this implementation was needed primarily to prove that neuron</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">dynamics and synaptic updates were ported correctly to NEST, efficiency was a minor concern here and this was not</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">investigated further<span class="highlight" title="Two consecutive dots. Suggestions: [., â€¦] (6784) [lt:en:DOUBLE_PUNCTUATION]">.}.</span>  Notably, this variant exhibits very little slowdown in response to an increase in network size.</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">My assumption is, that the vectorization of synaptic updates on a single thread scales up better than the communication</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">between threads that is required by most events in the NEST simulations. The NEST implementation using rate neurons</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">performed best in terms of speed across the board. This result was slightly surprising, as the demand on the</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">communication interface between threads is very high, since all neurons transmit an event to each of their postsynaptic</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">targets at every time step.</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">Finally, the novel spiking variant of this model performed substantially worse than anticipated. Particularly in</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">comparison to the rate implementation, I initially expected substantial performance improvements. The Difference between</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">the two was even greater when simulating on an office-grade processor (Benchmark was also run on an <span class="keyword1">\textit</span>{Intel Core</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">i5-9300H} at $2.40GHz$, results not shown). Three hints about the comparatively poor performance can be deduced: For</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">one, both the rate and the spiking neuron model employ almost identical neuron models, with minor changes to</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">parametrization and output generation. Thus, updates to the neuron state are unlikely to be responsible for the worse</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">performance. Secondly, the number of Events transmitted between neurons is much lower for the SNN compared to</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [The] (8175) [lt:en:UPPERCASE_SENTENCE_START]">the</span> <span class="keyword1">\textit</span>{relative} performance decrease when increasing the number neurons by the same <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [about, amount, amour, Amour, am out] (8256) [lt:en:MORFOLOGIK_RULE_EN_US]">amout</span> is much greater for the</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">spiking network. Thus, the most likely cause of slowdown are the updates at the synapses. This is supported by the fact,</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">that the number of synapses increases much faster for this kind of network than the number of neurons. For the given</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">network of $n_{x} = 9$ input neurons, $n_y = 3$ output neurons and $n_{h}$ neurons in the hidden layer $l$, the number</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">of total synapses in the network is given by</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline">    n_{synapses} &amp; = |w_{l}^{up}| + |w_{l}^{pi}| + |w_{l}^{ip}| + |w_{l}^{down}| + |w_{y}^{up}| \\</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">                 &amp; = n_h n_x + n_h n_y + n_y n_h  + n_h n_y + n_y  n_h                          \\</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">                 &amp; = n_h (n_x + n_y^4)</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [With] (8664) [lt:en:UPPERCASE_SENTENCE_START]">with</span> $|w|$ of a weight matrix $w$ in this case denoting the total number of elements in that matrix \what{Is there a</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">    more conventional notation?}. Thus, the number of synapses in a network grows much faster than the number of total</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">    neurons when increasing the size of the hidden layer.</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Availis] (8946) [lt:en:UPPERCASE_SENTENCE_START]">availis</span> given by the stark increase in</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 93 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span>{Response to unpredicted stimuli}</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">The activity of many cortical neurons increases when a brain is presented with unpredicted <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [stimuli] (9110) [lt:en:MORFOLOGIK_RULE_EN_US]">stimuly</span> that regard these</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">neurons \todo{check out \cite{whittington2019theories} refs 50-54}. This property is prominently replicated by</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">predictive coding networks, since activation of error nodes is a function of local prediction errors. Prediction errors</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">in the present model on the other hand are encoded in both positive and negative potentials of apical dendrites. Hence, </div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">the </div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_activity_unpredicted_stimulus}</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">    <span class="keyword1">\caption</span>{Comparison of network response to stimuli from the Bars dataset.}</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">    <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-stimulus-response is never referenced in the text [sh:figref]">fig-stimulus-response</span>}</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline">&nbsp;</div><div class="clear"></div>
</div>
<h2 class="filename">05_appendix.tex</h2>

<p>Found 23 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Appendix}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{<span class="highlight-spelling" title="Possible spelling mistake found. (13) [lt:en:MORFOLOGIK_RULE_EN_US]">Somato-dendritic</span> coupling}<span class="keyword1">\label</span>{sec-somato-dendr}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">\cite{urbanczik2014learning} discuss a possible extension to their neuron- and plasticity model, in which the</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (125) [lt:en:MORFOLOGIK_RULE_EN_US]">dendro-somatic</span> coupling transmits voltages in both directions. They show that the plasticity rule requires only minor</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">adaptations for successful learning under this paradigm. Yet, as described by passive cable theory, the flow between</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">neuronal compartments is dictated by their respective membrane <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [capacitance] (423) [lt:en:MORFOLOGIK_RULE_EN_US]">capacitances</span>. These are calculated from their membrane</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">areas, which vastly differ in the case of pyramidal neurons. \todo{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Find] (539) [lt:en:UPPERCASE_SENTENCE_START]">find</span> a nice citation for this}</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">15,006 458</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Will] (584) [lt:en:UPPERCASE_SENTENCE_START]">will</span> not be considered here. The motivation is, that dendritic membrane area is</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline"><span class="keyword1">\section</span>{Integration of the spike-based <span class="highlight-spelling" title="Possible spelling mistake found. (697) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity}</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">Starting with the complete Integral from $t=0$.</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">  \dot{W_{ij}}(t)    &amp; = \eta (\phi(u_i) - \phi(\alpha v^{basal}_i(t))) \phi(u_j)                                                     \\</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \int_t^T dt' \ \eta \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \  \phi(u_j^{t'})                         \\</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \eta \int_t^T dt' \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \ \phi(u_j^{t'})                            \\</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">  V_i^*              &amp; = \phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})                                                                    \\</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">  s_j^*              &amp; = \kappa_s * s_j                                                                                               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">  \Delta W_{ij}(0,t) &amp; =\eta \int_0^t dt' \  \int_0^{t'} dt'' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'')                          \\</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">                     &amp; = \eta \int_0^t dt'' \  \int_{t''}^{t} dt' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'')                      \\</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">                     &amp; = \eta \int_0^t dt'' \  \left[ \tilde{\kappa}(t-t'') - \tilde{\kappa}(0) \right] V_i^\ast (t'') s_j^\ast (t'') \\</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">With $\tilde{\kappa}$ being the antiderivative of $\kappa$:</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">  \kappa(t)         &amp; = \frac{\delta}{\delta t} \tilde{\kappa}(t) \\</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">  \tilde{\kappa}(t) &amp; = - e^{-\frac{t}{t_{\kappa}}}               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">The above can be split up into two separate integrals:</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">Which implies the identities</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">  I_1(t_1, t_2 + \Delta t) &amp; = I_1 (t_1, t_2) + I_1 (t_2, t_2 + \Delta t)                                       \\</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">  I_2(t_1, t_2 + \Delta t) &amp; = e^{- \frac{t_2 - t_1}{\tau_{\kappa}}} I_2 (t_1, t_2) + I_2 (t_2, t_2 + \Delta t)</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">  I_2 (t_1, t_2 + \Delta t) &amp; = -\int_{t_1}^{t_2 + \Delta t} dt' \ \tilde{\kappa} (t_2 + \Delta t - t') V_i^\ast (t') s_j^\ast (t')                                        \\</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">                            &amp; = -\int_{t_1}^{t_2} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">  -\int_{t_2}^{t_2 + \Delta t} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')                                             \\</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">                            &amp; = -e^{- \frac{ \Delta t}{\tau_\kappa}} \int_{t_1}^{t_2} dt' \ \left[ -e^{- \frac{t_2 - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">  -\int_{t_2}^{t_2 + \Delta t} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">Using this we can rewrite the weight change from $t$ to $T$ as:</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \Delta W_{ij}(0,T) - \Delta W_{ij}(0,t)                                               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">                     &amp; = \eta [-I_2(0,T) + I_1(0,T) + I_2(0,t) - I_1(0,t)]                                     \\</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">                     &amp; = \eta [I_1(t,T) - I_2(t,T) + I_2(0,t)\left( 1 - e^{- \frac{T-t}{\tau_\kappa}} \right)]</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">The simplified \cite{sacramento2018dendritic} case would be:</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">  \frac{dW_{ij}}{dt} &amp; = \eta (\phi(u_i) - \phi(\hat{v_i})) \phi(u_j)                                         \\</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \int_t^T dt' \ \eta \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \  \phi(u_j^{t'}) \\</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \eta \int_t^T dt' \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \ \phi(u_j^{t'})    \\</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">  V_i^*              &amp; = \phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})                                            \\</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">  s_j^*              &amp; = \kappa_s * s_j</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">Where $s_i$ is the postsynaptic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (1030) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> and $V_i^*$ is the error between dendritic prediction and somatic rate and</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">$h( u )$. The additional nonlinearity $h( u ) = \frac{d}{du} ln \  \phi(u)$ is <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [limited, committed, omitted, commuted, Olmsted, orbited, vomited, summited] (1146) [lt:en:MORFOLOGIK_RULE_EN_US]">ommited</span> in our model \todo{should it</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">though?}.</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Antiderivative] (1190) [lt:en:MORFOLOGIK_RULE_EN_US]">Antiderivatives</span>:</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">  \int_{-\infty}^x H(t)dt = tH(t) = max(0,t)</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">  \tau_l &amp; = \frac{C_m}{g_L} = 10 \\</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">  \tau_s &amp; = 3</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">Writing membrane potential to history (happens at every update step of the postsynaptic neuron):</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline"><span class="keyword2">\begin{lstlisting}</span>[language=C++, directivestyle={\color{black}}</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">                   emph={int,char,double,float,unsigned,exp},</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">                   emphstyle={\color{blue}}]</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">UrbanczikArchivingNode&lt; urbanczik_parameters &gt;::write_urbanczik_history(Time t, double V_W, int n_spikes, int comp)</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">{</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">	double V_W_star = ( ( E_L * g_L + V_W * g_D ) / ( g_D + g_L ) );</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">	double dPI = ( n_spikes - phi( V_W_star ) * Time::get_resolution().get_ms() )</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">      * h( V_W_star );</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">}<span class="keyword2">\end{lstlisting}</span></div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">I interpret this as:</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">  \int_{t_{ls}}^T dt' \ V_i^* &amp; = \int_{t_{ls}}^T dt' \  (s_i - \phi(V_i )) h(V_i),               \\</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">  \int_{t_{ls}}^T dt' \ V_i^* &amp; = \sum_{t=t_{ls}}^T \  (s_i(t) -  \phi(V_i^t ) \Delta t) h(V_i^t) \\</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline"><span class="keyword2">\begin{lstlisting}</span>[language=C++, directivestyle={\color{black}}</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">                   emph={int,char,double,float,unsigned,exp},</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">                   emphstyle={\color{blue}}]</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">for (t = t_ls; t&lt; T; t = t + delta_t)</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">{</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">   	minus_delta_t = t_ls - t;</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">    minus_t_down = t - T;</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">    PI = ( kappa_l * exp( minus_delta_t / tau_L ) - kappa_s * exp( minus_delta_t / tau_s ) ) * V_star(t);</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">    PI_integral_ += PI;</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">    dPI_exp_integral += exp( minus_t_down / tau_Delta_ ) * PI;</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">}  </div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">// I_2 (t,T) = I_2(0,t) * exp(-(T-t)/tau) + I_2(t,T)</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">PI_exp_integral_ = (exp((t_ls-T)/tau_Delta_) * PI_exp_integral_ + dPI_exp_integral);</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">W_ji = PI_integral_ - PI_exp_integral_;</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">W_ji = init_weight_ + W_ji * 15.0 * C_m * tau_s * eta_ / ( g_L * ( tau_L - tau_s ) );    </div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">  </div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">kappa_l = kappa_l * exp((t_ls - T)/tau_L) + 1.0;</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">kappa_s = kappa_s * exp((t_ls - T)/tau_s) + 1.0;</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">  <span class="keyword2">\end{lstlisting}</span></div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">  \int_{t_{ls}}^T dt' s_j^* &amp; =  \tilde{\kappa_L}(t') * s_j -  \tilde{\kappa_s}(t') * s_j</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">$I_1$ in the code is computed as a sum:</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">  I_1 (t,T) = \sum_{t'=t}^T \ (s_L^*(t') - s_s^*(t')) * V^*(t')</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline"><span class="keyword1">\section</span>{Dendritic leakage conductance}<span class="keyword1">\label</span>{sec-gl-dend}</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">In order to match the dendritic potential of rate neurons  in the spiking neuron model, a suitable leakage conductance</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">for dendritic compartments was required. As described in Equation \ref{eq-spiking-basal-compartment}, a dendritic</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">compartment evolves according to:</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">  C_m^{dend} \dot{v}_j^{dend} &amp; = -g_l^{dend} \  v_j^{dend} + \sum_i W_{ji} \    \langle <span class="keyword1">\textit</span>{n}_i \rangle</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">Under the assumption that the activation of all presynaptic neurons <span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (1711) [lt:en:I_LOWERCASE]">$i$</span> remains static over time, we can replace the</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">spontaneous activation $s_i(t)$ with the expected number of spikes per simulation step $\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [angle, Langley, Angle, tangle, dangle, jangle, mangle, bangle, wangle, l angle] (1840) [lt:en:MORFOLOGIK_RULE_EN_US]">langle</span> <span class="keyword1">\textit</span>{n}_i \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [range, angle, Angle, tangle, dangle, jangle, mangle, bangle, wrangle, rankle, wangle, Randle] (1852) [lt:en:MORFOLOGIK_RULE_EN_US]">rangle</span> =</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">r_i \ \Delta <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [to, TT, TV, ta, ts, TD, T, TA, TB, TC, TF, TG, TH, TI, TJ, TK, TL, TM, TN, TO, TP, TR, TS, TU, TW, TX, TY, TZ, Ta, Tb, Tc, Te, Th, Ti, Tl, Tm, Tu, Ty, t, tb, ti, tn, tr, TQ] (1874) [lt:en:MORFOLOGIK_RULE_EN_US]">t$</span> (cf Equation \ref{eq-n-spikes}). Note that these values do not employ matrix notation, but concern</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">individual neurons. Next, in order to find the convergence point of the ODE, we set the left side of the equation to $0$</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">and to solve it:</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">  0                        &amp; = -g_l^{dend} \  v_j^{dend} + \sum_i W_{ji} \    r_i \ \Delta t \\</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">  g_l^{dend} \  v_j^{dend} &amp; = W_{ji} \    r_i \ \Delta t</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">The desired dendritic potential of rate neurons is $v_j^{dend} = \sum_i W_{ji} \ r_i$, which occurs on both sides of the</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">above equation. Assuming that our dendritic model fulfills this equality, both terms to drop out from the equation.</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline">Thus, the correct parametrization for the dendritic leakage conductance remains:</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">  g_l^{dend} &amp; = \Delta t</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">It was shown experimentally that for <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [high, light, right, eight, fight, might, night, gift, height, lift, sight, tight, Right, highs, rift, bight, sift, haft, heft, wight] (2421) [lt:en:MORFOLOGIK_RULE_EN_US]">hight</span> spike frequencies, this <span class="highlight" title="Do not mix variants of the same word ('parameterization' and 'parametrization') within a single text.. Suggestions: [parametrization] (2451) [lt:en:EN_WORD_COHERENCY]">parameterization</span> leads to an exact match of dendritic</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">potentials between the neuron models. It will therefore be assumed as the default throughout all experiments where</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">spiking neurons are used. \newline</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">In order to keep the NEST models as similar as possible, rate neurons evolve according to the same dynamics. Like in the</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline">original implementation, dendrites of rate neurons are fully defined by their inputs at time $t$. This <span class="highlight-spelling" title="Possible spelling mistake. 'behaviour' is British English.. Suggestions: [behavior] (2878) [lt:en:MORFOLOGIK_RULE_EN_US]">behaviour</span> is</div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [achieved] (2891) [lt:en:MORFOLOGIK_RULE_EN_US]">acheived</span> by setting the leakage conductance to $1$ for all dendritic compartments. During network initialization,</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">dendritic leakage <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (3021) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span> are set to either one of these values depending on the type of neuron model employed.</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline"><span class="keyword1">\section</span>{Plasticity in feedback connections}<span class="keyword1">\label</span>{sec-feedback-plast}</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">\todo{move to results}</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">Within the present model, Pyramidal-to-pyramidal feedback weights evolve according to:</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">  \dot{w}_{l}^{down} &amp; = \eta_l^{down} \ ( \phi(u_l^{P}) - \phi(w_l^{down} r_{l+1}^P) )\ \phi(u_{l+1}^{P})^T</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">The error term in this case differs slightly from the others, but could arguably still be implemented by biological</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">neurons. An intuitive way to interpret the error term is as the difference between somatic activity and the activity of</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">a distant apical compartment that is innervated only by superficial pyramidal neurons. Within the NEST implementation,</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">this distal compartment leaks into the proximal apical compartment ($v^{api}$) with a conductance of $g^{api,dist}=1$.</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">The separation of pyramidal neuron apical dendrites into a proximal and a distal tree is well documented \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (3821) [lt:en:MORFOLOGIK_RULE_EN_US]">citeme</span>. A</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">difference between plasticity mechanisms for synapses arriving at these two integration zones is plausible, although I</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">was unable to find prior research supporting this type of plasticity \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (4020) [lt:en:MORFOLOGIK_RULE_EN_US]">citeme</span>.  A more sophisticated model of the apical</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">tree which resembles pyramidal neurons more closely could be a desirable extension to the model.</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">While the plasticity was successfully implemented in all variants of the model, it did not prove useful for training the</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">networks during my tests. A strong indicator to the reason behind this is the fact, that the dendritic error for this </div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline">rule is nonzero, even in the self-predicting state (cf. Fig. \ref{fig-error-comp-le}). Making these connections non-plastic led to the best learning performance, and is therefore</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">assumed as the default for all training simulations. This matches the previous implementations of this network too,</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">which typically set learning rates of these connections to $0$ <span class="highlight" title="Consider using 'except' or 'except for'. Suggestions: [except, except for] (4744) [lt:en:WITH_THE_EXCEPTION_OF]">with the exception of</span> a few experiments employing</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">steady-state approximations. Note that feedback information is transmitted through fixed weights in this case.</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline">Feedforward weights in turn learn to match these, meaning that the network effectively implements a type of Feedback</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">alignment \cite{Lillicrap2014}.</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline"><span class="keyword1">\section</span>{Presentation times and Latent Equilibrium}<span class="keyword1">\label</span>{sec-appendix-t-pres}</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">Exactly matching parameters and the training environment to those of existing implementations turned out to be a</div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">significant challenge. Particularly the way NEST handles signal transmissions made and exact numerical replication of</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">results impossible, as discussed in Section \todo{talk about timing differences}. In order to validate, that</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_3_numpy}</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">  <span class="keyword1">\caption</span>{Replication of Fig. \ref{fig-bars-le-snest} using a slightly modified version of the python code from</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">    \cite{Haider2021}. Resulting performance matches the original results closely, showing that this version can serve</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">    as a baseline for comparing performance of the NEST implementation to the original results.}</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">  <span class="keyword1">\label</span>{fig-bars-le-numpy}</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_3_rnest}</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline">  <span class="keyword1">\caption</span>{Replication of Fig. \ref{fig-bars-le-snest} using networks of rate neurons in the NEST simulator. A notable</div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">    difference to the python implementation in Fig. \ref{fig-bars-le-numpy} is, that this version does not handle very</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline">    low presentation times as well. This can likely be traced back to the synaptic delay enforced by NEST, which imposes</div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">    an upper bound on network relaxation time. Besides that, performance of the two variants is very similar.}</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline">  <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-bars-le-rnest is never referenced in the text [sh:figref]">fig-bars-le-rnest</span>}</div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline">&nbsp;</div><div class="clear"></div>
</div>
<h2 class="filename">01_introduction.tex</h2>

<p>Found 93 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Introduction}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{Motivation}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">The outstanding learning capabilities of the human brain have been found to be elusive and <span class="highlight" title="Consider using 'yet' or 'as yet'.. Suggestions: [yet, as yet] (120) [lt:en:AS_OF_YET]">as of yet</span> impossible to fully</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">explain or replicate in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [silicic] (174) [lt:en:MORFOLOGIK_RULE_EN_US]">silicio</span>. While the power of classical machine learning solutions for some tasks has improved</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">even beyond human capabilities in recent years, these approaches cannot serve as an adequate model of human cognition.</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline"><span class="highlight" title="If the text is a generality, 'of the' is not necessary.. Suggestions: [Some] (386) [lt:en:SOME_OF_THE]">Some of the</span> reasons brains and machines appear irreconcilable relate to questions about network structure and neuron</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">models. Yet more pressingly, almost <span class="highlight" title="Consider using 'all the'.. Suggestions: [all the] (539) [lt:en:ALL_OF_THE]">all of the</span> most powerful artificial neural networks are trained with the</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (612) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors algorithm, which is largely considered to be impossible for neurons to implement. Hence,</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">Neuroscience has dismissed this algorithm in an almost dogmatic way for many years after its development, stating that</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">the brain must employ a different mechanism to learn.</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">Yet in recent years, there has been a resurgence of attempts by neuroscientists towards reconciling biological and</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">artificial neural networks despite these issues. This led to a number of experimental results indicating that</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">brains might be capable of performing the impossible - using <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (1187) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors to learn. Furthermore, despite</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">rigorous efforts, no unifying alternative to this learning principle was found which performs well enough to account</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">for the brain's vast capabilities.</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">Hence, there is a vibrant <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [community] (1423) [lt:en:MORFOLOGIK_RULE_EN_US]">communitiy</span> developing  novel concepts for implementing this algorithm - or some approximation</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">of it. These novel approaches are capable of replicating an increasing number of properties of biological brains.</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">Nevertheless, many problems remain unsolved, and a lot of neuronal features remain unaccounted for in brain models that</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">are capable of any kind of learning. It is this open <span class="highlight" title="This word has been used in one of the immediately preceding sentences. Using a synonym could make your text more interesting to read, unless the repetition is intentional.. Suggestions: [issue, concern, difficulty] (1804) [lt:en:EN_REPEATEDWORDS]">problem</span>, to which I want to dedicate my efforts in this thesis.</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">After reviewing the existing literature, I have selected a promising model of learning in cortical circuits. This model</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">uses multi-compartment neuron models and local plasticity rules to implement a variant of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (2078) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span>. In this</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">project, I will investigate and attempt to further improve its concordance with data on the human neocortex. I will use</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">the approach of computationally modelling the model while increasingly adding biological features, attempting to retain</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">learning performance in the process.</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline"><span class="keyword1">\section</span>{The <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (2386) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors algorithm}</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">The <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (2427) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors algorithm (<span class="keyword1">\textit</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (2464) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop}</span>) \citep{werbos1982} is the workhorse of modern machine</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">learning and is able to outperform humans on a growing number of tasks \citep{LeCun2015}. Particularly for training deep</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">neural networks it has remained popular and largely unchanged since its initial development. Its learning potential</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">stems from its unique capability to attribute errors in the output of a network to activations of specific neurons and</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">connections within its hidden layers. This property also forms the basis of the algorithm's name; After an initial</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">forward pass to form a prediction about the nature of a given input, a separate backward pass propagates the arising</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">error through all layers in reverse order. During this second network traversal, local error gradients dictate to what</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">extent a given weight needs to be altered so that the next presentation of the same sample would elicit a lower error in</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">the output layer. It has been argued that through this mechanism, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (3393) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> solves the <span class="keyword1">\textit</span>{credit assignment problem}</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">- <span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>the question about to what degree a parameter contributes to an error signal - optimally \citep{Lillicrap2020}. </div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">With this critical information in hand, computing parameter changes that decrease error becomes almost trivial. Thus,</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">it is naturally desirable find a solution to the credit assignment which likewise explains how the brain updates its</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">parameters. This  </div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline"><span class="keyword1">\section</span>{Concerns over biological plausibility}</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">While <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (3841) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> continues to prove exceptionally useful in conventional machine learning systems, it is viewed critically</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">by many neuroscientists. For one, it relies on a slow adaptation of synaptic weights, and therefore requires a large</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">amount of examples to learn rather simple input-output mappings. In this particular way, its performance is far inferior</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">to the powerful one-shot learning exhibited by humans \citep{Brea2016}. Yet more importantly, no plausible mechanisms</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">have yet been found by which biological neural networks could implement this algorithm. In fact, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (4396) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> as an</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">algorithm by which brains may learn has been dismissed entirely by much of the neuroscience community for decades</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">\citep{Grossberg1987,Crick1989,Mazzoni1991,OReilly1996}. This dismissal is often focussed on three mechanisms that are</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">instrumental for the algorithm \citep{whittington2019theories,Bengio2015,Liao2016}:</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline"><span class="keyword1">\subsection</span>{Local error representation}</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">Neuron-specific errors in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (4685) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> are computed and propagated by a mechanism that is completely detached from the</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">network itself, which requires access to the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [entirety] (4819) [lt:en:MORFOLOGIK_RULE_EN_US]">entirity</span> of the network state. In order to compute the weight changes for a</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">given layer, the algorithm takes as an input the activation and synaptic weights of all downstream neurons. In contrast,</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">plasticity in biological neurons is largely considered to be primarily dependent on factors that are local to the</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">synapse \citep{Abbott2000,magee2020synaptic,urbanczik2014learning}. While <span class="highlight-spelling" title="Possible spelling mistake found. (5149) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromodulators</span> are known to influence</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">synaptic plasticity, their dispersion is too wide to communicate neuron-specific errors. Thus, biologically plausible</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (5306) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> would require a method for encoding errors locally, <span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>close to the neurons to which they relate. This has</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">been perhaps the strongest criticism of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (5464) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> in the brain, as many <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [questions] (5495) [lt:en:MORFOLOGIK_RULE_EN_US]">qestions</span> regarding biological mechanisms for both</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">computing and storing these errors remain unanswered <span class="highlight" title="Consider using 'yet' or 'as yet'.. Suggestions: [yet, as yet] (5598) [lt:en:AS_OF_YET]">as of yet</span>.</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline"><span class="keyword1">\subsection</span>{The weight transport problem}</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">During the weight update stage of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (5674) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>, errors are transmitted between layers with the same weights that are used in</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">the forward pass. In other words, the magnitude of a neuron-specific error that is propagated through a given connection</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">should be proportional to its impact on output loss during the forward pass. For this to work, a neuronal network</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">implementing <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (6009) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> would require feedback connections that mirror both the connectivity and synaptic weights of the</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">forward connections. Bidirectional connections that could theoretically back-propagate errors are common in the cortex,</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">yet it is unclear by which mechanism pairs of synapses would be able to align. This issue becomes particularly apparent</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">when considering long-range pyramidal projections, in which feedforward and feedback synapses would potentially be</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">separated by a considerable distance.</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline"><span class="keyword1">\subsection</span>{Neuron models}</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">Finally, the types of artificial neurons typically used in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (6583) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> transmit a continuous scalar activation at all</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">times, instead of discrete spikes. In theory, these activations correspond to the firing rate of a spiking neuron,</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">giving this class of models the title <span class="keyword1">\textit</span>{rate neurons}. Yet spike based communication requires more sophisticated</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">neuron models. Additionally, plasticity rules for rate neurons do not necessarily have an easily derived counterpart for</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">spiking neurons. A notable example for this issue is <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7038) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> itself; The local error gradient of a neuron is not</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">trivial to compute for Spiking neural networks (SNN), as a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (7158) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> has no natural derivative. Furthermore, a given</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">neuron's activation in classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7250) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> is computed from a simple weighted sum of all inputs. This fails to capture</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">the complex <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [collinearities] (7347) [lt:en:MORFOLOGIK_RULE_EN_US]">nonlinearities</span> of dendritic integration that are fundamental to cortical neurons</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">\citep{Gerstner2009,sjostrom2008dendritic,Eyal2018}. Finally, these abstract neurons - at least in classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7489) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> -</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">have no persistence through time. Thus, their activation is dictated strictly by the presentation of a single stimulus,</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">in contrast to the leaky membrane dynamics exhibited by biological neurons.\newline</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">Additional concerns regarding <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7735) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> will be discussed in Section<span class="highlight" title="Don't put a space before the full stop.. Suggestions: [.] (7772) [lt:en:COMMA_PARENTHESIS_WHITESPACE]"></span> \todo{}.</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">section</span>{Overcoming biological implausibility}</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">Despite these issues, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7837) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> has remained the gold standard against which most attempts at modelling learning in the</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">brain eventually have to compare. Also, despite its apparent biological implausibility, it does share some notable</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">parallels to learning in the brain. Artificial neural networks (ANN) trained with <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (8131) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> have been shown to develop</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">similar representations to those found in brain areas responsible for comparable tasks</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">\citep{Yamins2016,Whittington2018,KhalighRazavi2014,Kubilius2016}. Thus, numerous attempts have been made to define more</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">biologically plausible learning rules which approximate <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (8369) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> to some degree. A full review of the available</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">literature would be out of scope for this thesis, so only a few examples will be discussed in this section.</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">One approach to solve the issues around local error representations is, to drive synaptic plasticity through a global</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">error signal <span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (8666) [lt:en:MORFOLOGIK_RULE_EN_US]">citem</span>e</span>. The appeal of this solution is that such signalling could be plausibly performed by</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (8758) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromodulators</span> like dopamine \citep{Mazzoni1991,Seung2003,izhikevich2007solving}. These types solutions to not</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">approximate <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (8834) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>, but instead lead to a kind of reinforcement learning. While some consider this the most plausible</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">way for brains to learn, performance of global error/reward signalling stays far behind that of the exact credit</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">assignment performed in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (9079) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. Additionally, this class of algorithms requires even more examples of a training</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">dataset, and was shown to scale poorly with network size \citep{Werfel2003}. Two prominent classes of algorithms</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">encode errors in either activation changes over time \citep{}</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">The weight transport problem was successfully <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [addressed, dressed, a dressed] (9373) [lt:en:MORFOLOGIK_RULE_EN_US]">adressed</span> by a mechanism called <span class="keyword1">\textit</span>{Feedback Alignment} (FA)</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">\citep{Lillicrap2014}. This seminal paper shows that <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (9463) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors can still learn successfully when</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">feedback weights are random. In addition to learning to represent an input-output mapping in forward weights,</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (9633) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> is capable of training the network to extract information from randomly weighted instructive pathways.</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">The authors call this process <span class="keyword1">\textit</span>{learning to learn} and show that learning performance is even <span class="highlight" title="The comparative 'superior' is usually followed by 'to'.. Suggestions: [superior to] (9843) [lt:en:SUPERIOR_THAN_TO]"></span>superior than</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (9867) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> for some tasks. This mechanism was further expanded to show that the principles of FA perform very</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">well when biologically plausible plasticity rules are employed \citep{Liao2016,Zenke2018}. Another popular line of</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">thought is - instead of computing local errors - to compute optimal activations for hidden layer neurons using</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (10178) [lt:en:MORFOLOGIK_RULE_EN_US]">autoencoders</span> \citep{Bengio2014,Lee2015,Ahmad2020}. Approaches derived from this do not suffer from the weight transport</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">problem, and by design does not require local error representations. While these solutions promise to solve the weight</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">transport problem, on more complex benchmark datasets like</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline"><span class="keyword1">\textit</span>{\href{https://www.cs.toronto.edu/~kriz/cifar.html}{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [CIGAR, CIFAD, CIF AR] (10443) [lt:en:MORFOLOGIK_RULE_EN_US]">CIFAR}}</span> and</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline"><span class="keyword1">\textit</span>{\href{https://www.image-net.org/index.php}{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Imagine, Imagined, Magnet, Imagines, Imogene] (10453) [lt:en:MORFOLOGIK_RULE_EN_US]">ImageNet}}</span> both of them fall far behind traditional <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (10503) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>Backprop</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">\citep{Bartunov2018}.</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">Numerous approaches for implementing <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (10555) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> in more plausible neuron models exist, most of which employ variants of</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">the <span class="keyword1">\textit</span>{Leaky Integrate-and-fire} (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [IF, LIFE, LIE, LIFT, LID, LIP, LIN, LIZ, AIF, LEIF, LIB, LIFO, RIF, LIT, LIQ, BIF, CIF, DIF, FIF, GIF, HIF, IIF, LBF, LCF, LDIF, LEF, LFI, LGF, LI, LIA, LIH, LIM, LIR, LIX, LOF, LPF, LRF, LSF, LTF, LUF, MIF, NIF, PIF, WIF, ZIF, LIEF, LII, L IF, LI F, L&amp;F, LDF, LF, LIC, LIU, LIV, TIF] (10666) [lt:en:MORFOLOGIK_RULE_EN_US]">LIF</span>) neuron \citep{Sporea2013,Lee2016,Bengio2017,Lee2020}. The aforementioned</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">issue of computing the derivative over <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike trains] (10741) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrains</span> has been solved in <span class="highlight" title="Consider using 'several'.. Suggestions: [several] (10772) [lt:en:NUMEROUS_DIFFERENT]">several different</span> ways, with the most prominent</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">variant perhaps being <span class="keyword1">\textit</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SuperS pike] (10842) [lt:en:MORFOLOGIK_RULE_EN_US]">SuperSpike}</span> \citep{Zenke2018}. One might therefore view this as the weakest criticism</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">aimed at <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (10922) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. Yet none of the employed neuron models come close to portraying the intricacies of biological</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">neurons, and thus fail to provide explanations for their complexity.\newline</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">All of these approaches successfully solve one or more concerns of biological plausibility, while still approximating</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (11222) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> to some degree. Yet none of them are able to solve all three concerns, and some of them even rely on novel</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">mechanisms that are themselves biologically questionable. It further appears that in all but a few cases, an increase in</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">biological plausibility leads to a decrease in performance. Thus, whether <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (11533) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> could be implemented or approximate</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">by biological neurons remains an open question.</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline"><span class="keyword1">\subsection</span>{Dendrites as computational elements}</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">The issue of oversimplified neuron models is by far the most frequent to be <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [limited, committed, omitted, commuted, Olmsted, orbited, vomited, summited] (11740) [lt:en:MORFOLOGIK_RULE_EN_US]">ommited</span> from explanations of the biological</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">implausibility of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (11802) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> (See for example \citep{Meulemans2020,Lillicrap2014}). This disregard might stem from the</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">fact that rate-based point neurons are employed in many of the most powerful artificial neural networks. This</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">observation might be taken as an argument that the simple summation of synaptic inputs is sufficient for powerful and</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">generalized learning. Modelling neurons more closely to biology would <span class="highlight" title="Did you maybe mean 'buy' or 'be'?. Suggestions: [buy, be] (12167) [lt:en:BY_BUY]">by</span> this view only increase mathematical complexity</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">and computational cost without practical benefit. Another hypothesis states that the dominance of point neurons stems</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">from a <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span><span class="highlight-spelling" title="Possible spelling mistake found. (12344) [lt:en:MORFOLOGIK_RULE_EN_US]">somato-centric</span> perspective<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> within neuroscience \citep{Larkum2018}, which stems from the technical challenges</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">inherent to studying dendrites in vivo. The vastly different amount of available data regarding these two neuronal</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">components might have induced a bias in how neurons are modelled computationally. Some researchers have even questioned</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">whether dendrites should be seen as more of a 'bug' than a 'feature' \citep{Haeusser2003}, <span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>a biological necessity</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">which needs to be overcome and compensated for.</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">Yet in recent years, with novel mechanisms of dendritic computation being discovered, interest in researching and</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">explicitly modelling dendrites has increased. Particularly the vast dendritic branches of pyramidal neurons found in the</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">cerebral cortex, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [hippocampus] (13077) [lt:en:MORFOLOGIK_RULE_EN_US]">hyppocampus</span> and amygdala, were shown to perform complex integrations of their synaptic inputs</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">\citep{spruston2008pyramidal}. They recently have been shown to be capable of performing computations, which were</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">previously assumed to require multi-layer networks \citep{Schiess2016,Gidon2020}. The size of dendritic trees is also</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">known to discriminate regular spiking from burst firing pyramidal neurons \citep{Elburg2010}. \todo{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Expand] (13430) [lt:en:UPPERCASE_SENTENCE_START]"></span>expand}</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">(See \citep{Larkum2022} and \citep{Poirazi2020} for extensive reviews). These neuroscientific insights have also sparked</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">hope that modelling dendritic compartments explicitly might aid machine learning in terms in both learning and</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">efficiency \citep{Chavlis2021,guerguiev2017towards,Richards2019,Eyal2018}. It appears then that, if not for</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">computational gains, dendrites might be critical for any model that attempts to explain the power of human learning.</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">While the network discussed here is concerned with rather simple multi-compartment models, the choice of model was</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">strongly influenced by the recent excitement about dendrites.</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline"><span class="comment"><span class="comment">% \arrayrulecolor{white} % &lt;--- {\renewcommand{\arraystretch}{1.45} <span class="keyword2">\begin{table}</span>[t] \resizebox{\textwidth}{!}{%</span></span></div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline"><span class="comment"><span class="comment">% <span class="keyword2">\begin{tabular}</span>{|ll|ll|ll|} \hline \rowcolor[HTML]{B3B3B3} \multicolumn{2}{|l|}{\cellcolor[HTML]{B3B3B3}} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{2}{l|}{\cellcolor[HTML]{B3B3B3}Temporal-error model} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{2}{l|}{\cellcolor[HTML]{B3B3B3}Explicit-error model}                  \\ \cline{3-6}</span></span></div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline"><span class="comment"><span class="comment">% \rowcolor[HTML]{B3B3B3} \multicolumn{2}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{B3B3B3}}} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{1}{l|}{\cellcolor[HTML]{B3B3B3}Contrastive learning} &amp; Continuous update &amp;</span></span></div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{1}{l|}{\cellcolor[HTML]{B3B3B3}Predictive coding} &amp; Dendritic error \\ \hline \rowcolor[HTML]{D9D9D9}</span></span></div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Control signal} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{FE0000} Required}} &amp; {\color[HTML]{FE0000} Required} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00} Not required}} &amp; {\color[HTML]{32CB00} Not required}</span></span></div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline"><span class="comment"><span class="comment">% \\ \hline \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Connectivity} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00} Unconstrained}} &amp; {\color[HTML]{32CB00}</span></span></div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline"><span class="comment"><span class="comment">% Unconstrained} &amp; \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{FE0000} Constrained}}  &amp;</span></span></div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline"><span class="comment"><span class="comment">% {\color[HTML]{FE0000} Constrained} \\ \hline \rowcolor[HTML]{D9D9D9}</span></span></div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Propagation time} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00} L-1}} &amp; {\color[HTML]{32CB00} L-1} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline"><span class="comment"><span class="comment">% \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{FE0000} 2L-1}} &amp; {\color[HTML]{32CB00} L-1} \\</span></span></div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline"><span class="comment"><span class="comment">%         \hline \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Pre-training} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00} Not required}} &amp; {\color[HTML]{32CB00} Not</span></span></div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline"><span class="comment"><span class="comment">%         required} &amp; \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00} Not required}} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline"><span class="comment"><span class="comment">%         {\color[HTML]{FE0000} Required} \\ \hline \rowcolor[HTML]{D9D9D9}</span></span></div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Error encoded in} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Difference in activity \\ between</span></span></div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline"><span class="comment"><span class="comment">%         separate                           \\ phases<span class="keyword2">\end{tabular}</span>}        &amp; <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Rate of change</span></span></div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline"><span class="comment"><span class="comment">%         of \\ activity<span class="keyword2">\end{tabular}</span>                                     &amp;</span></span></div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Activity of specialised \\</span></span></div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline"><span class="comment"><span class="comment">%                                                         neurons<span class="keyword2">\end{tabular}</span>}               &amp;</span></span></div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline"><span class="comment"><span class="comment">%         <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Apical dendrites of \\ pyramidal neurons<span class="keyword2">\end{tabular}</span> \\</span></span></div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline"><span class="comment"><span class="comment">%         \hline \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Data accounted for} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Neural responses \\ and behaviour in</span></span></div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline"><span class="comment"><span class="comment">%         a\\</span></span></div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline"><span class="comment"><span class="comment">%                                                         variety of tasks<span class="keyword2">\end{tabular}</span>} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline"><span class="comment"><span class="comment">%         <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Typical spike-time- \\ dependent plasticity<span class="keyword2">\end{tabular}</span> &amp;</span></span></div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Increased neural \\ activity to\\</span></span></div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline"><span class="comment"><span class="comment">%                                                         unpredicted stimuli<span class="keyword2">\end{tabular}</span>}        &amp;</span></span></div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline"><span class="comment"><span class="comment">%         <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Properties of \\</span></span></div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline"><span class="comment"><span class="comment">%           pyramidal neurons<span class="keyword2">\end{tabular}</span> \\</span></span></div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline"><span class="comment"><span class="comment">%         \hline \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}MNIST performance} &amp;</span></span></div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}$\sim$2-3} &amp; - &amp;</span></span></div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline"><span class="comment"><span class="comment">%         \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}$\sim$1.7} &amp; $\sim$1.96 \\ \hline <span class="keyword2">\end{tabular}</span>% }<span class="keyword1">\caption</span>{</span></span></div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline"><span class="comment"><span class="comment">%         Comparison between some leading biologically plausible approximations of Backprop, adapted from</span></span></div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline"><span class="comment"><span class="comment">%         \cite{whittington2019theories}. From left to right: Contrastive hebbian learning \citep{OReilly1996},</span></span></div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline"><span class="comment"><span class="comment">%         Contrastive learing with continuous update \citep{Bengio2017}, Predictive Coding</span></span></div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline"><span class="comment"><span class="comment">%         \citep{Whittington2017,rao1999predictive}, Dendritic error network \citep{sacramento2018dendritic}. All</span></span></div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline"><span class="comment"><span class="comment">%         algorithms were selected due to them reflecting some properties of biological brains, some of which are</span></span></div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline"><span class="comment"><span class="comment">%         highlighted in the row "Data accounted for". To do this, all of them need to make concessions. In the first</span></span></div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline"><span class="comment"><span class="comment">%         few rows, desirable properties are highlighted in green, while undesirable traits are highlighted in red.}</span></span></div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline"><span class="comment"><span class="comment">%         <span class="keyword2">\end{table}</span></span></span></div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline"><span class="comment"><span class="comment">% }</span></span></div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline"><span class="keyword1">\section</span>{Cortical microcircuits}</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">Another feature of the brain which is often not considered in (biologically plausible) machine learning models is its</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">intricate connectivity. This is quite understandable, as there is still some uncertainty about which parts of the brain</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">are involved in generalized learning. It is also unclear, to what level of detail they need to be modeled. It has been</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">shown that the connectivity patterns of cortical circuits are superior to amorphous networks in some cases</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">\citep{haeusler2007statistical}, so there might be a computational gain from modeling network structure closer to</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">biology. The question over network structure goes hand in hand with the choice of neuron models, as synaptic connections</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">arrive at specific points of pyramidal neuron dendrites, depending on the origin of the connection</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline">\citep{felleman1991distributed,Ishizuka1995,Larkum2018}.</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">Several theories of cortical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [function] (14818) [lt:en:MORFOLOGIK_RULE_EN_US]">funcition</span> focus more on reinforcement \citep{Legenstein2008} or unsupervised learning</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline">\citep{George2009,haeusler2017}. Without dismissing these theories, this thesis will adopt the viewpoint that human</div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">brains require a form of gradient descent to successfully adapt to their <span class="highlight" title="This word is normally spelled with a hyphen.. Suggestions: [ever-changing] (15046) [lt:en:EN_COMPOUNDS]">ever changing</span> environments. Furthermore, we</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline">shall assume for now that this kind of learning occurs predominantly in the neocortex \citep{Marblestone2016}.</div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline">While many important exceptions have been published recently, the literature on the subject of learning historically</div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline">appears to be somewhat split. On the one hand, the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>machine-learning<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> point of view largely considers the utility of</div><div class="clear"></div>
<div class="linenb">252</div><div class="codeline">added network complexity first, with considerations of biology appearing as an afterthough<span class="highlight-sh" title="Add a space before citation or reference. [sh:c:001]">t\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (15508) [lt:en:MORFOLOGIK_RULE_EN_US]">cite</span>me</span>. On the other hand,</div><div class="clear"></div>
<div class="linenb">253</div><div class="codeline">intricate models of cortical circuits exist, which can so far not be trained to perform tasks</div><div class="clear"></div>
<div class="linenb">254</div><div class="codeline">\cite{potjans2014cell,schmidt2018multi,van2022bringing}. Within this thesis, I hope to contribute to the body of</div><div class="clear"></div>
<div class="linenb">255</div><div class="codeline">literature between those extremes. For this, my approach will be to select a learning model that is already highly</div><div class="clear"></div>
<div class="linenb">256</div><div class="codeline">biologically plausible, and to attempt to improve its plausibility - without fully breaking the learning rule.</div><div class="clear"></div>
<div class="linenb">257</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">258</div><div class="codeline"><span class="keyword1">\section</span>{Model selection}</div><div class="clear"></div>
<div class="linenb">259</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">260</div><div class="codeline">The model selection progress was strongly influenced by a review article on biologically plausible approximations of</div><div class="clear"></div>
<div class="linenb">261</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (16051) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> \citep{whittington2019theories}. The authors narrow the wide range of proposed solution down to four algorithms</div><div class="clear"></div>
<div class="linenb">262</div><div class="codeline">that are both highly performant and largely biologically plausible. The algorithms are united by requiring minimal</div><div class="clear"></div>
<div class="linenb">263</div><div class="codeline">external control, and by the fact that they can all be described within a common framework of energy minimization</div><div class="clear"></div>
<div class="linenb">264</div><div class="codeline">\citep{Scellier2017}. The first two models are Contrastive learning \cite{OReilly1996}, and its extension to</div><div class="clear"></div>
<div class="linenb">265</div><div class="codeline">time-continuous updates \citep{Bengio2017}. Both of these encode neuron-specific errors in the change of neural activity</div><div class="clear"></div>
<div class="linenb">266</div><div class="codeline">over time. One of their appeals is the fact that they rely on <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Serbian, Lesbian, Debian, Hessian] (16618) [lt:en:MORFOLOGIK_RULE_EN_US]">Hebbian</span> and <span class="highlight-spelling" title="Possible spelling mistake found. (16630) [lt:en:MORFOLOGIK_RULE_EN_US]">Anti-Hebbian</span> plasticity. Yet in the plasticity</div><div class="clear"></div>
<div class="linenb">267</div><div class="codeline">rule also lies their greatest weakness, as synapses need to switch between the two once the target for a given stimulus</div><div class="clear"></div>
<div class="linenb">268</div><div class="codeline">is provided. This switch requires a global signal that communicates the change in state to all neurons in the network</div><div class="clear"></div>
<div class="linenb">269</div><div class="codeline">simultaneously.</div><div class="clear"></div>
<div class="linenb">270</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">271</div><div class="codeline">The second class of models was more appealing to me, as both variants are based on the predictive coding account in</div><div class="clear"></div>
<div class="linenb">272</div><div class="codeline">Neuroscience \citep{rao1999predictive}, which deserves an introduction.</div><div class="clear"></div>
<div class="linenb">273</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">274</div><div class="codeline"><span class="keyword1">\subsection</span>{Predictive coding}</div><div class="clear"></div>
<div class="linenb">275</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">276</div><div class="codeline">In this seminal model of processing in the visual cortex, each level of the visual hierarchy represents the outside</div><div class="clear"></div>
<div class="linenb">277</div><div class="codeline">world at some level of abstraction. Recurrent connections then serve communicate prediction errors and predictions up</div><div class="clear"></div>
<div class="linenb">278</div><div class="codeline">and down the hierarchy respectively, which the network attempts to reconcile. The authors showed that through rather</div><div class="clear"></div>
<div class="linenb">279</div><div class="codeline">simple computations, these prediction errors can be minimized to obtain useful representations at each level of the</div><div class="clear"></div>
<div class="linenb">280</div><div class="codeline">hierarchy. They further showed that a predictive coding network trained on natural images exhibits end-stopping</div><div class="clear"></div>
<div class="linenb">281</div><div class="codeline">properties previously found in mammalian visual cortex neurons. This work was instrumental to shaping the modern</div><div class="clear"></div>
<div class="linenb">282</div><div class="codeline">neuroscientific perspective of perception and action as a unified process. The extension of predictive coding principles</div><div class="clear"></div>
<div class="linenb">283</div><div class="codeline">from visual processing to the entire living system is promising to revolutionize neuroscience under the name of</div><div class="clear"></div>
<div class="linenb">284</div><div class="codeline"><span class="keyword1">\textit</span>{Active inference} \citep{Friston2008,Friston2009,Adams2015}. By this view, the entire brain aims to minimize</div><div class="clear"></div>
<div class="linenb">285</div><div class="codeline">prediction errors with respect to an internal (generative) model of the world. A noteworthy property of this hypothesis</div><div class="clear"></div>
<div class="linenb">286</div><div class="codeline">is that it implies an agents action in the world as 'just another' way in which it can decrease discrepancies between</div><div class="clear"></div>
<div class="linenb">287</div><div class="codeline">its beliefs and sensory information. In a seminal paper, a model of the cortical microcircuit</div><div class="clear"></div>
<div class="linenb">288</div><div class="codeline">\citep{haeusler2007statistical} was shown to have a plausible way for performing the computations required by predictive</div><div class="clear"></div>
<div class="linenb">289</div><div class="codeline">coding \citep{bastos2012canonical}.</div><div class="clear"></div>
<div class="linenb">290</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">291</div><div class="codeline">While predictive coding was originally described for unsupervised learning, through a slight modification it is also</div><div class="clear"></div>
<div class="linenb">292</div><div class="codeline">capable of performing supervised learning \citep{Whittington2017}. This is the third model considered in the review</div><div class="clear"></div>
<div class="linenb">293</div><div class="codeline">paper, in which values (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>activations) and errors of a layer are encoded in separate, recurrently connected nodes. By</div><div class="clear"></div>
<div class="linenb">294</div><div class="codeline">employing only local <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Serbian, Lesbian, Debian, Hessian] (18906) [lt:en:MORFOLOGIK_RULE_EN_US]">Hebbian</span> plasticity, this network is capable of approximating <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (18967) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> in multilayer perceptrons</div><div class="clear"></div>
<div class="linenb">295</div><div class="codeline">while conforming to the principles of predictive coding. The constraint on network topology was further relaxed by</div><div class="clear"></div>
<div class="linenb">296</div><div class="codeline">showing that the model is capable of approximating <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (19168) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> for arbitrary computation graphs \citep{Millidge2022}. The</div><div class="clear"></div>
<div class="linenb">297</div><div class="codeline">neuron-based predictive coding net was therefore an important contribution towards unifying the fields of Active</div><div class="clear"></div>
<div class="linenb">298</div><div class="codeline">inference and machine learning research. As noted in a recent review article:</div><div class="clear"></div>
<div class="linenb">299</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">300</div><div class="codeline"><span class="keyword2">\begin{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Quotation] (19411) [lt:en:UPPERCASE_SENTENCE_START]"></span>quotation}</span></div><div class="clear"></div>
<div class="linenb">301</div><div class="codeline">  Since predictive coding is largely biologically plausible, and has many potentially plausible process theories, this</div><div class="clear"></div>
<div class="linenb">302</div><div class="codeline">  close link between the theories provides a potential route to the development of a biologically plausible alternative to</div><div class="clear"></div>
<div class="linenb">303</div><div class="codeline">  <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [backdrop, back prop] (19665) [lt:en:MORFOLOGIK_RULE_EN_US]">backprop</span>, which may be implemented in the brain. Additionally, since predictive coding can be derived as a variational</div><div class="clear"></div>
<div class="linenb">304</div><div class="codeline">  inference algorithm, it also provides a close and fascinating link between <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [back propagation] (19861) [lt:en:MORFOLOGIK_RULE_EN_US]">backpropagation</span> of error and variational</div><div class="clear"></div>
<div class="linenb">305</div><div class="codeline">  inference. \citep{millidge2021predictive}</div><div class="clear"></div>
<div class="linenb">306</div><div class="codeline"><span class="keyword2">\end{quotation}</span></div><div class="clear"></div>
<div class="linenb">307</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">308</div><div class="codeline">With this in mind, we turn to the final model discussed in the review paper.</div><div class="clear"></div>
<div class="linenb">309</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">310</div><div class="codeline"><span class="keyword1">\subsection</span>{The Dendritic error model}</div><div class="clear"></div>
<div class="linenb">311</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">312</div><div class="codeline">The predictive coding network stores local prediction errors in nodes (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>neurons) close to the nodes to which these</div><div class="clear"></div>
<div class="linenb">313</div><div class="codeline">errors relate. That errors may be represented within the activation of individual neurons is a promising hypothesis with</div><div class="clear"></div>
<div class="linenb">314</div><div class="codeline">some <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [advantages] (20280) [lt:en:MORFOLOGIK_RULE_EN_US]">advandages</span>, as well as results backing it up \citep{Hertaeg2022}. Yet there is a competing view, by which errors</div><div class="clear"></div>
<div class="linenb">315</div><div class="codeline">elicited by individual neurons may be stored in their dendritic compartments \citep{guerguiev2017towards}. The</div><div class="clear"></div>
<div class="linenb">316</div><div class="codeline"><span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>Dendritic error model<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> \citep{sacramento2018dendritic} - as the name implies - follows this line of thought. It</div><div class="clear"></div>
<div class="linenb">317</div><div class="codeline">contains a highly recurrent network of both pyramidal- and interneurons, in which pyramidal neuron apical dendrites</div><div class="clear"></div>
<div class="linenb">318</div><div class="codeline">encode prediction errors. This view is supported by behavioral rodent experiments which show that stimulation to</div><div class="clear"></div>
<div class="linenb">319</div><div class="codeline">pyramidal neuron apical tufts in cortical layer 1 controls learning \citep{Doron2020}.</div><div class="clear"></div>
<div class="linenb">320</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">321</div><div class="codeline">For the errors to be encoded successfully, the model requires a symmetry between feedforward and feedback sets of</div><div class="clear"></div>
<div class="linenb">322</div><div class="codeline">weights, which it has to learn prior to training. After that, apical compartments behave like the error nodes in a</div><div class="clear"></div>
<div class="linenb">323</div><div class="codeline">predictive coding network. They are silent during a feedforward network pass, and encode local prediction errors in</div><div class="clear"></div>
<div class="linenb">324</div><div class="codeline">their membrane potential when a target is applied to the output layer. Since they are a part of the pyramidal neuron,</div><div class="clear"></div>
<div class="linenb">325</div><div class="codeline">only local information is required to minimize these prediction errors through a plasticity rule for multi-compartment</div><div class="clear"></div>
<div class="linenb">326</div><div class="codeline">neurons \citep{urbanczik2014learning}. A critical observation made in</div><div class="clear"></div>
<div class="linenb">327</div><div class="codeline">\citep{whittington2019theories} is that the dendritic error model is mathematically equivalent to their</div><div class="clear"></div>
<div class="linenb">328</div><div class="codeline">predictive coding network \todo{expand if I have time, otherwise this will be a ref<span class="highlight" title="Two consecutive dots. Suggestions: [., â€¦] (21630) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> All of these factors combined</div><div class="clear"></div>
<div class="linenb">329</div><div class="codeline">make the dendritic error model a promising model to help us further understand both energy minimization and deep</div><div class="clear"></div>
<div class="linenb">330</div><div class="codeline">learning in cortical circuits. While both the employed neuron and connectivity model are far behind some of the more</div><div class="clear"></div>
<div class="linenb">331</div><div class="codeline">rigorous cortical simulations, it can be considered an important step towards integrating deep learning and neuroscience</div><div class="clear"></div>
<div class="linenb">332</div><div class="codeline">\citep{Marblestone2016}.</div><div class="clear"></div>
<div class="linenb">333</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">334</div><div class="codeline">Nevertheless, the model still suffers from some constraints with regard to its biological plausibility; Both the</div><div class="clear"></div>
<div class="linenb">335</div><div class="codeline">predictive coding network and the dendritic error network require strongly constrained connectivity schemes, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [without, whiteout] (22242) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>whithout</div><div class="clear"></div>
<div class="linenb">336</div><div class="codeline">which they cannot learn. This kind of specificity (in particular one-to-one relationships between pairs of neurons) are</div><div class="clear"></div>
<div class="linenb">337</div><div class="codeline">highly untypical for cortical connections \citep{Thomson2003}. Hence, their exact network architectures are unlikely to</div><div class="clear"></div>
<div class="linenb">338</div><div class="codeline">be present in the cortex. The Dendritic error model additionally requires Pre-training to be capable of approximating</div><div class="clear"></div>
<div class="linenb">339</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (22593) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. Both of these issues will be discussed in this thesis. Yet the most salient improvement to the network's</div><div class="clear"></div>
<div class="linenb">340</div><div class="codeline">biological plausibility is likely, to change neuron models from rate-based to spiking neurons. It has been shown that</div><div class="clear"></div>
<div class="linenb">341</div><div class="codeline">the Plasticity rule employed by the network is capable of performing simple learning tasks when adapted to spiking</div><div class="clear"></div>
<div class="linenb">342</div><div class="codeline">neurons \citep{Stapmanns2021}. Yet, (to the best of my knowledge) there are no studies investigating if this variant is</div><div class="clear"></div>
<div class="linenb">343</div><div class="codeline">capable of learning tasks on a network-level. A spiking implementation of the dendritic error network will therefore be</div><div class="clear"></div>
<div class="linenb">344</div><div class="codeline">the starting point for this thesis, upon which further analysis shall build.</div><div class="clear"></div>
<div class="linenb">345</div><div class="codeline">&nbsp;</div><div class="clear"></div>
</div>
<h2 class="filename">04_discussion.tex</h2>

<p>Found 55 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 4 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{Discussion}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 5 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span></span></span>{Contribution}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline"><span class="highlight-sh" title="A section title should start with a capital letter. [sh:001]"></span>\<span class="highlight-sh" title="This subsection is very short (about 138 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">subsection</span></span>{criteria for biological plausibility}</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">In which we discuss, to what extent the network conforms to the criteria for biologically plausible learning rules</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">introduced <span class="highlight-sh" title="Do not use 'by [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">by \cite{</span>Whittington2017}:</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline"><span class="keyword2">\begin{enumerate}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">      <span class="keyword1">\item</span> Local computation. A neuron performs computation only on the basis</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">            of the inputs it receives from other neurons weighted by the strengths</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">            of its synaptic connections.</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">      <span class="keyword1">\item</span>  Local plasticity. The amount of synaptic weight modification is dependent on only the activity of the two</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">            neurons the synapse connects (and possibly a <span class="highlight-spelling" title="Possible spelling mistake found. (570) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromodulator</span>).</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">      <span class="keyword1">\item</span>  Minimal external control. The neurons perform the computation autonomously with as little external control</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">            routing information in different ways at different times as possible.</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">      <span class="keyword1">\item</span>   Plausible architecture. The connectivity patterns in the model should</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">            be consistent with basic constraints of connectivity in neocortex.</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline"><span class="keyword2">\end{enumerate}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline"><span class="keyword1">\section</span>{Limitations}</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">Network needs to be reset between stimuli</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">original does not do that, in NEST it's <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [kind of] (1041) [lt:en:MORFOLOGIK_RULE_EN_US]">kindof</span> a big deal.</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline"><span class="keyword1">\subsection</span>*{Efficiency}</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">exposure time and training set still quite large</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline"><span class="keyword1">\subsection</span>*{Neuron model}</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">non-resetting, non-refractory</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline"><span class="keyword1">\subsection</span>*{Network}</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">Oversized networks have a higher tendency of failing on some tasks (results not shown). This issue is not consistent,</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">but plagued some of my experiments with no satisfactory explanation yet found.</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"> When injecting large currents (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [fun, fund, funk, UNC, FNC, FLNC] (1416) [lt:en:MORFOLOGIK_RULE_EN_US]">func</span> approx experiments), the network kinda <span class="highlight" title="This word is considered offensive. (1460) [lt:en:PROFANITY]">shits</span> itself. Somewhat</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">expected, somewhat annoying</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 2 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span>{Future directions}</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">\<span class="highlight-sh" title="This subsection is very short (about 136 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">subsection</span></span>{Additional <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (1545) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> concerns}</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">from <span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\citep{</span>Marblestone2016} on one-shot learning</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">Additionally, the nervous system may have a way of quickly</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">storing and replaying sequences of events. This would allow</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">the brain to move an item from episodic memory into a long-</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">term memory stored in the weights of a cortical network (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Hi, I, Jim, Jr, Jo, GI, JC, JD, JP, Pi, JM, Jib, Jig, AI, AJI, BJI, Bi, CI, Ci, DI, DJI, Di, EI, FI, FJI, HI, J, JA, JAI, JE, JIC, JIM, JIT, JNI, JO, JRI, JT, JU, JV, LI, Li, MI, NI, Ni, QI, RI, SI, Si, TI, Ti, UI, VI, WI, Xi, ZI, Ii, Jg, Mi, Oi, Vi, J i, JB, JCI, JJ, JK, JS, Jin, Jio, Yi, Ai] (1832) [lt:en:MORFOLOGIK_RULE_EN_US]">Ji</span> and</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">Wilson, 2007), by replaying the memory over and over. This</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">solution effectively uses many iterations of weight updating to</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">fully learn a single item, even if one has only been exposed to</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">it once. Alternatively, the brain could rapidly store an episodic</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">memory and then retrieve it later without the need to perform</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">slow gradient updates, which has proven to be useful for</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">fast reinforcement learning in scenarios with limited available</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">data (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Blunder, Blunders, Bluebell, Blondel, Lindell] (2281) [lt:en:MORFOLOGIK_RULE_EN_US]">Blundell</span> et al., 2016)</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">Where do targets come from? \cite{Bengio2015}</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 31 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span>{Should it be considered pre-training?}</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">\todo{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Someone] (2378) [lt:en:UPPERCASE_SENTENCE_START]">someone</span> said this network needs pre-training and that made me sad<span class="highlight-sh" title="There should not be a space before a punctuation mark. If in your language, typographic rules require a space here, LaTeX takes care of inserting it without your intervention. [sh:d:005]"> :</span><span class="highlight" title="Don't put a space after the opening parenthesis.. Suggestions: [(] (2445) [lt:en:COMMA_PARENTHESIS_WHITESPACE]"></span>(}</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"><span class="keyword1">\section</span>{Relation to energy minimization}</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 91 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span></span></span>{Correspondence of the final model to cortical circuitry}</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">Cortical neurons depend on <span class="highlight-spelling" title="Possible spelling mistake found. (2566) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromodulation</span> too \cite{Roelfsema2018}</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">Completely unclear whether cortical circuits perform supervised learning \citep{magee2020synaptic}</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">This would probs be super efficient on <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (2709) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphics</span>!</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">I am not going to try plasticity with spike-spike or spike-rate dendritic errors</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">Training on <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Imagine, Imagined, Magnet, Imagines, Imogene] (2819) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>ImageNet</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">Making this <span class="highlight" title="This word is considered offensive. (2841) [lt:en:PROFANITY]">shit</span> faster</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PTT, BPT, BTT, BITT, BUTT, B PTT, BP TT, BPT T] (2854) [lt:en:MORFOLOGIK_RULE_EN_US]">BPTT</span>/time-continuous inputs</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">The dendritic error rule at the core of <span class="highlight-spelling" title="Possible spelling mistake found. (2924) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> looks absurdly similar to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [super spike, supers pike] (2965) [lt:en:MORFOLOGIK_RULE_EN_US]">superspike</span> \citep{Zenke2018}.</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">Someone should look into that!</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">Different configurations for which synapses are plastic should be elaborated on.</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline"><span class="highlight-sh" title="A section title should start with a capital letter. [sh:001]"></span>\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This subsection is very short (about 84 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsection</span></span>{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Improvements] (3096) [lt:en:UPPERCASE_SENTENCE_START]">improvements</span> to the neuron models}</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">\todo{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Talk] (3132) [lt:en:UPPERCASE_SENTENCE_START]">talk</span> about <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [AIF, CALIF, ALI, ALF, AVIF, ARIF, ASIF] (3143) [lt:en:MORFOLOGIK_RULE_EN_US]">ALIF</span> neurons \citep{Gerstner2009} also reffed in</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">e-pro<span class="highlight-sh" title="Add a space before citation or reference. [sh:c:001]">p\cite</span>p{bellec2019biologically,bellec2020solution}. <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Look] (3186) [lt:en:UPPERCASE_SENTENCE_START]">look</span> for sources}</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [per, PR, par, pyre, PMR, PVR, pry, ppr, AYR, BYR, MYR, PAR, PBR, PCR, PER, PFR, PHR, PLR, PNR, POR, PPR, PQR, PRR, PSR, PTR, PUR, PWR, PYF, PYG, PYT, Pr, Pym, RYR, SYR, pr, pyx, yr, p yr, PDR] (3204) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr</span> and <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [into, inn, INT, INTG, int, in tn, INTJ, ITN] (3212) [lt:en:MORFOLOGIK_RULE_EN_US]">intn</span> are <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [way, ways, wary, wavy, waxy, AYY] (3221) [lt:en:MORFOLOGIK_RULE_EN_US]">wayy</span> to similar</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [IF, LIFE, LIE, LIFT, LID, LIP, LIN, LIZ, AIF, LEIF, LIB, LIFO, RIF, LIT, LIQ, BIF, CIF, DIF, FIF, GIF, HIF, IIF, LBF, LCF, LDIF, LEF, LFI, LGF, LI, LIA, LIH, LIM, LIR, LIX, LOF, LPF, LRF, LSF, LTF, LUF, MIF, NIF, PIF, WIF, ZIF, LIEF, LII, L IF, LI F, L&amp;F, LDF, LF, LIC, LIU, LIV, TIF] (3238) [lt:en:MORFOLOGIK_RULE_EN_US]">LIF</span>, membrane reset and $t_{ref}$</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Reward-modulated] (3265) [lt:en:UPPERCASE_SENTENCE_START]">reward-modulated</span> <span class="highlight-spelling" title="Possible spelling mistake found. (3282) [lt:en:MORFOLOGIK_RULE_EN_US]">urbanczik-senn</span> plasticity?</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Wtf] (3310) [lt:en:UPPERCASE_SENTENCE_START]">wtf</span> is shunting inhibition?</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">Consider neurogenesis deeper. Any network that is plausible must be able to develop in a plausible fashion. Investigating</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">how the cortex develops might hold insights into plausible connectivity schemes. This does not necessarily compete or</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">conflict with looking at connectivity of developed brains</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline"><span class="highlight-sh" title="A section title should start with a capital letter. [sh:001]"></span>\<span class="highlight-sh" title="This subsubsection is very short (about 54 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsubsection</span>{improve prospective activity with regard to spike creation}</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Le] (3699) [lt:en:UPPERCASE_SENTENCE_START]">le</span> does a lot, particularly at hidden layers. Yet response time under spiking paradigms is still</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">lackluster. In particular, prospective activation does next to nothing for these very low input time</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">constants. SNN performs best when $\tau_x$ is greater than $\Delta_t$ by roughly x10.</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 0 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This section is very short (about 0 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span>{Conclusion}</div><div class="clear"></div>
</div>
<h2 class="filename">thesis.tex</h2>

<p>Found 27 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline"><span class="keyword1">\documentclass</span>[11pt,a4paper,titlepage]{report}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline"><span class="keyword1">\usepackage</span>[english]{babel}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline"><span class="keyword1">\usepackage</span>[utf8]{inputenc}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline"><span class="keyword1">\usepackage</span>[table,xcdraw]{xcolor}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline"><span class="keyword1">\usepackage</span>{graphicx}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline"><span class="keyword1">\usepackage</span>{subcaption}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">\graphicspath{ {./images/} }</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline"><span class="keyword1">\usepackage</span>[font=small,labelfont=bf]{caption}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline"><span class="keyword1">\usepackage</span>{listings}</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline"><span class="keyword1">\usepackage</span>{amsmath}</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline"><span class="keyword1">\usepackage</span>{amssymb}</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline"><span class="keyword1">\usepackage</span>{multirow}</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline"><span class="keyword1">\usepackage</span>[round]{natbib}</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">\bibliographystyle{apalike}</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline"><span class="keyword1">\usepackage</span>[onehalfspacing]{setspace}</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline"><span class="keyword1">\usepackage</span>{etoolbox}</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">\AtBeginEnvironment{quote}{\par\singlespacing\small}</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline"><span class="keyword1">\usepackage</span>[top=100pt,bottom=100pt,left=75pt,right=75pt]{geometry}</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline"><span class="keyword1">\usepackage</span>{enumitem}</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline"><span class="keyword1">\usepackage</span>{hyperref}</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline"><span class="keyword1">\usepackage</span>[noabbrev]{cleveref}</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">\DeclareSymbolFont{letters}{OML}{ztmcm}{m}{it}</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">\DeclareSymbolFontAlphabet{\mathnormal}{letters}</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">\pagestyle{headings}</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">\hypersetup{</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">    colorlinks=true,</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">    linkcolor=black,</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">    citecolor=black,</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">    urlcolor=blue,</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">}</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">\newcommand*\ttvar[1]{\texttt{\expandafter\dottvar\detokenize{#1}\relax}}</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">\newcommand*\dottvar[1]{\ifx\relax#1\else</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">  \expandafter\ifx\string_#1\string_\allowbreak\else#1\fi</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">  \expandafter\dottvar\fi}</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">\definecolor{codegreen}{rgb}{0,0.6,0}</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">\definecolor{codegray}{rgb}{0.5,0.5,0.5}</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">\definecolor{codepurple}{rgb}{0.58,0,0.82}</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">\definecolor{backcolour}{rgb}{0.95,0.95,0.92}</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">\lstdefinestyle{mystyle}{</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">    backgroundcolor=\color{backcolour},   </div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">    commentstyle=\color{codegreen},</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">    keywordstyle=\color{magenta},</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">    numberstyle=\tiny\color{codegray},</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">    stringstyle=\color{codepurple},</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">    basicstyle=\ttfamily\footnotesize,</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">    breakatwhitespace=false,         </div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">    breaklines=true,                 </div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">    captionpos=b,                    </div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">    keepspaces=true,                 </div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">    numbers=left,                    </div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">    numbersep=5pt,                  </div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">    showspaces=false,                </div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">    showstringspaces=false,</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">    showtabs=false,                  </div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">    tabsize=2</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">}</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">\lstset{style=mystyle}</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">\newcommand{\what}[1] {\textcolor{red}{<span class="keyword1">\textbf</span>{#1} \addcontentsline{toc}{subsection}{\textcolor{orange}{#1}}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">\newcommand{\todo}[1] {\textcolor{orange}{<span class="keyword1">\textbf</span>{TODO: #1}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">\newcommand{\citeme}{\textcolor{orange}{<span class="keyword1">\textbf</span>{TODO: cite}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">\newcommand{\phrasing}{\textcolor{green}{<span class="keyword1">\textbf</span>{phrasing}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">\newcommand{\intro}{\textcolor{blue}{<span class="keyword1">\textbf</span>{introduce term?}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">\newcommand{\image}[3][1]</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">{</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">	\centerline{<span class="keyword1">\includegraphics</span>[width={1\linewidth}]{#1}}</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">	<span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{#2</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">	<span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure #3 is never referenced in the text [sh:figref]">#3</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">}</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline"><span class="keyword2">\begin{document}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline"><span class="keyword1">\title</span>{<span class="keyword1">\textbf</span>{<span class="highlight-spelling" title="Possible spelling mistake found. (1) [lt:en:MORFOLOGIK_RULE_EN_US]">Philipps-UniversitÃ¤t</span> Marburg}}</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">\author{Johannes <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Grille, Gill, Gills, Lille, Gillie, Gilles, Gilly, Guille] (40) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>Gille}</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">\date{\parbox{\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [line width] (47) [lt:en:MORFOLOGIK_RULE_EN_US]">linewidth}{</span>\centering<span class="comment">%</span></div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found. (71) [lt:en:MORFOLOGIK_RULE_EN_US]">Fachbereich</span> 17\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (86) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">    AG <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Allegiance] (101) [lt:en:MORFOLOGIK_RULE_EN_US]">Allgemeine</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [UND, and, end, fund, UNC, undo, CND, DND, HND, UNB, Ind, 2nd, AND, BND, END, GND, LND, ND, Nd, OND, PND, SND, TND, UAD, UCD, UED, UFD, UMD, UN, UNF, UNH, UNI, UNL, UNM, UNR, UNT, UNV, UPD, URD, USD, VND, ind, uni, DnD, IND, UHD, UID, UNDP, UNO, in, on, an, any, had, one, under, used, band, did, found, no, up, us, use, UK, land, led, old, round, run, June, find, hand, mid, red, runs, sound, unit, CD, DVD, God, add, aid, bad, bound, ends, funds, gun, guns, kind, mind, send, tend, wind, ad, bed, bid, bond, fed, fun, god, odd, punk, sand, sun, sung, tune, Andy, CNN, HD, Land, Ltd, PhD, RNA, aunt, hung, hunt, ink, lung, mud, pond, pound, rod, sued, sunk, ups, wound, Bond, Hunt, ID, LCD, LED, SD, TNA, TNT, Ted, UHF, USA, USB, UTC, UV, bend, bind, dad, fond, funk, hid, hind, id, inn, kid, lend, lid, mad, mod, mound, nun, nuns, nut, pad, pod, pun, punt, rid, sad, tuned, unto, Ana, BNP, BSD, Bud, CCD, CNS, DNS, INS, JD, Jun, Jung, Juno, LSD, Luna, NAD, NP, PD, Rand, SNL, SNP, Sand, UA, UDP, URL, USDA, Unix, Urdu, ant, cod, dune, fend, junk, nod, nu, quad, quid, tuna, urn, wed, ASD, CNC, CNG, CNR, CNT, FD, GNP, HUD, Hung, Indy, Judd, LD, LNG, PNC, PNG, PNP, PPD, PSD, Pound, RCD, Sung, Suns, UAW, UCL, UDF, UE, UPC, UPI, UPS, USF, USO, USP, WMD, bud, bun, bunk, dunk, fad, hound, lad, med, mend, puns, rune, rung, sod, um, undue, urns, wand, AD, ADD, APD, BCD, BMD, BPD, CFD, CKD, CNE, CSD, DNR, ESD, Enid, FWD, GED, HDD, HNL, Hurd, IED, Jed, Lind, MCD, MHD, MNA, MVD, NPD, NY, PNA, PNL, PNS, QED, RFD, SBD, SNA, SNC, SVD, TLD, TNG, Tod, UCA, UDC, UDR, UNIDO, UUP, WD, buns, bunt, curd, dud, jun, rind, suns, uh, ulna, unwed, ACD, BNC, CNH, CNI, CVD, DDD, DMD, DPD, DSD, FNC, Fundy, GMD, IUD, Ina, Kurd, MNP, MNS, NDB, PGD, PNM, SNF, TDD, UAR, UNEF, USG, Uzi, VN, WNW, Zuni, cued, hued, hunk, inf, mung, rad, rand, undid, DCD, MLD, RNG, SGD, UAM, URB, USNO, Wed, Zn, bung, cad, cud, duns, hod, nub, puny, rend, runt, tad, TBD, bod, gnu, gunk, nus, rued, ted, tuns, ugh, unfed, upend, vend, ETD, YTD, Ann, DNA, GNU, MD, Ned, TD, UFO, USC, USS, pend, UNDRO, cpd, 1D, 2D, 3D, 3rd, 4D, 8D, AAD, ABD, AED, AFD, AGD, AHD, AID, AKD, ALD, AMD, AN, ANB, ANC, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANQ, ANR, ANS, ANT, ANV, ANW, ANX, ANY, ANZ, AOD, ARD, ATD, AUD, AUN, AWD, AXD, AYD, AZD, BBD, BD, BDD, BED, BFD, BGD, BHD, BID, BKD, BLD, BNA, BNB, BNE, BNF, BNG, BNI, BNJ, BNK, BNL, BNM, BNN, BNO, BNQ, BNR, BNS, BNT, BNU, BNV, BNW, BNY, BNZ, BOD, BRD, BUD, BVD, BWD, BXD, BZD, Bend, CAD, CBD, CDD, CED, CHD, CID, CLD, CMD, CN, CNA, CNB, CNDP, CNDS, CNED, CNF, CNJ, CNK, CNL, CNM, CNO, CNP, CNQ, CNU, CNV, CNW, CNX, CNY, CNZ, COD, CPD, CRD, CUD, Cd, Cid, Cod, D, D2D, DBD, DD, DGD, DID, DN, DNB, DNF, DNK, DNL, DNT, DOD, DUD, DZD, Dunn, EAD, EDD, EED, EGD, EID, ELD, EMD, EN, ENC, ENE, ENF, ENG, ENH, ENI, ENL, ENM, ENP, ENS, ERD, EUN, Ed, Eng, FCD, FED, FFD, FID, FJD, FN, FNA, FNB, FNE, FNG, FNO, FNQ, FNR, FNS, FNT, FOD, FPD, FRD, FSD, FUD, Fed, GAD, GDD, GN, GNC, GNF, GNT, GNV, GRD, GYD, Gd, HKD, HMD, HNE, HNF, HNR, HNS, HPD, Hun, Huns, IAD, ICD, IFD, IMD, IN, INA, INB, INC, INE, INED, INF, ING, INH, INM, INP, INR, INT, IPD, IRD, ISD, IVD, In, Inc, JED, JLD, JMD, JNA, JNE, JNI, JNT, JWD, KED, KN, KNA, KNB, KNK, KNM, KNO, KNU, KNX, KOD, KPD, KTD, KWD, LBD, LDD, LLD, LMD, LN, LN2, LNA, LNB, LNE, LNH, LNI, LNO, LNR, LNT, LPD, LRD, LTD, LVD, LYD, Ln, MAD, MDD, MED, MEND, MGD, MID, MKD, MN, MN4, MNB, MNE, MNG, MNK, MNL, MNT, MNW, MOND, MPD, MSD, MTD, Md, Mn, N, NB, NC, NDF, NDH, NDK, NDR, NDS, NDT, NE, NED, NF, NG, NH, NI, NID, NJ, NK, NL, NLD, NM, NNI, NNN, NO, NR, NS, NT, NTD, NU, NV, NW, NZ, NZD, Na, Nb, Ne, Ni, No, Np, OAD, OCD, OD, ODD, OED, OGD, ON, ONB, ONE, ONF, ONG, ONM, ONP, ONS, ONT, ONU, ORD, OSD, Ono, Ont, PCD, PDD, PED, PFD, PHD, PID, PKD, PLD, PNB, PNE, PNF, PNH, PNI, PNK, PNN, PNR, PNT, PNUD, PNV, PNW, PRD, PUN, PUPD, PVD, Pd, QDD, QNE, QNH, QNJ, RAD, RBD, RD, RED, RHD, RJD, RKD, RLD, RN, RN6, RNB, RNE, RNF, RNN, RNO, RNR, RNS, RNT, RNU, RPD, RTD, RUD, RUN, RWD, Rd, Rn, Rod, SAD, SCD, SDD, SHD, SID, SN, SNB, SNE, SNG, SNI, SNR, SNS, SNU, SNV, SOD, SRD, STD, SUD, SYD, Sid, Sn, Sun, TAD, TCD, TED, TGD, THD, TID, TKD, TMD, TN, TNB, TNC, TNE, TNF, TNK, TNL, TNM, TNN, TNO, TNP, TNR, TNS, TSD, TTD, TUN, TWD, Tad, U, U10, U11, U12, U13, U14, U15, U16, U17, U18, U19, U20, U21, U22, U23, U30, U31, U32, U37, U38, U39, U40, U41, U42, U43, U44, U45, U46, U47, U49, U50, U51, U54, U55, U56, U57, U58, U59, U60, U61, U62, U63, U64, U65, U66, U67, U68, U69, U70, U71, U72, U73, U74, U75, U76, U77, U78, U79, U80, U83, U87, U96, U99, UAA, UAB, UAC, UAE, UAG, UAH, UAI, UAL, UAP, UAS, UAU, UB4, UBB, UBC, UBS, UBV, UBX, UC, UCAD, UCB, UCC, UCF, UCM, UCU, UDB, UDDM, UDL, UDM, UDS, UEC, UEM, UER, UES, UET, UFA, UFC, UFE, UFF, UFG, UFI, UFP, UFR, UFS, UFW, UG, UGA, UGB, UGC, UGE, UGG, UGI, UGM, UGS, UGT, UGU, UGX, UHCD, UI, UIA, UIC, UIG, UIL, UIM, UINL, UIO, UIP, UIR, UIT, UIZ, UJA, UJM, UKR, UL, ULC, ULG, ULK, ULM, ULR, ULT, ULX, UMA, UMB, UMF, UMH, UMI, UML, UMM, UMP, UMR, UMS, UMT, UNAC, UNAF, UNIL, UNIS, UNIT, UNIX, UNODC, UNPO, UNSA, UO, UO2, UOB, UP, UP1, UPA, UPB, UPE, UPF, UPK, UPN, UPP, UPR, UPU, UPV, UQC, UQO, UQR, URA, URE, URG, URI, URN, URS, URT, URV, URY, US, USE, USI, USJ, USN, USR, UT, UT1, UTA, UTF, UTG, UTL, UTP, UTV, UUA, UUB, UUC, UUG, UUID, UUO, UUS, UUU, UVA, UVB, UVF, UVM, UVT, UWA, UWC, UWP, UWW, UY, UYF, UZ, UZB, Ufa, Ut, Ute, VD, VDD, VGD, VHD, VNC, VNE, VNM, VNU, VRD, VSD, WAD, WNS, WNT, WSD, X2D, XCD, XNA, YHD, ZAD, ZED, ZNP, ang, ans, d, dd, dun, dung, ed, en, enc, ens, fwd, gad, inc, ins, int, kn, ltd, mun, n, nude, pd, pud, rd, std, tn, tun, turd, u, uhf, ult, ump, unbid, unis, univ, usu, wad, wend, yd, yid, zed, 'n', A&amp;D, BYD, Bundy, D&amp;D, DNC, DNI, DNP, DTD, DoD, ECD, ENA, ENT, EOD, FHD, FMD, FNL, FTD, GID, GNI, GTD, GUID, HRD, HWD, IBD, INI, JVD, KD, L&amp;D, LOD, Lundy, MNC, NAND, NCD, NDA, NDC, NDP, NHD, NWD, ONR, PMD, PN, Pune, QLD, Quid, R&amp;D, RNC, RNZ, Rudd, SPD, SSD, Syd, TNW, UAN, UAT, UAV, UBA, UCI, UCP, UCR, UCS, UCSD, UDA, UHC, UIs, ULB, UMC, UNAM, UNSC, UNSW, UOM, UOS, UPT, UPnP, USDT, UTI, UTM, UVC, UW, UX, Udo, Ulm, Urs, VVD, VoD, XD, XSD, ZD, ZuÃ±i, bundy, cmd, env, gtd, kN, nm, nÃ©, uhh, uhm, umm, undos, vid] (112) [lt:en:MORFOLOGIK_RULE_EN_US]">und</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Biological, Biologist, Biologists, Biologic, Biologics] (116) [lt:en:MORFOLOGIK_RULE_EN_US]">Biologische</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Psychologies, Psychologic, Psychologize] (128) [lt:en:MORFOLOGIK_RULE_EN_US]">Psychologie</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (140) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">    AE <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Theoretical, Theoretic] (155) [lt:en:MORFOLOGIK_RULE_EN_US]">Theoretische</span> <span class="highlight-spelling" title="Possible spelling mistake found. (168) [lt:en:MORFOLOGIK_RULE_EN_US]">Kognitionswissenschaft</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (191) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (204) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (217) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">    Learning in cortical microcircuits with multi-compartment pyramidal neurons\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (305) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">    <span class="keyword1">\textbf</span>{Supervisors:}\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (330) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (343) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">    Prof. Dr. Dominik <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Endures, Andres, Endues, End res, AndrÃ©s] (373) [lt:en:MORFOLOGIK_RULE_EN_US]">Endres</span>, <span class="highlight-spelling" title="Possible spelling mistake found. (381) [lt:en:MORFOLOGIK_RULE_EN_US]">Philipps-UniversitÃ¤t</span> Marburg\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (410) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (423) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">    Dr. Johan <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Without] (445) [lt:en:MORFOLOGIK_RULE_EN_US]">Kwisthout</span>, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Radioed, Redbud] (456) [lt:en:MORFOLOGIK_RULE_EN_US]">Radboud</span> University}}</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline"><span class="keyword1">\maketitle</span></div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found. (478) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>tableofcontents</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">\include{01_introduction.tex}</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">\include{02_methods.tex}</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">\include{03_results.tex}</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">\include{04_discussion.tex}</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">\include{05_appendix.tex}</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">\include{06_notes.tex}</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">\bibliography{bib/library.bib}</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline"><span class="keyword2">\end{document}</span></div><div class="clear"></div>
</div>
<h2 class="filename">02_methods.tex</h2>

<p>Found 189 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 3 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{Methods}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{The dendritic error model}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">This section will go into detail about the dendritic error network \citep{sacramento2018dendritic}. The model contains a</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">somewhat complex and strongly recurrent connectivity, which poses one of the major criticisms aimed at it</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">\citep{whittington2019theories}. Much like traditional machine learning networks, it can be functionally separated into</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">layers. Yet in this particular model, input- hidden- and output layers are quite distinct in both neuron populations and</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">connectivity.</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline"><span class="keyword1">\subsection</span>{Network architecture}</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[t]</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">  <span class="keyword2">\begin{minipage}</span>{0.5\textwidth}</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{network_a}</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">  <span class="keyword2">\end{minipage}</span>\hfill</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">  <span class="keyword2">\begin{minipage}</span>{0.4\textwidth}</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{network_b}</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">  <span class="keyword2">\end{minipage}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">  <span class="keyword1">\caption</span>{Network structure, <span class="highlight-sh" title="Do not use 'from [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">from <span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\cite{</span></span>Haider2021}. <span class="keyword1">\textbf</span>{a:} pyramidal- (red) and interneurons (blue) in a network</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">    of three layers. Note the fact that the number interneurons in a layer is equal to the number of pyramidal neurons</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">    in the subsequent layer\protect\footnotemark. <span class="keyword1">\textbf</span>{b:} connectivity within the highlighted section. Feedback</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">    pyramidal-to-interneuron connections (displayed with rectangular synapse) transmit pyramidal somatic potential</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">    directly and connect to a single interneuron. This enables these interneurons to learn to match their corresponding</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">    next-layer pyramidal neurons. All other synapses (circles) transmit the neuron somatic activation $\phi (u^{som})$</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">    and fully connect their origin and target populations.}</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">  <span class="keyword1">\label</span>{fig-network}</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">\footnotetext{{Note that the input layer is displayed as having interneurons here. This appears to be a mistake in the</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">      Figure. Within the implementation, interneurons are only modelled in hidden layers.}}</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">The basic connectivity scheme of the Model is shown in Fig. \ref{fig-network}. Neurons at the input layer receive no</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">feedback signals and serve primarily to apply a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [temporal slow-pass, temporal low-pass, temp orals low-pass] (833) [lt:en:MORFOLOGIK_RULE_EN_US]">temporals low-pass</span> filter to the stimulus which is injected directly</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">into their membrane.  Hidden layers consist of a pyramidal- and an interneuron population, which are fully connected to</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">each other reciprocally. Both types of neurons are represented by multi-compartment neuron models with leaky membrane</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">dynamics. Interneurons contain one somatic and one dendritic compartment, while pyramidal neurons are modeled with both</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">a basal and an apical dendrite.  Feedforward connections between layers are facilitated by all-to-all connections</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">between their respective pyramidal neurons and innervate basal compartments. Feedback connections from superficial</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">pyramidal neurons, as well as lateral interneuron connections arrive at the apical compartments of pyramidal neurons.</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">Thus, a hidden layer pyramidal neuron forms two reciprocal loops, one with all interneurons in the same layer, and one</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">with all pyramidal neurons in the next layer.</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">Interneurons receive feedback information from superficial pyramidal neurons in addition to their lateral connections.</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">These feedback connections are unique in this model, as they connect one pyramidal neuron to exactly one interneuron.</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">Instead of transmitting a neuronal activation as all other connections do, these connections relay somatic voltage</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">directly. This one-to-one connectivity puts a strict constraint on the number of interneurons in a hidden layer, as it</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">must be equal to the number of subsequent pyramidal neurons. These pairs of inter- and pyramidal neurons will henceforth</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">be called <span class="keyword1">\textit</span>{sister <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [neurons] (2382) [lt:en:MORFOLOGIK_RULE_EN_US]">neruons}</span>. The top-down signal serves to <span class="keyword1">\textit</span>{nudge} interneuron somatic activation towards</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">that of their pyramidal sisters. The purpose of an interneuron in this architecture is then, to predict the activity of</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">its sister neuron. Any failure to do so results in layer-specific errors which in turn are the driving force of learning</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">in this context, but more on this later.</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">Output layers have no interneurons, and are usually modeled as pyramidal neurons without an apical compartment. During</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">learning, the target for the network's activation is injected into their somatic compartment. Through the feedback</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">connections, it can propagate through the entire network. To understand what purpose this rather complex connectivity</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">scheme serves in our model, neuron models and plasticity rules require some elaboration.</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline"><span class="keyword1">\subsection</span>{Neuron models}<span class="keyword1">\label</span>{sec-neurons}</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">The network contains two types of multi-compartment neurons; Pyramidal neurons with three compartments each, and</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">interneurons with two compartments each. They integrate synaptic inputs into dendritic potentials, which in turn leak</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (3449) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> with specific <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (3468) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span>. Note that vector notation will be used throughout this section, and $u_l^P$</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">and $u_l^I$ denote the column vectors of pyramidal- and interneuron somatic voltages at layer $l$, respectively.</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">Synaptic weights $W$ are likewise assumed matrices of size $n \times m$, which are the number of output and input</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">neurons of the connected populations respectively. The activation (or rate) $r_l^P$ of pyramidal neurons is given by</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">applying the neuronal transfer function $\phi$ <span class="highlight" title="Possible typo: you repeated a word. Suggestions: [to] (3911) [lt:en:ENGLISH_WORD_REPEAT_RULE]">to to</span> their somatic potentials $u_l^P$:</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">  r_l^P   &amp; = \phi(u_l^P)                                                                      \\</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">  \phi(x) &amp; = <span class="keyword2">\begin{cases}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">                0                                   &amp; \textrm{if } \ x &lt; -\epsilon               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">                \gamma \ log(1+e^{\beta(x-\theta)}) &amp; \textrm{if } \ -\epsilon \leq x &lt; \epsilon \\</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">                \gamma \ x                          &amp; \textrm{otherwise}</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">              <span class="keyword2">\end{cases}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (3946) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $\phi$ acts <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [component wise] (3959) [lt:en:MORFOLOGIK_RULE_EN_US]">componentwise</span> on $u$ and can be interpreted as a smoothed variant of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Rely, Rel, ELU, REL, Re Lu, ReL u] (4026) [lt:en:MORFOLOGIK_RULE_EN_US]">ReLu</span> (sometimes called</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"><span class="keyword1">\textit</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Soft plus] (4049) [lt:en:MORFOLOGIK_RULE_EN_US]">Softplus}</span>) with scaling factors $\gamma=1$, $\beta=1$, $\theta=0$. Splitting the computation with the threshold</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">parameter $\epsilon=15$ does not alter its output much, but instead serves to prevent overflow errors for large absolute</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">values of $x$.</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">As mentioned before, pyramidal and interneurons are modeled as rate neurons with leaky membrane dynamics and multiple</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">compartments. Where applicable, they will be differentiated with superscripts $P$ and $I$ respectively. The basal and</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">apical dendrites of pyramidal neurons are denoted with superscripts <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [beys, BAS, was, as, has, base, gas, bad, bar, bass, bus, ban, bars, bay, bag, bat, bats, bias, Bass, SAS, bags, bays, BBS, bans, pas, IAS, OAS, bras, BDS, BMS, Boas, bash, BRS, abs, bask, bast, boas, bah, bps, BKS, baa, baas, AAS, As, BA, BAA, BAE, BAES, BAF, BAH, BAM, BAP, BAU, BAV, BAW, BAX, BAZ, BCS, BES, BFS, BGS, BHS, BIS, BJS, BLS, BNS, BOS, BPS, BS, BSS, BTS, BVS, BWS, BXS, BYS, BZS, Ba, EAS, GAS, Las, MAS, NAS, PAS, RAS, UAS, bap, baps, bey, bis, bxs, mas, b as, BAC, BASF, BJs, CAS, FAS, PAs, TAS, bam] (4557) [lt:en:MORFOLOGIK_RULE_EN_US]">$bas$</span> and <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [API, APA, APC, CPI, ape, apt, pi, APG, APS, PPI, UPI, APD, APM, Apr, DPI, RPI, APB, APN, Apia, IPI, app, Ali, AAI, ACI, AEI, AFI, AFPI, AGI, AHI, AI, AII, AJI, ANI, AOI, AP, APE, APF, APH, APII, APJ, APK, APL, APO, APP, APQ, APR, APRI, APT, APU, APV, APW, APX, APY, APZ, AQI, ARI, ASPI, ATI, AUI, AVI, AYI, AZI, BPI, DAPI, EPI, LPI, MAPI, MPI, NPI, OPI, SPI, TAPI, TPI, VPI, dpi, a pi, ABI, ADI, AMI, APIs, APs, Abi, KPI, ai, a, and, I, as, at, an, any, are, all, age, up, air, art, April, act, anti, pay, acid, add, ago, aid, am, arm, eye, eyes, map, paid, pair, rapid, Asia, FBI, PA, PC, ad, ask, axis, cap, gap, hip, lap, maps, mph, pain, spin, tape, tip, AFC, CPU, Nazi, PM, ads, aka, arc, ash, caps, epic, laps, pan, pin, pit, pm, rap, rpm, ski, spy, taxi, ups, GPS, HP, PCI, PR, PT, RPG, Wei, ace, aft, akin, amid, apex, arid, ate, avid, cape, eyed, gaps, lip, opt, pH, pad, par, pie, pig, spa, tap, ACM, ACS, ADC, ADP, AF, ANSI, AOL, ATC, ATM, ATV, Alps, Ana, Audi, Axis, CGI, CPA, CPC, CPR, CSI, GI, GPA, GPL, GUI, IPA, IPO, JP, LAPD, MRI, Mali, NP, NPC, NPR, PD, PPP, PRI, PV, RPM, SAP, VP, WPA, Wii, ale, amp, ant, apes, apse, aria, aux, awe, dip, hi, pa, pas, ppm, psi, rip, sap, sci, spit, zip, AAU, ADR, AEC, AEG, AIF, AIG, AMX, APEC, ARL, ARP, ASB, ASD, Abe, Ada, Aug, Ava, BPA, Bali, CBI, CCI, DCI, DUI, EPP, FP, GPO, GPU, HPV, Hui, IPC, IPS, Kali, LPG, MPA, MPH, MPP, Mani, Mari, Maui, NPA, NPS, Napa, OSI, PJ, PKI, PPD, Pa, RPC, RPF, SGI, SPL, TDI, UPC, UPS, VPN, WWI, ah, amps, ark, avg, capo, maxi, moi, nape, ops, pal, paw, raps, taps, AAT, ABM, ACU, AD, ADD, ADT, AEF, AFM, AFN, AGC, AGS, AHS, AIT, ARF, ASN, Agni, Ara, BPD, BPM, CAI, CLI, CPE, CPM, CPO, CTI, Cali, Capri, DRI, DTI, EPG, FCI, FDI, FSI, GDI, GPC, Hopi, IAP, IPN, IPP, IPR, Kari, LSI, MDI, MPR, MPV, MSI, Magi, NPD, PMI, PW, RMI, RPA, RPO, RPR, RSI, SBI, SDI, SPF, TBI, TPA, TPC, TPM, WAP, aphid, asp, av, ax, aye, gape, mpg, nap, pat, phi, pic, pip, ppb, pt, sari, sip, wadi, ACD, ACG, ACH, AGR, AUT, AZT, Adm, Avis, BRI, BWI, CFI, CNI, CPF, DPD, DPT, DWI, Dali, EPL, FPA, HMI, Jami, LPM, LPO, LPP, PX, RCI, RPN, RPT, SMI, Saki, Tupi, Uzi, abs, aim, alps, amt, fps, kepi, kip, magi, mp, nip, obi, pail, pap, papa, tapir, vapid, AFO, DLI, DPN, DPs, EPF, FPO, FYI, Fri, GMI, GPM, GPV, HRI, LPN, MPE, OAP, TPL, TPN, Tami, VRI, adj, ado, aha, ain, alp, aping, aspic, awl, bani, bps, naps, okapi, pah, pk, rapt, saps, topi, wpm, zap, zaps, AHQ, Amie, Ariz, Lapp, MLI, adv, ail, aped, apps, carpi, hap, kph, opp, yip, APDU, Jpn, asps, lapin, tali, yap, yaps, AAA, ABC, AMC, ATP, Afr, Amy, Ann, IP, PS, RBI, apish, ave, cpl, dpt, jape, op, rps, cpd, ppr, A, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26, A27, A28, A29, A30, A31, A32, A33, A34, A35, A36, A37, A38, A39, A40, A41, A42, A43, A44, A45, A46, A47, A48, A49, A50, A51, A52, A53, A54, A55, A56, A57, A58, A59, A60, A61, A62, A63, A64, A65, A66, A67, A68, A69, A6M, A70, A71, A72, A73, A74, A75, A76, A77, A78, A79, A80, A81, A82, A83, A84, A85, A86, A87, A88, A89, A90, A91, A92, A93, A94, A95, A96, A97, A98, A99, A9C, AA, AAB, AAC, AAD, AAE, AAF, AAG, AAH, AAIB, AAJ, AAK, AAL, AAM, AAN, AAO, AAP, AAQ, AAR, AAS, AAV, AAW, AAX, AAY, AAZ, AB, ABA, ABB, ABD, ABF, ABG, ABH, ABIS, ABJ, ABK, ABL, ABN, ABP, ABQ, ABR, ABS, ABT, ABU, ABV, ABW, ABX, ABZ, AC, ACA, ACB, ACC, ACDI, ACF, ACJ, ACK, ACL, ACN, ACO, ACP, ACR, ACT, ACV, ACW, ACX, ACY, ACZ, ADA, ADB, ADF, ADG, ADH, ADIE, ADJ, ADK, ADL, ADM, ADN, ADQ, ADS, ADSI, ADV, ADX, ADY, ADZ, AE, AEA, AEB, AED, AEE, AEJ, AEK, AEL, AEM, AEN, AEO, AEP, AEQ, AER, AES, AET, AEU, AEV, AEW, AEX, AEZ, AFA, AFB, AFD, AFE, AFF, AFG, AFH, AFIA, AFIS, AFJ, AFK, AFL, AFPA, AFPS, AFQ, AFR, AFS, AFT, AFU, AFV, AFW, AFX, AFZ, AG, AGB, AGD, AGE, AGF, AGG, AGH, AGJ, AGK, AGL, AGM, AGN, AGO, AGP, AGPL, AGPM, AGQ, AGT, AGU, AGV, AGW, AGX, AGY, AGZ, AHA, AHB, AHC, AHD, AHE, AHF, AHG, AHH, AHIP, AHJ, AHK, AHL, AHM, AHN, AHO, AHR, AHSI, AHT, AHU, AHV, AHW, AHX, AHY, AHZ, AIB, AIC, AID, AIE, AIH, AIIB, AIIC, AIJ, AIK, AIM, AIO, AIP, AIPE, AIPP, AIQ, AIS, AIU, AIV, AIW, AIY, AIZ, AIs, AJ, AJB, AJC, AJF, AJG, AJH, AJJ, AJK, AJL, AJN, AJO, AJP, AJQ, AJR, AJS, AJT, AJU, AJV, AJW, AJY, AK, AKB, AKC, AKD, AKE, AKF, AKG, AKJ, AKK, AKL, AKM, AKN, AKO, AKP, AKQ, AKR, AKS, AKT, AKU, AKV, AKX, AKY, AL, ALB, ALC, ALD, ALE, ALG, ALH, ALJ, ALK, ALL, ALM, ALN, ALO, ALP, ALPA, ALQ, ALR, ALS, ALV, ALW, ALX, ALZ, AM, AMA, AMD, AME, AMF, AMG, AMH, AMIS, AMJ, AMK, AML, AMM, AMN, AMO, AMP, AMQ, AMR, AMS, AMT, AMU, AMV, AMW, AMY, AMZ, AN, ANB, ANC, AND, ANE, ANF, ANG, ANH, ANIA, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANPE, ANPN, ANPS, ANQ, ANR, ANS, ANT, ANV, ANW, ANX, ANY, ANZ, AO, AOA, AOB, AOC, AOD, AOE, AOF, AOG, AOH, AOJ, AOK, AOM, AON, AOO, AOP, AOQ, AOR, AOS, AOT, AOU, AOV, AOW, AOX, APDR, APPR, APRS, AQA, AQB, AQC, AQF, AQG, AQH, AQK, AQL, AQM, AQMI, AQP, AQPA, AQPC, AQR, AQS, AQT, AQY, AR, ARB, ARC, ARD, ARE, ARH, ARIA, ARIC, ARJ, ARK, ARM, ARN, ARPU, ARQ, ARR, ARSI, ART, ARU, ARV, ARW, ARX, ARY, ASA, ASC, ASE, ASF, ASFI, ASG, ASH, ASIN, ASJ, ASK, ASL, ASM, ASP, ASPA, ASQ, ASR, ASS, AST, ASU, ASV, ASW, ASX, ASY, ASZ, AT, AT3, ATA, ATAPI, ATB, ATD, ATE, ATF, ATG, ATH, ATK, ATL, ATN, ATO, ATPL, ATQ, ATR, ATS, ATT, ATU, ATW, ATX, ATY, ATZ, AU, AUA, AUC, AUD, AUE, AUF, AUG, AUJ, AUL, AUM, AUN, AUO, AUP, AUQ, AUR, AUS, AUU, AUW, AUX, AUY, AUZ, AV, AVA, AVB, AVC, AVE, AVF, AVG, AVH, AVK, AVL, AVN, AVO, AVP, AVR, AVS, AVU, AVV, AVW, AVX, AVY, AVZ, AWA, AWB, AWD, AWE, AWG, AWK, AWM, AWN, AWP, AWR, AWS, AWU, AWZ, AX, AXB, AXC, AXD, AXG, AXK, AXL, AXM, AXN, AXP, AXR, AXS, AXV, AXX, AYA, AYC, AYD, AYE, AYG, AYH, AYJ, AYL, AYN, AYO, AYP, AYQ, AYR, AYS, AYU, AYY, AYZ, AZ, AZA, AZB, AZD, AZE, AZF, AZN, AZO, AZS, AZW, AZZ, Ac, Ag, Al, Ala, Alpo, Am, Ar, Ark, Art, As, At, Ats, Au, Av, Ave, BAII, BAP, BAPR, BBI, BCI, BDI, BEI, BFI, BGI, BHI, BII, BJI, BKI, BLI, BMI, BNI, BOI, BP, BPB, BPC, BPF, BPG, BPH, BPK, BPL, BPN, BPO, BPP, BPR, BPS, BPT, BPU, BPY, BSI, BTI, BUI, BVI, BYI, Bi, CAP, CAPO, CAPOI, CAPV, CDI, CEI, CFPI, CHI, CI, CII, CMI, COI, CPD, CPG, CPH, CPIE, CPJ, CPL, CPS, CPV, CQI, CVI, Caph, Capt, Chi, Ci, Cpl, DAI, DDI, DEI, DFI, DGI, DHI, DI, DJI, DMI, DOI, DP, DPA, DPB, DPC, DPE, DPH, DPIC, DPMI, DPO, DPS, DPU, DPW, DSI, DVI, DZI, Di, EAI, EAP, EBI, EDI, EFI, EI, ELI, EMI, ENI, EPA, EPCI, EPIC, EPM, EPO, EPR, EPS, EPSI, EPT, EPU, EPV, ESI, Eli, FAI, FAP, FCPI, FEI, FFI, FGI, FI, FII, FJI, FMI, FPD, FPF, FPH, FPJ, FPM, FPP, FPR, FPS, FPT, FPU, FPV, FTI, FWI, GAFI, GAP, GCI, GEI, GFI, GHI, GOI, GP, GPE, GPG, GPIS, GPP, GPR, GPX, GPZ, GRI, GSI, GTI, Gap, HAI, HAP, HCI, HDI, HI, HKI, HOI, HPA, HPC, HPD, HPE, HPL, HPS, IAI, IASI, ICI, IFI, IGI, IOI, IPB, IPD, IPE, IPF, IPG, IPJ, IPM, IPNI, IPT, IPY, IRI, ISI, IVI, JAI, JAP, JNI, JPA, JPF, JPN, JPO, JPP, JPR, JPX, JPY, JRI, Jap, Japs, KOI, KP, KPD, KPF, KPL, KPN, KPU, KPW, Kip, LBI, LFI, LHI, LI, LNI, LOI, LP, LPC, LPD, LPE, LPF, LPJ, LPQ, LPR, LRI, LTI, Li, M3I, MAP, MAPA, MBI, MCI, MFI, MGI, MI, MMI, MOI, MP, MPC, MPD, MPL, MPM, MPN, MPS, MPSI, MPT, MPX, Mai, NAP, NI, NMI, NNI, NOI, NP0, NPAI, NPF, NPG, NPL, NPO, NPP, NPV, NUI, Ni, Np, OACI, OAI, OCI, ODAPI, ODI, OHI, OMI, OMPI, OPA, OPG, OPK, OPL, OPM, OPO, OPQ, OPT, ORI, OUI, P, PAP, PAPS, PB, PBI, PDI, PE, PEI, PG, PGI, PH, PHI, PIB, PIC, PID, PIE, PIF, PIJ, PIL, PIM, PIN, PIO, PIP, PIS, PIT, PIX, PIZ, PK, PL, PNI, PO, POI, PP, PP1, PP2, PP3, PP4, PP7, PPA, PPB, PPC, PPE, PPH, PPK, PPL, PPM, PPN, PPO, PPQ, PPR, PPRI, PPS, PPT, PPU, PPV, PQ, PTI, PU, PVI, PWI, PXI, Pb, Pd, Pei, Pl, Pm, Po, Pr, Pt, Pu, QI, QPC, QPM, QPO, QPV, RAI, RAP, RDI, REI, RFI, RI, RLI, ROI, RP, RPB, RPD, RPE, RPK, RPP, RPS, RPU, RPV, RPX, RRI, RVI, SAI, SCI, SCPI, SFI, SI, SII, SKI, SLI, SNI, SOI, SP, SP1, SPA, SPB, SPC, SPE, SPG, SPH, SPM, SPP, SPR, SPS, SPT, SPV, SPW, SRI, SSI, SSPI, STI, SVI, Shi, Si, Sp, Sui, TAI, TAP, TCI, TEI, TGI, TI, TMI, TP, TPE, TPF, TPG, TPJ, TPO, TPP, TPR, TPS, TPT, TPV, TRI, TSI, TTI, Ti, UAI, UAP, UFI, UGI, UI, UMI, UNI, UP, UP1, UPA, UPB, UPD, UPE, UPF, UPK, UPN, UPP, UPR, UPU, UPV, URI, USI, VAP, VCI, VDI, VEI, VFI, VI, VMI, VPC, VPE, VPH, VPP, VPS, WDI, WI, WP, WPC, WPF, WPS, WRI, XAP, XP, XPF, XPM, XPS, Xi, YP, ZAP, ZAPA, ZI, ZPE, ZPP, ZTI, aah, ab, ac, alb, alt, ang, ans, arr, ass, auk, aw, awn, bap, baps, bi, bpm, capt, chi, cps, hp, i, ii, iii, lei, lii, lvi, lxi, mi, oi, p, paps, pd, pf, pg, pis, pix, pj, pl, poi, pp, pr, rape, spic, spiv, ti, uni, vape, vi, vii, xci, xi, xii, xvi, xxi, A&amp;A, A&amp;D, A&amp;E, A&amp;M, A&amp;P, A&amp;R, AAPL, ACE, ACh, ADU, AFP, AGA, AGs, AHCI, AIX, ALF, ALU, AMPQ, AOPA, APAC, APFA, APFC, APFS, APKs, APNG, APNs, APRN, ARPA, ASIC, ASPR, AVIF, AVM, AVs, AWC, AWF, AXA, Abu, AjiÃ«, Aldi, Amir, Amit, Anil, Aon, Arif, Asif, Aziz, CP, CPN, CPP, CPQ, CPT, CZI, DAP, DNI, DalÃ­, Dani, EOI, EP, EPC, EPs, FPC, FPs, GNI, GPF, GPT, HPR, IFPI, INI, IPs, JCI, JPG, KPIs, Kai, LPA, LPL, LPS, LPs, MFi, MP3, MP4, MPs, MYI, NCI, NPM, NPN, NRI, NTI, PN, Pia, PyPi, RAPL, RKI, RTI, Ravi, SPD, Sami, Sri, TOI, TPU, TPX, TUI, Tai, UCI, UPT, UTI, VPs, WPP, WaPo, XPC, Xavi, Yi, ahh, ais, amu, aww, axe, aÃ§ai, mCi, padi, pax, pc, piÃ¹, ppl, tai] (4565) [lt:en:MORFOLOGIK_RULE_EN_US]">$api$</span> respectively, while interneuron</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">dendrites are simply denoted <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [end, dead, send, tend, deny, bend, deed, den, lend, fend, Deng, dens, mend, dent, DND, Dena, rend, vend, pend, Bend, END, MEND, wend, d end, DnD] (4630) [lt:en:MORFOLOGIK_RULE_EN_US]">$dend$</span>.  The derivative somatic membrane potentials of layer $l$ pyramidal neurons is given</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">by:</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">  C_m \dot{u}_l^P &amp; = - g_l u_l^{P} + g^{bas} v_l^{bas} + g^{api} v_l^{api} <span class="keyword1">\label</span>{eq-pyr-dynamics-rate}</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (4724) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $g_l$ is the somatic leakage conductance, and $C_m$ is the somatic membrane capacitance which will be assumed to</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">be $1$ from here on out. $v_l^{bas}$ and $v_l^{api}$ are the membrane potentials of basal and apical dendrites</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">respectively, and $g^{bas}$ and $g^{api}$ their corresponding coupling <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (4979) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span>.  Dendritic compartments in this</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">model have no persistence between simulation steps. Thus, they are defined at every <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [time step] (5109) [lt:en:MORFOLOGIK_RULE_EN_US]">timestep</span> $t$ through incoming weight</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">matrices and presynaptic activities:</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">  v_l^{bas}(t) &amp; = W_l^{up} \ \phi(u_{l-1}^P(t)) <span class="keyword1">\label</span>{eq-v-bas-rate}                                     \\</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">  v_l^{api}(t) &amp; =  W_l^{pi} \ \phi(u_l^I(t)) \ + \  W_l^{down} \ \phi(u_{l+1}^P(t)) <span class="keyword1">\label</span>{eq-v-api-rate}</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">The nomenclature for weight matrices conforms to \cite{Haider2021} where they are indexed by the layer in which their</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">target neurons lie, and belong to one of four populations: Feedforward and feedback pyramidal-to-pyramidal connections</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">arriving at layer $l$ are denoted $W_l^{up}$ and $W_l^{down}$ respectively. Lateral pyramidal-to-interneuron connections</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">are denoted with $W_l^{ip}$ and their corresponding feedback connections with $W_l^{pi}$.</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">Interneurons integrate synaptic information by largely the same principle, but instead of top-down signals from their</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">sister neurons arriving at an apical compartment, it is injected directly into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (5788) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span>.</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">  C_m \dot{u}_l^I &amp; = - g_l u_l^{I} + g^{dend} v_l^{dend} + i^{nudge, I}<span class="keyword1">\label</span>{eq-intn-dynamics} \\</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">  i^{nudge, I}    &amp; = g^{nudge, I} u_{l+1}^P                                                     \\</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">  v_l^{dend}      &amp; = W_l^{ip} \ \phi(u_{l}^P)</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (5796) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $ g^{nudge, I}$ is the interneuron nudging conductance, and $u_{l+1}^P$ is the somatic voltage of pyramidal</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">neurons in the next layer.  Pyramidal neurons in the output layer $N$ effectively behave like interneurons, as they</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">receive no input to their apical compartment. Instead, the target  activation $u^{tgt}$ is injected into their <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (6103) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span>:</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">  C_m \dot{u}_N^P &amp; = - g_l u_N^{P} + g^{bas} v_N^{bas} + i^{nudge, tgt} \\</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">  i^{nudge, tgt}  &amp; = g^{nudge, tgt} u^{tgt}</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">These neuron dynamics correspond closely to those <span class="highlight-sh" title="Do not use 'by [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">by \cite{</span>urbanczik2014learning}, including the extension to more than</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">two compartments which was proposed in the original paper. It should be noted however, that they are simplified in some</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">ways. For example, dendritic couplings and nudging are not relative to the somatic or some reversal potential (i.e.</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">$i^{nudge, tgt}= g^{nudge, tgt} (u^{tgt} - u_N^P )$), but are only dependent on absolute currents. Additionally, the</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">strict separation of excitatory and inhibitory synaptic integration (cf. Section \todo{write about this}), as well as</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">additional <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [collinearities] (6632) [lt:en:MORFOLOGIK_RULE_EN_US]">nonlinearities</span> in the plasticity are omitted.  These simplifications do increase computational speed and</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">allow for a much simpler approximation of network dynamics via a steady-state. Yet they do come at the cost of omitting</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">neuroscientific insights from the model, which will be discussed later.</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 52 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This section is very short (about 53 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span></span>{<span class="highlight-spelling" title="Possible spelling mistake found. (6930) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> Plasticity}<span class="keyword1">\label</span>{sec-urb-senn-plast}</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">The synapses in the network are all modulated according to variations of the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span><span class="highlight-spelling" title="Possible spelling mistake found. (7035) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]"></span>"</span> plasticity rule</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">\citep{urbanczik2014learning}, which will be discussed in this section. Note that as for the neuron model, the dendritic</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">error model slightly simplifies <span class="highlight" title="If the text is a generality, 'of the' is not necessary.. Suggestions: [some] (7194) [lt:en:SOME_OF_THE]">some of the</span> equations of the plasticity rule from its original implementation.</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline"><span class="keyword1">\subsection</span>{Derivation}</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">The plasticity rule is defined for postsynaptic neurons which have one somatic and at least one dendritic compartment,</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">to the latter of which synapses of this type can connect. Functionally, synaptic weights are changed in such a way, as</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">to minimize discrepancies between the somatic activity and dendritic potential. This discrepancy is called the</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline"><span class="keyword1">\textit</span>{dendritic prediction error}, and is computed from a hypothetical dendritic activation. The change in weight for</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">a synapse <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [from, front, iron, frog, Fran, Aron, fro, frond, frown, Freon, Ron, FRO, pron, tron, fr on, Efron] (7756) [lt:en:MORFOLOGIK_RULE_EN_US]">fron</span> neuron $j$ to the basal compartment of a pyramidal neuron <span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (7817) [lt:en:I_LOWERCASE]">$i$</span> is given by:</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">  \dot{w}_{ij}    &amp; = \eta \ ( \phi(u_i^{som}) - \phi(\hat{v}_i^{bas}) ) \ \phi(u_j^{som})^T \\</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">  \hat{v}_i^{bas} &amp; = \alpha \  v_i^{bas}</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [With] (7834) [lt:en:UPPERCASE_SENTENCE_START]">with</span> learning rate $\eta$, and $u^T$ denoting the transposition of the vector $u$ (which is by default assumed a column</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline">vector). The dendritic prediction $\hat{v}_i^{bas}$ is a scaled version of the dendritic potential by the constant</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">factor $\alpha$, which is calculated from coupling and leakage <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (8098) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span>. As an example, basal dendrites of pyramidal</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">neurons <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite{</span>sacramento2018dendritic} are attenuated by $\alpha = \frac{g^{bas}}{g_l + g^{bas} + g^{api}}$. A key</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">property of this value for $\alpha$ is, that dendritic error is $0$ when the only input to a neuron stems from the given</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">dendrite. In other words, the dendrite predicts somatic activity perfectly, and no change in synaptic weights is</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">required. Neuron- and layer-specific differences in $\alpha$, as well as an analytical derivation are detailed in</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">\citep{sacramento2018dendritic}.</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">If a current is injected into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (8570) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> (or in this case, into a different dendrite), a dendritic error arises, and</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">plasticity drives synaptic weights to minimize it. In addition to the learning rate $\eta$, the change in weight</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">$\dot{w}_{ij}$ is proportional to presynaptic activity $\phi(u_j^{som})$. Therefore, a dendritic error arising without</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">presynaptic contribution does not elicit a change in that particular synapse. This ensures that only synapses are</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">modified which recently influenced the postsynaptic neuron, providing a form of credit assignment. Updates for the</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">weight matrices in a hidden layer $l$ of the dendritic error model are given by:</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">  \dot{w}_{l}^{up}   &amp; = \eta_l^{up} \ ( \phi(u_l^{P}) - \phi(\hat{v}_l^{bas}) ) \ \phi(u_{l-1}^{P})^T<span class="keyword1">\label</span>{eq-delta_w_up}         \\</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">  \dot{w}_{l}^{ip}   &amp; = \eta_l^{ip} \ ( \phi(u_l^{I}) - \phi(\hat{v}_l^{dend}) ) \ \phi(u_{l}^{P})^T<span class="keyword1">\label</span>{eq-delta_w_ip}          \\</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">  \dot{w}_{l}^{pi}   &amp; = \eta_l^{pi} \ - v_l^{api} \ \phi(u_l^{I})^T<span class="keyword1">\label</span>{eq-delta_w_pi}                                           \\</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">  \dot{w}_{l}^{down} &amp; = \eta_l^{down} \ ( \phi(u_l^{P}) - \phi(w_l^{down} r_{l+1}^P) )\ \phi(u_{l+1}^{P})^T<span class="keyword1">\label</span>{eq-delta_w_down}</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">Each set of connections is updated with a specific learning rate $\eta$ and a specific dendritic error term. The purpose</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">of these particular dendritic errors will be explained in Section \ref{sec-selfpred}. Note that pyramidal-to-pyramidal</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">feedback weights $w_l^{down}$ are not plastic in the present simulations and are only listed for completeness, see</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline">Section \ref{sec-feedback-plast}.</div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline"><span class="keyword1">\section</span>{The self-predicting network state}<span class="keyword1">\label</span>{sec-selfpred}</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">In the dendritic error model neuron dynamics, plasticity rules and network architecture form an elegant interplay, which</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">will be explained in this section. Since each interneuron receives a somatic nudging signal from its corresponding</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">sister neuron, incoming synapses from lateral pyramidal neurons adapt their weights to match feedforward</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">pyramidal-to-pyramidal weights. In intuitive terms; Feedforward pyramidal-to-pyramidal weights elicit a certain</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">activation in the subsequent layer, which is fed back into corresponding interneurons. Hence, in the absence of incoming</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">connections, nudging from sister neurons causes interneurons to take on a proportional somatic potential. In order to</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">minimize the dendritic error term in Equation \ref{eq-delta_w_ip}, pyramidal-to-interneuron weight matrices at every</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">layer must match these forward weights ($w_l^{ip} \approx \rho w_l^{up}$) up to some scaling factor $\rho$. The exact</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">value for $\rho$ is parameter-dependent and immaterial for now. As long as no feedback information arrives at the</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">pyramidal neurons, plasticity drives synaptic weight to fulfill this constraint. Note, that this alignment of two</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">separate sets of outgoing weights is <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [achieved] (10661) [lt:en:MORFOLOGIK_RULE_EN_US]">acheived</span> with only local information. <span class="highlight" title="A comma may be missing after the conjunctive/linking adverb 'Therefore'.. Suggestions: [Therefore,] (10699) [lt:en:SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA]">Therefore</span> this mechanism could plausibly</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">align the weights of biological synapses that are physically separated by long distances. \newline</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">Next, consider the special case for interneuron-to-pyramidal weights in Equation \ref{eq-delta_w_pi}, in which</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">plasticity does not serve to reduce discrepancies between dendritic and somatic potential. The error term is instead</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">defined solely by the apical compartment <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [voltage In] (11091) [lt:en:MORFOLOGIK_RULE_EN_US]">voltage\footnote{In</span> strict terms, it is defined by the deviation of the</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">dendritic potential from its specific reversal potential. Since that potential is zero throughout, $- v_l^{api}$ remains</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">as the error term<span class="highlight" title="Two consecutive dots. Suggestions: [., â€¦] (11279) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Thus, plasticity in these synapses works towards silencing the apical compartment. The apical</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">compartments also receive feedback from superficial pyramidal neurons, whose synapses will be considered non-plastic for</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">now. As shown above, interneurons each learn to match their respective sister neuron activity. Thus, silencing of apical</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">compartments can only be <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [achieved] (11643) [lt:en:MORFOLOGIK_RULE_EN_US]">acheived</span> by mirroring the pyramidal-to-pyramidal feedback weights (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PW, cw, MW, CW, EW, KW, NW, SW, TW, W, WW, YW, aw, kW, kw, ow, w, DW, FW, GW, HW, UW, VW, mW] (11710) [lt:en:MORFOLOGIK_RULE_EN_US]">$w</span>_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [LPI] (11713) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{pi}</span> \approx</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">-w_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [down, Downs, downs, downy, letdown, Downy, lowdown, letdowns, Ladonna] (11729) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{down}$</span>).\newline</div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">When enabling plasticity in only these two synapse types, the network converges on the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"<span class="keyword1">\textbf</span>{</span>self-predicting state}<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]"></span>"</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">\citep{sacramento2018dendritic}. This state is defined by a minimization of four error metrics at each hidden layer $l$:</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline"><span class="keyword2">\begin{itemize}</span></div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">  <span class="keyword1">\item</span> The symmetries between feedforward ($w_l^{ip} \rightarrow \rho w_l^{up}$) and feedback (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PW, cw, MW, CW, EW, KW, NW, SW, TW, W, WW, YW, aw, kW, kw, ow, w, DW, FW, GW, HW, UW, VW, mW] (12007) [lt:en:MORFOLOGIK_RULE_EN_US]">$w</span>_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [LPI] (12010) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{pi}</span> \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (12016) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>rightarrow</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">        -w_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [down, Downs, downs, downy, letdown, Downy, lowdown, letdowns, Ladonna] (12038) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{down}$</span>) weights. Mean squared error between these pairs of matrices will be called <span class="keyword1">\textbf</span>{Feedforward - }</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">        and <span class="keyword1">\textbf</span>{Feedback weight error} respectively.</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline">  <span class="keyword1">\item</span> Silencing of pyramidal neuron apical compartments ($v_l^{api} \rightarrow 0$). Mean absolute apical compartment</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">        voltage within a layer is called the <span class="keyword1">\textbf</span>{Apical error}.</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">  <span class="keyword1">\item</span> Equal activations in interneurons and their respective sister neurons ($\phi (u_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [LSI, LBI, LFI, LHI, LI, LNI, LOI, LPI, LRI, LTI, Li, lei, lii, lvi, lxi] (12416) [lt:en:MORFOLOGIK_RULE_EN_US]">l^I</span>) \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (12422) [lt:en:MORFOLOGIK_RULE_EN_US]">rightarrow</span> \phi</div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">        (u_{l+1}^P)$). The mean squared error over these vectors is called the <span class="keyword1">\textbf</span>{Interneuron error}.</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline"><span class="keyword2">\end{itemize}</span></div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">Note that particularly in fully plastic networks, these convergences (except apical error) can be driven by both sides,</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">as for example a change interneuron activity can have a profound impact on their sister neurons. Furthermore, the</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">network does not ever reach a state in which all of these error terms are zero. In the original implementation, these</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">deviations are minute and can likely be explained with floating point conversions. Since it is impossible to replicate</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">the timing of the original precisely within NEST, the NEST simulations deviate more strongly from this ideal. The key</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">insight here is that this state is not clearly defined by absolute error thresholds, but is rather flexible. Thus,</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">networks are able to learn successfully even when their weights are initialized imperfectly. </div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">An analysis of the equations describing the network reveals that the idealized self-predicting state forms a stable</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">point of minimal energy. When Interneuron error is zero, the nudging conductance is predicted perfectly, thus</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">effectively disabling plasticity in incoming synapses. Likewise, a silenced apical compartment will disable plasticity</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">in all incoming synapses from interneurons. Similarly, the apical compartment is also the driving factor for the</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">dendritic error of feedforward synapses (Equation \ref{eq-delta_w_up}), since it leaks into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (13872) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> when</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [activities, activates, activators, actives, activeness] (13882) [lt:en:MORFOLOGIK_RULE_EN_US]">active\footnote{This</span> feature is actually rather important when contemplating biological neurons using the <span class="highlight-spelling" title="Possible spelling mistake found. (13978) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>Urbanczik-Senn</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline">plasticity. In the original paper, currents were injected directly into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (14069) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> to change the error term. <span class="highlight" title="Possible typo: you repeated a word. Suggestions: [The] (14100) [lt:en:ENGLISH_WORD_REPEAT_RULE]"></span>The the</div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">introduction of a second dendrite which performs that very task is much more useful, as originally described by the</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline">authors<span class="highlight" title="Two consecutive dots. Suggestions: [., â€¦] (14231) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Thus, in the self-predicting state all <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [plasticity] (14273) [lt:en:MORFOLOGIK_RULE_EN_US]">plasticiy</span> in the network is disabled, and the state is stable</div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">regardless of the kind of stimulus injected into the input layer. Next, notice how information flows backwards through</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline">the network; All feedback pathways between layers ultimately pass through the apical compartments of pyramidal neurons.</div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline">Thus, successful silencing of all apical compartments implies that no information can travel backwards between layers.</div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline">As a result, the network behaves strictly like a fully connected feedforward network consisting only of pyramidal</div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline">neurons. The recurrence within this network is in balance, and completely cancels out its own effects. This holds true</div><div class="clear"></div>
<div class="linenb">252</div><div class="codeline">as long as the network only receives external stimulation at the input layer. One interpretation of this is, that the</div><div class="clear"></div>
<div class="linenb">253</div><div class="codeline">network has learned to predict its own top-down input. A failure by interneurons to fully explain (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>cancel out)</div><div class="clear"></div>
<div class="linenb">254</div><div class="codeline">top-down input thus results in a prediction error, encoded in deviation of apical dendrite potentials from their resting</div><div class="clear"></div>
<div class="linenb">255</div><div class="codeline">state. This prediction error in turn elicits a cascade of plasticity in several synapses, which drives the network</div><div class="clear"></div>
<div class="linenb">256</div><div class="codeline">towards a self-predicting state that is congruent with the novel top-down signal. Therefore, these neuron- specific</div><div class="clear"></div>
<div class="linenb">257</div><div class="codeline">prediction errors are the driving force of learning in these networks.</div><div class="clear"></div>
<div class="linenb">258</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">259</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">260</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">261</div><div class="codeline"><span class="keyword1">\section</span>{Training the network}</div><div class="clear"></div>
<div class="linenb">262</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">263</div><div class="codeline">Starting with a network in the self-predicting state, performing time-continuous supervised learning then requires the</div><div class="clear"></div>
<div class="linenb">264</div><div class="codeline">injection of a target activation into the network's output layer alongside with a stimulus at the input layer. Since</div><div class="clear"></div>
<div class="linenb">265</div><div class="codeline">output layer neurons feed back into both interneurons and pyramidal neurons of the previous layer, local prediction</div><div class="clear"></div>
<div class="linenb">266</div><div class="codeline">errors arise. Synapses activate and drive to minimize the prediction errors, which requires the network to replicate the</div><div class="clear"></div>
<div class="linenb">267</div><div class="codeline">target activation from activations and weights of the last hidden layer. Note that this mechanism is not exclusive to</div><div class="clear"></div>
<div class="linenb">268</div><div class="codeline">the last two layers. Any Apical errors cause a change in somatic activity, which previous layers will fail to predict.</div><div class="clear"></div>
<div class="linenb">269</div><div class="codeline">Thus, errors are propagated backwards through the entire network, causing error minimization at every layer. See the</div><div class="clear"></div>
<div class="linenb">270</div><div class="codeline">Supplementary analysis of \cite{sacramento2018dendritic} for a rigorous proof that this type of network does indeed</div><div class="clear"></div>
<div class="linenb">271</div><div class="codeline">approximate the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (16540) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> algorithm.</div><div class="clear"></div>
<div class="linenb">272</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">273</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">274</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">275</div><div class="codeline">Classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [back propagation] (16580) [lt:en:MORFOLOGIK_RULE_EN_US]">backpropagation</span> relies on a strict separation of a forward pass of some stimulus, and subsequent a backwards</div><div class="clear"></div>
<div class="linenb">276</div><div class="codeline">pass dependent on the arising loss at the output layer. Since the present network is time-continuous, stimulus and</div><div class="clear"></div>
<div class="linenb">277</div><div class="codeline">target activation are injected into the network simultaneously. These injections are maintained for a given presentation</div><div class="clear"></div>
<div class="linenb">278</div><div class="codeline">time $t_{pres}$, in order to allow the network to calculate errors through its recurrent connections before slowly</div><div class="clear"></div>
<div class="linenb">279</div><div class="codeline">adapting weights. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Particularly] (17049) [lt:en:MORFOLOGIK_RULE_EN_US]">Particluarly</span> for deep networks, signals travelling from both the input and output layer require some</div><div class="clear"></div>
<div class="linenb">280</div><div class="codeline">time to balance out and elicit the correct dendritic error terms. This property poses the most significant drawback of</div><div class="clear"></div>
<div class="linenb">281</div><div class="codeline">this type of time-continuous approximation of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (17315) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span>: The network tends to overshoot activations in some</div><div class="clear"></div>
<div class="linenb">282</div><div class="codeline">neurons, which in turn causes an imbalance between dendritic and somatic compartments. This effect causes the network to</div><div class="clear"></div>
<div class="linenb">283</div><div class="codeline">change synaptic weights away from the desired state during the first few <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [milliseconds] (17577) [lt:en:MORFOLOGIK_RULE_EN_US]">miliseconds</span> of a stimulus presentation. The</div><div class="clear"></div>
<div class="linenb">284</div><div class="codeline">solution Sacramento <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>found for this issue was to drastically reduce learning rates, while increasing stimulus</div><div class="clear"></div>
<div class="linenb">285</div><div class="codeline">presentation time. This solution is sufficient to prove that plasticity in this kind of network is able to perform error</div><div class="clear"></div>
<div class="linenb">286</div><div class="codeline">propagation, but still has some issues. Most notably, training is highly inefficient and computationally intensive. A</div><div class="clear"></div>
<div class="linenb">287</div><div class="codeline">closer investigation of the issue together with a different solution will be discussed in Section \ref{sec-latent-eq}.</div><div class="clear"></div>
<div class="linenb">288</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">289</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">290</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">291</div><div class="codeline"><span class="keyword1">\section</span>{The NEST simulator}</div><div class="clear"></div>
<div class="linenb">292</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">293</div><div class="codeline">One of the key research questions motivating this thesis is whether the network would be able to learn successfully when</div><div class="clear"></div>
<div class="linenb">294</div><div class="codeline">employing spike-based communication instead of the rate neurons for which it was developed. As a framework for the</div><div class="clear"></div>
<div class="linenb">295</div><div class="codeline">spike-based implementation two options were considered: The first one was to use the existing implementation of the</div><div class="clear"></div>
<div class="linenb">296</div><div class="codeline">network which employs the Python frameworks \texttt{PyTorch} and \texttt{NumPy}, and expand it to employ spiking</div><div class="clear"></div>
<div class="linenb">297</div><div class="codeline">neurons. PyTorch does in principle support spiking communication between layers, but is streamlined for implementing</div><div class="clear"></div>
<div class="linenb">298</div><div class="codeline">less recurrent and less complex network and neuron models. Another concern is efficiency; PyTorch is very well optimized</div><div class="clear"></div>
<div class="linenb">299</div><div class="codeline">for computing matrix operations on dedicated hardware. This makes it a good choice for simulating large networks of rate</div><div class="clear"></div>
<div class="linenb">300</div><div class="codeline">neurons, which transmit all of their activations between layers at every simulation step. Spiking communication between</div><div class="clear"></div>
<div class="linenb">301</div><div class="codeline">leaky neurons is almost antithetical to this design philosophy and thus can be expected to perform comparatively poorly</div><div class="clear"></div>
<div class="linenb">302</div><div class="codeline">when using this backend.</div><div class="clear"></div>
<div class="linenb">303</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">304</div><div class="codeline">The second option was to use the NEST simulator</div><div class="clear"></div>
<div class="linenb">305</div><div class="codeline">(\href{https://nest-simulator.readthedocs.io}{nest-simulator.readthedocs.io}, \cite{Gewaltig2007}), which was developed</div><div class="clear"></div>
<div class="linenb">306</div><div class="codeline">with highly parallel simulations of large spiking neural networks in mind. It is written in C++ and uses the</div><div class="clear"></div>
<div class="linenb">307</div><div class="codeline"><span class="keyword1">\textit</span>{Message Passing Interface} (\href{https://www.mpi-forum.org/}{MPI}) to efficiently communicate events between</div><div class="clear"></div>
<div class="linenb">308</div><div class="codeline">both threads and compute nodes. One design pillar of the simulator, which is particularly relevant for this project, is</div><div class="clear"></div>
<div class="linenb">309</div><div class="codeline">the event-based communication scheme that underpins all simulated nodes. It ensures that communication bandwidth at</div><div class="clear"></div>
<div class="linenb">310</div><div class="codeline">every simulation step is only used by the subset of nodes which transmit signals at that time step, which is</div><div class="clear"></div>
<div class="linenb">311</div><div class="codeline">particularly efficient for spiking communication. Another important advantage of the NEST simulator is, that an</div><div class="clear"></div>
<div class="linenb">312</div><div class="codeline">event-based implementation of the <span class="highlight-spelling" title="Possible spelling mistake found. (19952) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity alongside a corresponding neuron model had already been</div><div class="clear"></div>
<div class="linenb">313</div><div class="codeline">developed for it. Therefore, it was decided to implement the spiking neuron model in the NEST simulator.</div><div class="clear"></div>
<div class="linenb">314</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">315</div><div class="codeline">The simulator has one particular limitation which needs to be considered. As communication between physically separate</div><div class="clear"></div>
<div class="linenb">316</div><div class="codeline">compute nodes takes time, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Events, Eventual] (20285) [lt:en:MORFOLOGIK_RULE_EN_US]">Events\footnote{An</span> Event in NEST is an abstract C++ Class that is created by neurons, and</div><div class="clear"></div>
<div class="linenb">317</div><div class="codeline">transmitted across threads and compute nodes by the Simulator. A Multitude of Event types are provided (i.e.</div><div class="clear"></div>
<div class="linenb">318</div><div class="codeline">\texttt{<span class="highlight-spelling" title="Possible spelling mistake found. (20474) [lt:en:MORFOLOGIK_RULE_EN_US]">SpikeEvent</span>, <span class="highlight-spelling" title="Possible spelling mistake found. (20486) [lt:en:MORFOLOGIK_RULE_EN_US]">CurrentEvent</span>, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Reinvent, Atrovent, Reinterment] (20500) [lt:en:MORFOLOGIK_RULE_EN_US]">RateEvent}</span>), each able to carry specific types of payload and being processed</div><div class="clear"></div>
<div class="linenb">319</div><div class="codeline">differently by postsynaptic neurons.} <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [In] (20614) [lt:en:UPPERCASE_SENTENCE_START]">in</span> NEST can not be handled in the same simulation step in which they were sent.</div><div class="clear"></div>
<div class="linenb">320</div><div class="codeline">Thus, NEST enforces a synaptic transmission delay of at least one simulation step for all connections. This property is</div><div class="clear"></div>
<div class="linenb">321</div><div class="codeline">integral to other parallel simulation backends \citep{Hines1997} as well as <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (20876) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphic</span> hardware</div><div class="clear"></div>
<div class="linenb">322</div><div class="codeline">\citep{davies2018loihi}. It may not even <span class="highlight" title="The modal verb 'may' requires the verb's base form.. Suggestions: [consider] (20919) [lt:en:MD_BASEFORM]">considered</span> a limitation by some, as synaptic transmission within biological</div><div class="clear"></div>
<div class="linenb">323</div><div class="codeline">neurons is never instantaneous either \citep{kandel2021principles}. Yet particularly with regard to the relaxation</div><div class="clear"></div>
<div class="linenb">324</div><div class="codeline">period issue of this model (cf. Section \ref{sec-latent-eq}), it can be expected to affect performance.</div><div class="clear"></div>
<div class="linenb">325</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">326</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">327</div><div class="codeline"><span class="keyword1">\section</span>{Transitioning to spiking communication}</div><div class="clear"></div>
<div class="linenb">328</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">329</div><div class="codeline">The spiking neuron models rely heavily on the NEST implementation from <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (21284) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> and colleagues \citep{Stapmanns2021},</div><div class="clear"></div>
<div class="linenb">330</div><div class="codeline">which was used show that spiking neurons are able to perform learning tasks that were designed for the rate neurons</div><div class="clear"></div>
<div class="linenb">331</div><div class="codeline">described <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite{</span>urbanczik2014learning}. The existing model is an exact replication of the <span class="highlight-spelling" title="Possible spelling mistake found. (21498) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> neuron in</div><div class="clear"></div>
<div class="linenb">332</div><div class="codeline">terms of membrane dynamics. The critical update of the NEST variant is that instead of transmitting their hypothetical</div><div class="clear"></div>
<div class="linenb">333</div><div class="codeline">rate $r = \phi(u)$ at every time step, these neurons emit spikes <span class="highlight" title="Consider replacing this phrase with the adverb 'similarly' to avoid wordiness.. Suggestions: [similarly] (21695) [lt:en:IN_A_X_MANNER]">in a similar way</span> to stochastic binary neurons</div><div class="clear"></div>
<div class="linenb">334</div><div class="codeline">\citep{Ginzburg1994}. The number of spikes to be generated during a simulation step $n$ is determined by drawing from a</div><div class="clear"></div>
<div class="linenb">335</div><div class="codeline">Poisson distribution, which takes $r$ as a parameter:</div><div class="clear"></div>
<div class="linenb">336</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">337</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">338</div><div class="codeline">  P\{<span class="keyword1">\textit</span>{n} \ \text{spikes during} \ \Delta t\} &amp; = e^{-r \Delta t} \frac{(r \ \Delta t) ^ n}{n!}<span class="keyword1">\label</span>{eq-pr-n-spikes} \\</div><div class="clear"></div>
<div class="linenb">339</div><div class="codeline">  \langle <span class="keyword1">\textit</span>{n} \rangle                        &amp; = r \ \Delta t <span class="keyword1">\label</span>{eq-n-spikes}</div><div class="clear"></div>
<div class="linenb">340</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">341</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">342</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (21896) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $\Delta t$ denotes the integration time step of the simulator, which will be assumed to be $0.1 ms$ from here on</div><div class="clear"></div>
<div class="linenb">343</div><div class="codeline">out.  $\langle <span class="keyword1">\textit</span>{n} \rangle$ denotes the expected number of spikes to be emitted in a simulation step. Note that</div><div class="clear"></div>
<div class="linenb">344</div><div class="codeline">this mechanism makes the assumption that more than one spike can occur per simulation step. NEST was developed with this</div><div class="clear"></div>
<div class="linenb">345</div><div class="codeline">possibility in mind and provides a <span class="keyword1">\textit</span>{multiplicity} parameter for <span class="highlight-spelling" title="Possible spelling mistake found. (22274) [lt:en:MORFOLOGIK_RULE_EN_US]">SpikeEvents</span>, which is processed at the</div><div class="clear"></div>
<div class="linenb">346</div><div class="codeline">postsynaptic neuron. As the high spike frequencies resulting from this could not occur in biological neurons, the model</div><div class="clear"></div>
<div class="linenb">347</div><div class="codeline">is also capable of simulating a refractory period. For this, the number of spikes per step is limited to $1$, and the</div><div class="clear"></div>
<div class="linenb">348</div><div class="codeline">spiking probability is set to 0 for the duration of the refractory period $t_{ref}$. The probability of at least one</div><div class="clear"></div>
<div class="linenb">349</div><div class="codeline">spike <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [occurring] (22664) [lt:en:MORFOLOGIK_RULE_EN_US]">occuring</span> within the next simulation step is given the inverse probability of no spike <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [occurring] (22750) [lt:en:MORFOLOGIK_RULE_EN_US]">occuring</span>. Thus, when</div><div class="clear"></div>
<div class="linenb">350</div><div class="codeline">inserting $n=0$ into Equation \ref{eq-pr-n-spikes}, the probability of eliciting at least one spike within the next</div><div class="clear"></div>
<div class="linenb">351</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [simulation] (22864) [lt:en:MORFOLOGIK_RULE_EN_US]">simualtion</span> step can be derived as:</div><div class="clear"></div>
<div class="linenb">352</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">353</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">354</div><div class="codeline">  P\{ <span class="keyword1">\textit</span>{n} \geq 1\} &amp; = 1 - e^{-r \Delta t}</div><div class="clear"></div>
<div class="linenb">355</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">356</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">357</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">358</div><div class="codeline">Drawing from this probability then determines <span class="highlight" title="Consider shortening this phrase to just 'whether'. It is correct though if you mean 'regardless of whether'.. Suggestions: [whether] (22948) [lt:en:WHETHER]">whether or not</span> a spike is sent during that step, henceforth denoted with</div><div class="clear"></div>
<div class="linenb">359</div><div class="codeline">the function $s(t)$, which outputs $1$ if a spike is sent during the interval $[t, t+\Delta t]$, and $0$ otherwise.</div><div class="clear"></div>
<div class="linenb">360</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">361</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">362</div><div class="codeline">In order to implement the plasticity rule for spiking neurons, dendritic compartments need to be modeled with leaky</div><div class="clear"></div>
<div class="linenb">363</div><div class="codeline">dynamics. These dynamics are fundamentally the same as those described for the somatic compartment. Thus, the basal</div><div class="clear"></div>
<div class="linenb">364</div><div class="codeline">compartment of a pyramidal neuron $j$ evolves according to:</div><div class="clear"></div>
<div class="linenb">365</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">366</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">367</div><div class="codeline">  C_m^{bas} \dot{v}_j^{bas} &amp; = -g_l^{bas} \  v_j^{bas} + \sum_{i \in I} W_{ji} s_i(t)     <span class="keyword1">\label</span>{eq-spiking-basal-compartment}</div><div class="clear"></div>
<div class="linenb">368</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">369</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">370</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [With] (23414) [lt:en:UPPERCASE_SENTENCE_START]">with</span> presynaptic neurons $I$, and membrane capacitance $C_m^{bas}$ and leakage conductance $g_l^{bas}$ being specific to</div><div class="clear"></div>
<div class="linenb">371</div><div class="codeline">the basal dendrite. Note that these equations are calculated individually for each neuron and do not employ the matrix</div><div class="clear"></div>
<div class="linenb">372</div><div class="codeline">notation used for layers of rate neurons. Pyramidal apical and interneuron dendritic compartments evolve by the same</div><div class="clear"></div>
<div class="linenb">373</div><div class="codeline">principle and with largely the same parameters.</div><div class="clear"></div>
<div class="linenb">374</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">375</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">section</span>{Event-based <span class="highlight-spelling" title="Possible spelling mistake found. (23810) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity}<span class="keyword1">\label</span>{sec-event-urb}</div><div class="clear"></div>
<div class="linenb">376</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">377</div><div class="codeline">One major challenge in implementing this architecture with spiking neurons is the <span class="highlight-spelling" title="Possible spelling mistake found. (23919) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity introduced</div><div class="clear"></div>
<div class="linenb">378</div><div class="codeline">in Section \ref{sec-urb-senn-plast}. Since the plasticity rule is originally defined for rate neurons, computing the</div><div class="clear"></div>
<div class="linenb">379</div><div class="codeline">updates for spiking neurons requires some additional effort. Fortunately, this problem has already been solved in NEST</div><div class="clear"></div>
<div class="linenb">380</div><div class="codeline">for two-compartment neurons \citep{Stapmanns2021}. This Section will discuss its algorithm and its implementation.</div><div class="clear"></div>
<div class="linenb">381</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">382</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">383</div><div class="codeline">Since NEST is an event-based simulator, most of the plasticity mechanisms developed for it compute weight changes at the</div><div class="clear"></div>
<div class="linenb">384</div><div class="codeline">location (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>thread and compute node) of the postsynaptic neuron whenever an Event is received. This has several</div><div class="clear"></div>
<div class="linenb">385</div><div class="codeline">advantages; It allows the thread that created the Event to continue processing neuron updates instead of having to</div><div class="clear"></div>
<div class="linenb">386</div><div class="codeline">synchronize with all threads that manage recipient neurons.  More importantly, this feature mirrors the local properties</div><div class="clear"></div>
<div class="linenb">387</div><div class="codeline">of most biologically plausible synaptic plasticity models, as these are often considered to be primarily dependent on</div><div class="clear"></div>
<div class="linenb">388</div><div class="codeline">factors that are local to the synapse \citep{magee2020synaptic}. For a spiking implementation of the <span class="highlight-spelling" title="Possible spelling mistake found. (24945) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>Urbanczik-Senn</div><div class="clear"></div>
<div class="linenb">389</div><div class="codeline">plasticity, dendritic errors at every time step are required instead of just a scalar trace at the time of a spike, as</div><div class="clear"></div>
<div class="linenb">390</div><div class="codeline">would be the case for STDP. Thus, a mechanism for managing these errors was <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [required] (25155) [lt:en:MORFOLOGIK_RULE_EN_US]">requred</span>, for which two basic possibilities</div><div class="clear"></div>
<div class="linenb">391</div><div class="codeline">were considered:</div><div class="clear"></div>
<div class="linenb">392</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">393</div><div class="codeline">In a <span class="keyword1">\textbf</span>{Time-driven scheme}, dendritic errors are made available to synapses at every <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [time step] (25298) [lt:en:MORFOLOGIK_RULE_EN_US]">timestep</span>, and weight changes</div><div class="clear"></div>
<div class="linenb">394</div><div class="codeline">are applied instantaneously. This approach is in principle an adaptation of the original computations for <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike trains] (25433) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrains</span>.</div><div class="clear"></div>
<div class="linenb">395</div><div class="codeline">Its main drawback is that calls to the synaptic update function are as frequent as neuron updates - for all synapses.</div><div class="clear"></div>
<div class="linenb">396</div><div class="codeline">Particularly for large numbers of incoming synapses, as is common for simulations of cortical pyramidal neurons</div><div class="clear"></div>
<div class="linenb">397</div><div class="codeline">\citep{potjans2014cell,vezoli2004quantitative}, this implies <span class="highlight" title="Specify a number, remove phrase, or simply use 'many' or 'numerous'. Suggestions: [many, numerous] (25694) [lt:en:LARGE_NUMBER_OF]">a large number of</span> function calls per time step.  Therefore,</div><div class="clear"></div>
<div class="linenb">398</div><div class="codeline">this approach proved costly in terms of computational resources.</div><div class="clear"></div>
<div class="linenb">399</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">400</div><div class="codeline">An <span class="keyword1">\textbf</span>{Event-driven scheme} on the other hand, updates synaptic weights only when a spike is sent through the</div><div class="clear"></div>
<div class="linenb">401</div><div class="codeline">synapse. A history of the dendritic error is stored at the postsynaptic neuron, which is read by each synapse when a</div><div class="clear"></div>
<div class="linenb">402</div><div class="codeline">spike is transmitted in order to compute weight changes. As the history of dendritic error applies equally to all</div><div class="clear"></div>
<div class="linenb">403</div><div class="codeline">incoming synapses, it only needs to be recorded once at the neuron. Alongside each entry in the history, a counter is</div><div class="clear"></div>
<div class="linenb">404</div><div class="codeline">stored and incremented whenever a synapse has read the history at that time step. Once all synapses have read out an</div><div class="clear"></div>
<div class="linenb">405</div><div class="codeline">entry, it is deleted. Thus, the history dynamically grows and shrinks during simulation and is only ever as long as the</div><div class="clear"></div>
<div class="linenb">406</div><div class="codeline">largest inter-spike interval (ISI) of all presynaptic neurons. This approach proves to be more efficient in terms of</div><div class="clear"></div>
<div class="linenb">407</div><div class="codeline">computation time, since fewer calls to the update function are required per synapse. It does come at the cost of memory</div><div class="clear"></div>
<div class="linenb">408</div><div class="codeline">consumption, as the history can grow particularly large for simulations with low in-degrees or large <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [ISI It] (26849) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>ISI\footnote{It</div><div class="clear"></div>
<div class="linenb">409</div><div class="codeline">should also be noted that in this approach requires redundant integration of the history by every synapse. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (26962) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> et</div><div class="clear"></div>
<div class="linenb">410</div><div class="codeline">al. propose a third solution, in which this integration is performed once whenever a spike is transmitted through any</div><div class="clear"></div>
<div class="linenb">411</div><div class="codeline">incoming connection, with the resulting weight change being applied to all synapses immediately. This approach proved to</div><div class="clear"></div>
<div class="linenb">412</div><div class="codeline">be even more efficient for some network configurations, but is incompatible with simulations where incoming synapses</div><div class="clear"></div>
<div class="linenb">413</div><div class="codeline">have heterogeneous synaptic delays due to the way that these delays are processed by the NEST simulator. See Section</div><div class="clear"></div>
<div class="linenb">414</div><div class="codeline">3.1.3 <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite{</span>Stapmanns2021} for a detailed explanation<span class="highlight" title="Two consecutive dots. Suggestions: [., â€¦] (27487) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> During testing, the Event-based schemed proved substantially</div><div class="clear"></div>
<div class="linenb">415</div><div class="codeline">more efficient for many network types. This did however introduce the challenge of retroactively computing weight</div><div class="clear"></div>
<div class="linenb">416</div><div class="codeline">changes from the time of the last spike upon arrival of a new spike. \newline</div><div class="clear"></div>
<div class="linenb">417</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">418</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">419</div><div class="codeline"><span class="keyword1">\subsection</span>{Integrating weight changes}</div><div class="clear"></div>
<div class="linenb">420</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">421</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">422</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (27774) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>describe the <span class="highlight-spelling" title="Possible spelling mistake found. (27804) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity rule based on the general form for weight changes</div><div class="clear"></div>
<div class="linenb">423</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">424</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">425</div><div class="codeline">  \dot{w}_{ij}(t) &amp; = F(s_j^\ast (t), V_i^\ast (t)) <span class="keyword1">\label</span>{eq-delta-w-spiking}</div><div class="clear"></div>
<div class="linenb">426</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">427</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">428</div><div class="codeline">where the change in weight $\dot{w}_{ij}$ of a synapse from neuron $j$ to neuron $i$ at time $t$ is given by a function</div><div class="clear"></div>
<div class="linenb">429</div><div class="codeline">$F$ that depends on the dendritic error of the postsynaptic neuron $V_i^\ast$ and the presynaptic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (28070) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> $s_j^\ast$.</div><div class="clear"></div>
<div class="linenb">430</div><div class="codeline">The $\ast$ operator denotes a causal function, indicating that a value $V_i^\ast(t)$ potentially depends on all previous</div><div class="clear"></div>
<div class="linenb">431</div><div class="codeline">values of $V_i(t' &lt; t)$. One can formally integrate Equation \ref{eq-delta-w-spiking} in order to obtain the weight</div><div class="clear"></div>
<div class="linenb">432</div><div class="codeline">change between two arbitrary time points $t$ and $T$:</div><div class="clear"></div>
<div class="linenb">433</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">434</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">435</div><div class="codeline">  \Delta w_{ij}(t,T) &amp; = \int_t^T dt' F[s_j^\ast, V_i^\ast](t') <span class="keyword1">\label</span>{eq-delta-w-t-T}</div><div class="clear"></div>
<div class="linenb">436</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">437</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">438</div><div class="codeline">This integral is of forms the basis of computing the change in weight between two arriving spikes. Thus, at the</div><div class="clear"></div>
<div class="linenb">439</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [implementation, implementations] (28433) [lt:en:MORFOLOGIK_RULE_EN_US]">implementational</span> level, $t$ is usually the time of the last spike that traversed the synapse, and $T$ is the current</div><div class="clear"></div>
<div class="linenb">440</div><div class="codeline">\texttt{biological\_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [times, Timothy, timeless, timers, tithes, Mathis, Tibetans, bioethics, mimetic, timeouts, Tethys, timothy, Epimethius, meths, teethes, timeous, MÃ©tis, Tetris, TimothÃ©e, dimethyl, kimchis, mimesis, timeshift, timeshifts, timewise] (28558) [lt:en:MORFOLOGIK_RULE_EN_US]">time}\footnote{This</span> term is adopted from the NEST convention, where it describes the time for which</div><div class="clear"></div>
<div class="linenb">441</div><div class="codeline">a neuron or network has been simulated in $ms$. It counts the number of simulation steps of length $\Delta t \ ms$ which</div><div class="clear"></div>
<div class="linenb">442</div><div class="codeline">have been computed, and is therefore <span class="highlight" title="The usual collocation for 'independent' is 'of', not 'from'. Did you mean 'independent of'?. Suggestions: [independent of] (28789) [lt:en:INDEPENDENTLY_FROM_OF]">independent from</span> a simulation's runtime  on the employed hardware (sometimes also</div><div class="clear"></div>
<div class="linenb">443</div><div class="codeline">called <span class="keyword1">\textit</span>{wall clock time} \citep{albada2018performance}). }<span class="highlight" title="Don't put a space before the full stop.. Suggestions: [.] (28900) [lt:en:COMMA_PARENTHESIS_WHITESPACE]">.</span> For spiking neurons, it is necessary to approximate</div><div class="clear"></div>
<div class="linenb">444</div><div class="codeline">the presynaptic rate ($r_j=\phi(u_j)$). For this, a well established solution is to transform the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (29038) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> $s_j$ into</div><div class="clear"></div>
<div class="linenb">445</div><div class="codeline">a decaying trace using an exponential filter kernel $\kappa$:</div><div class="clear"></div>
<div class="linenb">446</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">447</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">448</div><div class="codeline">  \kappa(t)     &amp; = H(t) \frac{1}{t}e^{\frac{-t}{\tau_{\kappa}}}                        \\</div><div class="clear"></div>
<div class="linenb">449</div><div class="codeline">  H(t)          &amp; =</div><div class="clear"></div>
<div class="linenb">450</div><div class="codeline">  <span class="keyword2">\begin{cases}</span></div><div class="clear"></div>
<div class="linenb">451</div><div class="codeline">    1 &amp; \text{if $t &gt; 0$}    \\</div><div class="clear"></div>
<div class="linenb">452</div><div class="codeline">    0 &amp; \text{if $t \leq 0$} \\</div><div class="clear"></div>
<div class="linenb">453</div><div class="codeline">  <span class="keyword2">\end{cases}</span>                                                              \\</div><div class="clear"></div>
<div class="linenb">454</div><div class="codeline">  (f \ast g)(t) &amp; = \int_{- \infty }^{\infty} f(t') g(t-t') d t' <span class="keyword1">\label</span>{eq-convolution} \\</div><div class="clear"></div>
<div class="linenb">455</div><div class="codeline">  s_j^\ast      &amp; = \kappa_s \ast s_j. <span class="keyword1">\label</span>{eq-spike-trace}</div><div class="clear"></div>
<div class="linenb">456</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">457</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">458</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [With] (29113) [lt:en:UPPERCASE_SENTENCE_START]">with</span> filter time constant $\tau_\kappa$. To obtain the trace of a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (29167) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span>, it is <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [coevolved] (29185) [lt:en:MORFOLOGIK_RULE_EN_US]">convolved</span> (Equation</div><div class="clear"></div>
<div class="linenb">459</div><div class="codeline">\ref{eq-convolution}) with the exponential filter kernel $\kappa$. The filter uses the Heaviside step function $H(t)$,</div><div class="clear"></div>
<div class="linenb">460</div><div class="codeline">and is therefore only supported on positive values of $t$ (also called a one-sided exponential decay kernel). This</div><div class="clear"></div>
<div class="linenb">461</div><div class="codeline">property is important, as integration limits of the convolution can be truncated when $f$ and $g$ are both only</div><div class="clear"></div>
<div class="linenb">462</div><div class="codeline">supported on $[0,\infty)$:</div><div class="clear"></div>
<div class="linenb">463</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">464</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">465</div><div class="codeline">  (f \ast g)(t) &amp; = \int_{0}^{t} f(t') g(t-t') d t'</div><div class="clear"></div>
<div class="linenb">466</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">467</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">468</div><div class="codeline">Since spikes naturally only occur at $t&gt;0$, this simplified integral allows for a much more efficient computation of the</div><div class="clear"></div>
<div class="linenb">469</div><div class="codeline">convolution. In this particular case, the Function $F$ on the <span class="highlight" title="Did you mean the adjective 'right-hand'?. Suggestions: [right-hand] (29709) [lt:en:MISSING_HYPHEN]">right hand</span> side of Equation \ref{eq-delta-w-spiking} is</div><div class="clear"></div>
<div class="linenb">470</div><div class="codeline">defined as:</div><div class="clear"></div>
<div class="linenb">471</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">472</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">473</div><div class="codeline">  F[s_j^\ast, V_i^\ast] &amp; = \eta \kappa \ast (V_i^\ast s_j^\ast)        \\</div><div class="clear"></div>
<div class="linenb">474</div><div class="codeline">  V_i^\ast              &amp; = (\phi(u_i^{som}) - \phi(\hat{v}_i^{dend}) )</div><div class="clear"></div>
<div class="linenb">475</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">476</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">477</div><div class="codeline">\what{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [This] (29756) [lt:en:UPPERCASE_SENTENCE_START]">this</span> notation seems slightly abusive, as neither side considers $t$, but it is taken precisely from</div><div class="clear"></div>
<div class="linenb">478</div><div class="codeline">\cite{Stapmanns2021}} with learning rate $\eta$. $V_i^\ast$ then is the dendritic error of the dendrite that the synapse</div><div class="clear"></div>
<div class="linenb">479</div><div class="codeline">between $j$ and <span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (29957) [lt:en:I_LOWERCASE]">$i$</span> is located <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [the, battle, Battle, cattle, Matthew, anthem, ate, attire, attic, Attic, ante, bathe, lathe, matte, rattle, tithe, ACTH, Artie, Ashe, atone, Althea, Attlee, Mattie, ache, anther, attache, latte, lithe, Bethe, Lethe, attune, tattle, attar, withe, ACTE, AHE, ARTE, ATE, ATEE, ATH, ATT, ATTAC, Attn, FTTH, Hattie, LTTE, acte, attn, atty, tattie, wattle, Agathe, Authy, Sarthe, attachÃ©, auth, authed, auths, pattie] (29970) [lt:en:MORFOLOGIK_RULE_EN_US]">at\footnote{The</span> dendritic error here is defined as the difference between two</div><div class="clear"></div>
<div class="linenb">480</div><div class="codeline">hypothetical rates based on the arbitrary function $\phi$. The original implementation uses the difference between the</div><div class="clear"></div>
<div class="linenb">481</div><div class="codeline">actual postsynaptic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (30172) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> and this dendritic prediction ($V_i^\ast = (s_i - \phi(\hat{v}_i^{dend}) )$).</div><div class="clear"></div>
<div class="linenb">482</div><div class="codeline">Furthermore, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (30231) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>show that generating a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (30271) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> from the dendritic potential (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [UV, CV, MV, PV, TV, AV, GV, IV, JV, KV, NV, RV, V, WV, eV, XV, BV, DV, EV, LV, SV] (30312) [lt:en:MORFOLOGIK_RULE_EN_US]">$V</span>_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [in, is, I, it, if, ID, IQ, id, IH, IP, IA, IE, IF, IJ, IL, IN, IR, IS, IT, IV, IZ, Ia, In, Io, Ir, It, i, ii, iv, ix, IB, IC, IG, IM, IU] (30315) [lt:en:MORFOLOGIK_RULE_EN_US]">i^</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [AST, as, at, last, art, east, East, St, act, cast, past, fast, vast, ask, sat, ash, aft, mast, LST, ant, apt, ASB, ASD, CST, DST, MST, SST, st, AAT, ADT, AIT, ASN, PST, asp, AUT, AZT, amt, bast, asst, hast, 1st, ABT, ACT, AET, AFT, AGT, AHT, AJT, AKT, AMST, AMT, ANT, AOT, APT, AQT, ART, ASA, ASC, ASE, ASF, ASG, ASH, ASJ, ASK, ASL, ASM, ASP, ASQ, ASR, ASS, ASU, ASV, ASW, ASX, ASY, ASZ, AT, ATT, Art, As, At, Ats, BST, EST, HST, IST, NST, OST, RST, ST, VST, WST, alt, ass, est, ACST, AEST, ASTM, AWST, FST, GST, a, and, is, was, an, any, are, but, has, it, not, all, get, most, must, out, part, age, its, set, so, us, use, air, best, got, just, lost, wait, west, West, also, anti, base, case, coast, cost, cut, eight, fact, gas, hit, host, least, let, list, mass, pass, post, put, rest, saw, want, yet, Best, acts, add, ago, aid, am, arm, arts, bass, bit, easy, eye, eyes, hot, lot, say, task, test, Asia, NASA, Post, ad, asks, cash, cat, eat, fit, ft, hat, jet, met, net, salt, sit, taste, vs, waste, wet, AFC, Matt, NSW, ads, aka, arc, asset, aunt, auto, bat, bats, blast, caste, cats, dust, ease, fat, feast, halt, kit, lasts, mask, nest, pet, pit, pot, rats, Acts, Bass, CSS, Case, Hart, ICT, Lt, Mass, Nash, PT, Pat, SAS, SD, SF, SG, SK, SL, SSR, TNT, TT, USA, USB, Walt, ace, ants, ate, beast, bet, bust, cart, casts, dash, dot, eats, eyed, hash, hats, hut, nut, opt, pact, paste, rat, sad, waist, wash, wit, yeast, ACM, ACS, ADC, ADP, AF, ANSI, AOL, APA, APC, ATC, ATM, ATV, Ana, Aston, BMT, BSA, BSD, BT, Bart, CRT, CSA, CSI, CSU, CSX, Cash, DDT, DSM, DSO, DSP, DT, ESA, ESP, GMT, IIT, ISP, LSD, MRT, MSC, MSN, NSA, Oct, PET, PS2, PSA, PSP, PSV, RSA, RSS, Sgt, Taft, Watt, ale, alto, amp, ape, apse, aux, awe, bait, boast, dart, fats, fist, gait, gut, lest, lust, malt, masts, mat, mats, mist, oft, pas, pasta, pest, psi, raft, rash, roast, rot, rust, sac, sap, sax, toast, vase, vest, watt, AAU, ADR, ADSL, AEC, AEG, AIF, AIG, AMX, APG, APS, ARL, ARP, Abe, Ada, Alta, Astor, Aug, Ava, CNT, CSC, CSF, CSL, CSP, CSR, DAT, DSC, DSL, DSS, EMT, ESC, ESL, FSA, FSU, Faust, Host, IAS, ITT, JSA, Kant, MSF, NSF, Nat, OAS, OSI, OSU, PSD, PSL, PSU, RSC, SSA, SSE, SSL, SSP, TSA, USF, USO, USP, WSU, Wash, ah, angst, ante, ark, avg, baht, cask, cyst, fest, gust, hadst, haste, hasty, kart, mash, ms, oats, oust, sash, sh, vat, vet, wasp, ABM, ACTH, ACU, AD, ADD, AEF, AFM, AFN, AGC, AGS, AHS, APD, APM, ARF, ASAP, ASSR, Apr, Ara, Ashe, BLT, CBT, CDT, CSD, CSG, CSV, CVT, DSA, DSB, DSE, ESB, ESD, ESO, ESR, ESS, Esq, FSH, FSI, FSK, FSM, GATT, HSE, HSL, HSP, HSS, IMT, KSC, LCT, LSA, LSC, LSI, LSM, LSO, LSP, Lat, MBT, MSI, MSW, NWT, ORT, OSB, OSF, PDT, PSB, PSE, PSF, PSG, PSM, PSO, PSS, PTT, RSI, RSM, RSV, SALT, SBT, SRT, SSM, SSW, STP, Sat, TSS, USTA, WASP, av, ax, aye, bash, bot, cant, cit, cot, esp, fasts, gash, gasp, jest, lash, lat, mart, oat, pasts, pat, pt, rant, rut, sag, sf, tact, tart, tasty, tat, taut, tot, ts, vats, zest, ACD, ACG, ACH, AGR, APB, APN, Adm, CFT, CLT, DPT, DSD, DSR, DWT, ELT, ESE, Easts, HSA, KSS, LSB, MASH, MCT, MDT, MKT, MTT, Pabst, RPT, RSP, RSX, RTT, SLT, SSO, USG, WSW, abs, abut, aim, arty, aster, bask, dist, gist, hart, lass, pant, pasty, pats, rasp, wart, AFO, CSH, CTT, Catt, EFT, ESN, Tut, WSJ, abet, adj, ado, aha, ain, alp, ashy, awl, dost, ext, haft, jot, jut, masc, nit, rapt, rt, sass, sate, ssh, sty, tut, AHQ, Inst, SASE, Tass, VDT, Zest, adv, ail, app, ascot, canst, fut, gt, inst, lase, pct, pvt, sot, tsp, waft, zit, asps, baste, hasp, tats, AAA, ABC, AMC, ATP, Afr, Ali, Amy, Ann, MIT, PS, USC, USS, alts, assn, astir, ave, dpt, jct, lit, wist, Hts, psst, A, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26, A27, A28, A29, A30, A31, A32, A33, A34, A35, A36, A37, A38, A39, A40, A41, A42, A43, A44, A45, A46, A47, A48, A49, A50, A51, A52, A53, A54, A55, A56, A57, A58, A59, A60, A61, A62, A63, A64, A65, A66, A67, A68, A69, A6M, A70, A71, A72, A73, A74, A75, A76, A77, A78, A79, A80, A81, A82, A83, A84, A85, A86, A87, A88, A89, A90, A91, A92, A93, A94, A95, A96, A97, A98, A99, A9C, AA, AAB, AAC, AAD, AAE, AAF, AAG, AAH, AAI, AAJ, AAK, AAL, AAM, AAN, AAO, AAP, AAQ, AAR, AAS, AAV, AAW, AAX, AAY, AAZ, AB, ABA, ABB, ABD, ABF, ABG, ABH, ABJ, ABK, ABL, ABN, ABP, ABQ, ABR, ABS, ABU, ABV, ABVT, ABW, ABX, ABZ, AC, ACA, ACAT, ACB, ACC, ACF, ACI, ACJ, ACK, ACL, ACN, ACO, ACP, ACR, ACTA, ACTE, ACTP, ACV, ACW, ACX, ACY, ACZ, ADA, ADB, ADF, ADG, ADH, ADJ, ADK, ADL, ADM, ADN, ADQ, ADS, ADSI, ADV, ADX, ADY, ADZ, AE, AEA, AEB, AED, AEE, AEI, AEJ, AEK, AEL, AEM, AEN, AEO, AEP, AEQ, AER, AES, AESA, AESV, AEU, AEV, AEW, AEX, AEZ, AFA, AFAT, AFB, AFD, AFE, AFF, AFG, AFH, AFI, AFJ, AFK, AFL, AFQ, AFR, AFS, AFU, AFV, AFW, AFX, AFZ, AG, AGB, AGD, AGE, AGF, AGG, AGH, AGI, AGJ, AGK, AGL, AGM, AGN, AGO, AGP, AGQ, AGU, AGV, AGW, AGX, AGY, AGZ, AHA, AHB, AHC, AHD, AHE, AHF, AHG, AHH, AHI, AHJ, AHK, AHL, AHM, AHN, AHO, AHR, AHSI, AHTV, AHU, AHV, AHW, AHX, AHY, AHZ, AI, AIB, AIC, AID, AIE, AIH, AII, AIJ, AIK, AIM, AIO, AIP, AIQ, AIS, AITA, AITF, AIU, AIV, AIW, AIY, AIZ, AIs, AJ, AJB, AJC, AJF, AJG, AJH, AJI, AJJ, AJK, AJL, AJN, AJO, AJP, AJQ, AJR, AJS, AJU, AJV, AJW, AJY, AK, AKB, AKC, AKD, AKE, AKF, AKG, AKJ, AKK, AKL, AKM, AKN, AKO, AKP, AKQ, AKR, AKS, AKU, AKV, AKX, AKY, AL, ALAT, ALB, ALC, ALD, ALE, ALG, ALH, ALJ, ALK, ALL, ALM, ALN, ALO, ALP, ALQ, ALR, ALS, ALV, ALW, ALX, ALZ, AM, AMA, AMD, AME, AMF, AMG, AMH, AMJ, AMK, AML, AMM, AMN, AMO, AMP, AMQ, AMR, AMS, AMSL, AMU, AMV, AMW, AMY, AMZ, AN, ANB, ANC, AND, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANQ, ANR, ANS, ANSM, ANTS, ANV, ANW, ANX, ANY, ANZ, AO, AOA, AOB, AOC, AOD, AOE, AOF, AOG, AOH, AOI, AOJ, AOK, AOM, AON, AOO, AOP, AOQ, AOR, AOS, AOU, AOV, AOW, AOX, AP, APE, APF, APH, API, APJ, APK, APL, APO, APP, APQ, APR, APU, APV, APW, APX, APY, APZ, AQA, AQB, AQC, AQF, AQG, AQH, AQI, AQK, AQL, AQM, AQP, AQR, AQS, AQY, AR, ARB, ARC, ARD, ARE, ARH, ARI, ARJ, ARK, ARM, ARN, ARQ, ARR, ARSI, ARTE, ARTS, ARU, ARV, ARW, ARX, ARY, ASBL, ASEM, ASFI, ASGS, ASIN, ASLM, ASMP, ASMR, ASNL, ASPA, ASPI, ASSE, ASVP, AT&amp;T, AT3, ATA, ATB, ATD, ATE, ATF, ATG, ATH, ATI, ATK, ATL, ATN, ATO, ATQ, ATR, ATS, ATU, ATW, ATX, ATY, ATZ, AU, AUA, AUC, AUD, AUE, AUF, AUG, AUI, AUJ, AUL, AUM, AUN, AUO, AUP, AUQ, AUR, AUS, AUU, AUW, AUX, AUY, AUZ, AV, AVA, AVB, AVC, AVE, AVF, AVG, AVH, AVI, AVK, AVL, AVN, AVO, AVP, AVR, AVS, AVSF, AVU, AVV, AVW, AVX, AVY, AVZ, AWA, AWB, AWD, AWE, AWG, AWK, AWM, AWN, AWP, AWR, AWS, AWU, AWZ, AX, AXB, AXC, AXD, AXG, AXK, AXL, AXM, AXN, AXP, AXR, AXS, AXV, AXX, AYA, AYC, AYD, AYE, AYG, AYH, AYI, AYJ, AYL, AYN, AYO, AYP, AYQ, AYR, AYS, AYU, AYY, AYZ, AZ, AZA, AZB, AZD, AZE, AZF, AZI, AZN, AZO, AZS, AZW, AZZ, Ac, Ag, Al, Ala, Am, Ar, Ark, Attn, Au, Av, Ave, BAS, BBT, BCST, BCT, BDT, BEST, BET, BFT, BGT, BHT, BKT, BNT, BPT, BQT, BRT, BS, BSB, BSC, BSE, BSF, BSG, BSH, BSI, BSK, BSL, BSM, BSN, BSO, BSP, BSQ, BSR, BSS, BSU, BSV, BSW, BSX, BTT, BUT, BVT, BWT, BZT, CALT, CAMT, CASA, CASE, CASM, CAT, CCT, CEST, CET, CGT, CHT, CIT, CMT, CQT, CS, CSE, CSM, CSST, CT, CUT, CWT, CYT, CZT, Capt, Cs, Ct, DCT, DET, DFT, DGT, DHT, DJT, DLT, DNT, DOT, DRT, DS, DSF, DSG, DSI, DSK, DSN, DSQ, DSTN, DSU, DSV, Dot, EAS, EAT, EDT, EGT, EIT, ENST, EOST, EPT, ERT, ES, ESF, ESH, ESI, ESM, ESTP, ET, ETT, Es, Esc, FASM, FAT, FET, FFT, FIT, FJT, FMT, FNT, FPT, FRT, FS, FSB, FSC, FSD, FSF, FSG, FSP, FSR, FSS, FSW, GAS, GBT, GDT, GET, GHT, GLT, GNT, GSA, GSC, GSF, GSH, GSI, GSIT, GSK, GSL, GSM, GSO, GSP, GSR, GSW, GT, GUT, GVT, HFT, HGT, HIT, HRT, HS, HSC, HSF, HSM, HSV, HT, IAMT, IASB, IASI, IDT, IHT, INT, IOT, IPT, IRT, IS, ISA, ISB, ISC, ISD, ISF, ISG, ISGT, ISI, ISK, ISL, ISM, ISN, ISO, ISR, ISS, ISTC, ISV, IT, It, JIT, JNT, JS2, JSB, JSF, JSL, JSP, JSU, JSX, JT, KET, KS, KSA, KSM, KSR, KT, KWT, Kit, Ks, LAT, LDT, LFST, LFT, LGT, LNT, LS, LS1, LS2, LS3, LS4, LSAT, LSE, LSF, LSJ, LSL, LSN, LSQ, LSW, LSX, LT, LTT, LUT, Las, Lot, MAS, MAT, MET, MFT, MLT, MMT, MNT, MOT, MPT, MQT, MS, MSA, MSB, MSD, MSE, MSFT, MSG, MSH, MSIT, MSK, MSM, MSO, MSR, MSS, MSTS, MSU, MT, Ms, Mt, Myst, NAS, NBT, NDT, NFT, NMT, NRT, NS, NSB, NSC, NSM, NSS, NSU, NT, NUT, NVT, OAT, OBT, OFT, OIT, OLT, OMT, ONT, OPT, OS, OSA, OSC, OSD, OSG, OSL, OSM, OSQ, OSR, OSS, OSV, OT, OXT, Ont, Os, PAS, PBT, PCT, PGT, PIT, PLT, PMT, PNT, POST, PPT, PRT, PS1, PSC, PSK, PSN, PSQ, PSR, PSTN, PSX, PYT, Pt, Pvt, QAT, QFT, QIT, QSE, QSR, QSV, QVT, RAS, RDT, RNT, ROT, RRT, RS, RS7, RSE, RSF, RSH, RSO, RSR, RSW, RT, RVT, Rasta, S, SA, SADT, SART, SAT, SB, SC, SE, SET, SFT, SGT, SH, SI, SIT, SJ, SM, SMT, SN, SO, SP, SPT, SS, SS7, SSB, SSF, SSG, SSH, SSI, SSK, SSS, SSV, STA, STB, STC, STD, STE, STF, STG, STI, STL, STM, STN, STO, STR, STS, STT, STV, STX, SU, SVT, SW, SWT, SY, SYT, SZ, Sask, Sb, Sc, Se, Set, Si, Sm, Sn, Sp, Sq, Sr, Sta, Ste, Stu, T, TASS, TAT, TBT, TDT, TFT, THT, TKT, TLT, TMT, TOT, TPT, TRT, TS, TSB, TSC, TSD, TSE, TSG, TSH, TSI, TSK, TSL, TSM, TSN, TSO, TSP, TSR, TSV, TSX, TTT, Tet, UAS, UET, UGT, UIT, ULT, UMT, UNT, URT, US, USD, USE, USI, USJ, USN, USR, USTB, USTL, USTM, UT, UVT, Ut, VAT, VCT, VOST, VSB, VSC, VSD, VSF, VSG, VSL, VSM, VSN, VSOT, VSP, VSR, VSS, VSTI, VSV, VT, VTT, VUT, VVT, Vt, WCT, WET, WHT, WKT, WLT, WNT, WRT, WSA, WSC, WSD, WSF, WSM, WSP, WSS, XFT, XRT, XS, XSLT, XSN, YT, ZSE, ZSL, ZSP, Zs, aah, ab, ac, acct, acte, advt, alb, ang, ans, arr, asap, attn, atty, auk, avast, aw, awn, capt, cs, ct, cwt, daft, fart, flt, git, gs, hasn, hgt, hist, ht, int, isl, ism, isn, ks, kt, lats, ls, mas, mayst, mot, nasty, pkt, qt, qts, rs, s, sq, std, t, tit, ult, usu, vasts, wasn, wot, wt, yest, A&amp;A, A&amp;D, A&amp;E, A&amp;M, A&amp;P, A&amp;R, AAWT, ABI, ACE, ACTs, ACh, ADI, ADU, AFP, AGA, AGs, AIX, ALF, ALU, AMI, AOSP, APs, ASIC, ASML, ASOS, ASPR, ASUS, AVM, AVs, AWC, AWF, AXA, Aalst, Abi, Abu, Amit, Aon, Asha, Asif, Astra, Asus, BASF, BSc, CAS, CPT, CSAT, CSET, CSO, DMT, DSAT, DTT, DVT, EASA, EASO, ECT, ENT, ES5, ES6, ESET, ESG, ESTA, ESV, ESY, FAS, FASD, FSE, FSN, FT, GCT, GPT, GSE, GSTs, HCT, HSK, IAT, IGST, ISTA, ISU, IoT, JS, JSC, JWT, Kat, LMT, LSTM, LSU, M&amp;T, MSc, NCT, NIST, NLT, NSO, NTT, NYT, ODT, OSX, PAs, PS3, PS4, PS5, PSAT, PSTG, QS, RET, RSU, SGST, SHT, SSC, SSD, SSN, STW, SV, TAS, TXT, UAT, UPT, USDT, WS, WSB, WSL, WSN, Wasm, XSD, XSL, XSS, YSL, ahh, ai, ais, amu, auth, aww, axe, cts, msg, pts, tase] (30318) [lt:en:MORFOLOGIK_RULE_EN_US]">ast</span> = (s_i -</div><div class="clear"></div>
<div class="linenb">483</div><div class="codeline">s_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [indeed, intend, indent, addend, impend, IDed, Indeed] (30333) [lt:en:MORFOLOGIK_RULE_EN_US]">i^{dend}</span>)$) also results in successful learning, although at the cost of additional training time. The rate-based</div><div class="clear"></div>
<div class="linenb">484</div><div class="codeline">variant was chosen in order to not hinder learning performance any more than necessary<span class="highlight" title="Two consecutive dots. Suggestions: [., â€¦] (30531) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Writing out the convolutions</div><div class="clear"></div>
<div class="linenb">485</div><div class="codeline">in Equation \ref{eq-delta-w-t-T} explicitly, we obtain</div><div class="clear"></div>
<div class="linenb">486</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">487</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">488</div><div class="codeline">  \Delta w_{ij}(t,T) &amp; = \int_t^T dt' F[s_j^\ast, V_i^\ast](t')                                                                           \\</div><div class="clear"></div>
<div class="linenb">489</div><div class="codeline">                     &amp; =  \int_t^T dt' \  \eta\int_0^{t'} dt'' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'') <span class="keyword1">\label</span>{eq-delta-w-t-T-long}</div><div class="clear"></div>
<div class="linenb">490</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">491</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">492</div><div class="codeline">Computing this Equation directly is inefficient due to the nested integrals. Yet, it is possible to break up the</div><div class="clear"></div>
<div class="linenb">493</div><div class="codeline">integrals into two simpler computations and rewrite the weight change as:</div><div class="clear"></div>
<div class="linenb">494</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">495</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">496</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">497</div><div class="codeline">  \Delta W_{ij}(t, T) &amp; = \eta \left[ I_1 (t, T) - I_2(t,T) + I_2(0,t)\left( 1- e^{-\frac{T-t}{\tau_\kappa}} \right) \right] \\</div><div class="clear"></div>
<div class="linenb">498</div><div class="codeline">  I_1(a, b)           &amp; = \int_{a}^{b} dt \ V_i^\ast (t) s_j^\ast (t)                                                        \\</div><div class="clear"></div>
<div class="linenb">499</div><div class="codeline">  I_2(a, b)           &amp; = \int_{a}^{b} dt \ e^{-\frac{b-t}{\tau_\kappa}} V_i^\ast (t) s_j^\ast (t)                           \\</div><div class="clear"></div>
<div class="linenb">500</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">501</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">502</div><div class="codeline">See Section \todo{ref} for a rigorous proof that this is in fact <span class="highlight" title="A word may be missing after 'the'. (30849) [lt:en:THE_SENT_END]">the .</span> The resulting equations allow for a rather</div><div class="clear"></div>
<div class="linenb">503</div><div class="codeline">efficient computation of weight changes compared to the complex integral described in Equation</div><div class="clear"></div>
<div class="linenb">504</div><div class="codeline">\ref{eq-delta-w-t-T-long}. This integration is performed whenever a spike traverses a synapse. It generalizes to all</div><div class="clear"></div>
<div class="linenb">505</div><div class="codeline">special cases in Equations \ref{eq-delta_w_up}-\ref{eq-delta_w_down}, as long as the appropriate dendritic error is</div><div class="clear"></div>
<div class="linenb">506</div><div class="codeline">stored by the postsynaptic neuron.</div><div class="clear"></div>
<div class="linenb">507</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">508</div><div class="codeline"><span class="keyword1">\section</span>{Latent Equilibrium}<span class="keyword1">\label</span>{sec-latent-eq}</div><div class="clear"></div>
<div class="linenb">509</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">510</div><div class="codeline">The most significant drawback of the Sacramento model is the previously mentioned requirement for long stimulus</div><div class="clear"></div>
<div class="linenb">511</div><div class="codeline">presentation times and appropriately low learning rates. This makes the network prohibitively inefficient for the large</div><div class="clear"></div>
<div class="linenb">512</div><div class="codeline">networks required for complex learning tasks. Sacramento <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>developed a steady-state approximation of their network</div><div class="clear"></div>
<div class="linenb">513</div><div class="codeline">which models the state of the network after it has balanced out in response to a stimulus-target pair. It does not</div><div class="clear"></div>
<div class="linenb">514</div><div class="codeline">suffer from these issues and shows that their model can in principle solve more demanding learning tasks such as <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MOIST, MIST, MONIST, MN IST, NIST] (31800) [lt:en:MORFOLOGIK_RULE_EN_US]">MNIST</span>.</div><div class="clear"></div>
<div class="linenb">515</div><div class="codeline">Yet these types of approximation are much further detached from biological neurons than the original model and thus do</div><div class="clear"></div>
<div class="linenb">516</div><div class="codeline">not lend themselves well to an investigation of biological plausibility \citep{Gerstner2009}. Furthermore, the</div><div class="clear"></div>
<div class="linenb">517</div><div class="codeline">approximation is unsuitable for and investigation of spike-based communication, since the steady state of both network</div><div class="clear"></div>
<div class="linenb">518</div><div class="codeline">ideally would be the same. Thus, neither the fully modeled neuron dynamics nor the steady-state approximation are suited</div><div class="clear"></div>
<div class="linenb">519</div><div class="codeline">for complex learning tasks. A substantial improvement to rate neurons which promises to solve this dilemma was developed</div><div class="clear"></div>
<div class="linenb">520</div><div class="codeline"><span class="highlight-sh" title="Do not use 'by [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">by \cite{</span>Haider2021}, and will be discussed here.</div><div class="clear"></div>
<div class="linenb">521</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">522</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">523</div><div class="codeline">The requirement for long stimulus presentation times of the dendritic error network is caused by the slow development of</div><div class="clear"></div>
<div class="linenb">524</div><div class="codeline">leaky neuron dynamics, and is therefore not unique to this model. When a stimulus-target pair is presented to the</div><div class="clear"></div>
<div class="linenb">525</div><div class="codeline">network, membrane potentials in all neurons slowly evolve until a steady state is reached. The time until a network of</div><div class="clear"></div>
<div class="linenb">526</div><div class="codeline">has reached this state after a change in input is called the <span class="keyword1">\textit</span>{relaxation period} following \cite{Haider2021}.</div><div class="clear"></div>
<div class="linenb">527</div><div class="codeline">Given a membrane time constant $\tau_m$, a feedforward network with $N$ layers of leaky neurons thus has a relaxation</div><div class="clear"></div>
<div class="linenb">528</div><div class="codeline">time constant of $N \tau_m$. Yet in our case, a target activation simultaneously injected into the output neurons slowly</div><div class="clear"></div>
<div class="linenb">529</div><div class="codeline">propagates backwards through the highly recurrent network. <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Neurons] (33155) [lt:en:UPPERCASE_SENTENCE_START]">neurons</span> at early layers require subsequent layers to be fully</div><div class="clear"></div>
<div class="linenb">530</div><div class="codeline">relaxed in order to correctly compute their dendritic error terms, effectively being dependent on two network passes.</div><div class="clear"></div>
<div class="linenb">531</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (33335) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>state that this kind of network therefore requires $2N\tau_m$ to relax in response to a given input-output</div><div class="clear"></div>
<div class="linenb">532</div><div class="codeline">pairing. This prediction proved quite accurate in experiments, as shown in Fig. \ref{fig-error-comp-le}.</div><div class="clear"></div>
<div class="linenb">533</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">534</div><div class="codeline">This is a major issue, as it implies that plasticity during the first few <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [milliseconds] (33607) [lt:en:MORFOLOGIK_RULE_EN_US]">miliseconds</span> of a stimulus presentation is</div><div class="clear"></div>
<div class="linenb">535</div><div class="codeline">driven by faulty error terms. The network thus tends to 'overshoot', and needs to undo the synaptic weight changes made</div><div class="clear"></div>
<div class="linenb">536</div><div class="codeline">during the relaxation period in the later phase of a stimulus presentation, in order to make tangible progress on the</div><div class="clear"></div>
<div class="linenb">537</div><div class="codeline">learning task. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (33902) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>call this issue the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>relaxation problem<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> and suggest that it might be inherent to most</div><div class="clear"></div>
<div class="linenb">538</div><div class="codeline">established attempts at biologically plausible <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (34050) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> algorithms</div><div class="clear"></div>
<div class="linenb">539</div><div class="codeline">\citep{Whittington2017,guerguiev2017towards,sacramento2018dendritic,millidge2020activation}.</div><div class="clear"></div>
<div class="linenb">540</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">541</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">542</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">543</div><div class="codeline">The choice to simply increase presentation time to compensate for the relaxation period is therefore somewhat</div><div class="clear"></div>
<div class="linenb">544</div><div class="codeline">problematic. It implicitly tolerates adverse synaptic plasticity in all synapses, which are counteracted by enforcing</div><div class="clear"></div>
<div class="linenb">545</div><div class="codeline">the desired plasticity for a longer time. Physiological changes that are meant to immediately be undone are of course an</div><div class="clear"></div>
<div class="linenb">546</div><div class="codeline">inefficient use of a brain's resources, which can be considered highly untypical for a biological system. One possible</div><div class="clear"></div>
<div class="linenb">547</div><div class="codeline">solution to this is to decrease synaptic time constants and remove the temporal filtering of stimulus injections. Yet</div><div class="clear"></div>
<div class="linenb">548</div><div class="codeline">this does not solve the fundamental issue that during a substantial portion of stimulus presentations, the network is</div><div class="clear"></div>
<div class="linenb">549</div><div class="codeline">driven by erroneous plasticity. Removing temporal filtering does decrease the length of the relaxation period, but</div><div class="clear"></div>
<div class="linenb">550</div><div class="codeline">causes a drastic increase in dendritic error values during that period. Therefore, while improving response time, this</div><div class="clear"></div>
<div class="linenb">551</div><div class="codeline">change effectively impedes learning. Another possible solution is to disable plasticity for the first few milliseconds</div><div class="clear"></div>
<div class="linenb">552</div><div class="codeline">of stimulus presentation. After the network has relaxed, the plasticity rules produce useful weight changes and learning</div><div class="clear"></div>
<div class="linenb">553</div><div class="codeline">rates can consequently be safely increased. Yet a mechanism by which neurons could implement this style of phased</div><div class="clear"></div>
<div class="linenb">554</div><div class="codeline">plasticity is yet to be found, making this approach questionable in terms of biological plausibility. Furthermore, it</div><div class="clear"></div>
<div class="linenb">555</div><div class="codeline">introduces a requirement for external control to the network, a trait that is considered highly undesirable for</div><div class="clear"></div>
<div class="linenb">556</div><div class="codeline">approximations of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (35625) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> \citep{whittington2019theories}. Ideally, the relaxation period would be skipped or</div><div class="clear"></div>
<div class="linenb">557</div><div class="codeline">shortened, in order to reduce the erroneous plasticity. This would allow for a loosening of the constraints put on</div><div class="clear"></div>
<div class="linenb">558</div><div class="codeline">presentation time and learning rates, thus increasing computational efficiency. \newline</div><div class="clear"></div>
<div class="linenb">559</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">560</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">561</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">562</div><div class="codeline">The approach proposed by <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (35922) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>is to change the parameter of the activation function $\phi$, a mechanism called</div><div class="clear"></div>
<div class="linenb">563</div><div class="codeline"><span class="keyword1">\textit</span>{Latent Equilibrium} (LE). Neurons in the original dendritic error network (henceforth called <span class="keyword1">\textit</span>{Sacramento</div><div class="clear"></div>
<div class="linenb">564</div><div class="codeline">neurons}) transmit a function of their somatic potential $u_i$, which is updated through Euler integration at every</div><div class="clear"></div>
<div class="linenb">565</div><div class="codeline">simulation step (Equation \ref{eq-r-t-sacramento}). In contrast, neurons using Latent Equilibrium (henceforth called</div><div class="clear"></div>
<div class="linenb">566</div><div class="codeline"><span class="keyword1">\textit</span>{LE neurons}) transmit a function of what the somatic potential is expected to be in the future. To calculate</div><div class="clear"></div>
<div class="linenb">567</div><div class="codeline">this expected future somatic potential $\breve{u}$, the integration is performed with a larger Euler step:</div><div class="clear"></div>
<div class="linenb">568</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">569</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">570</div><div class="codeline">  u_i(t+ \Delta t)          &amp; = u_i(t) + \dot{u}_i(t) \ \Delta t <span class="keyword1">\label</span>{eq-r-t-sacramento} \\</div><div class="clear"></div>
<div class="linenb">571</div><div class="codeline">  \breve{u}_i(t + \Delta t) &amp; = u_i(t) + \dot{u}_i(t) \ \tau_{eff} <span class="keyword1">\label</span>{eq-r-t-haider}</div><div class="clear"></div>
<div class="linenb">572</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">573</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">574</div><div class="codeline">Instead of broadcasting their rate based on the current somatic potential ($r_i(t) = \phi(u_i(t))$), LE neurons send</div><div class="clear"></div>
<div class="linenb">575</div><div class="codeline">their predicted future activation, denoted as $\breve{r}_i(t) = \phi(\breve{u}_i(t))$. The degree to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [with, which, rich, wish, Rich, witch, wick, winch, Mich, ICH, WIC, Wish] (36686) [lt:en:MORFOLOGIK_RULE_EN_US]">wich</span> LE neurons</div><div class="clear"></div>
<div class="linenb">576</div><div class="codeline">look ahead is determined by the <span class="keyword1">\textit</span>{effective membrane time constant} <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Jeff, eff] (36767) [lt:en:MORFOLOGIK_RULE_EN_US]">$\tau_{eff}</span> = \frac{C_m}{g_l + <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [gas, grabs, Gibbs, Abbas, GBA, galas, tubas, gobs, gibes, gabs, BAS, GAS, GBS, GoiÃ¡s, babas] (36783) [lt:en:MORFOLOGIK_RULE_EN_US]">g^{bas}</span> +</div><div class="clear"></div>
<div class="linenb">577</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [gain, rapid, gap, grain, graphic, grape, graph, gaps, grapes, graphs, Gavin, gait, goalie, Gaia, Gracie, Grail, gape, gaping, Apia, grail, gratis, tapir, vapid, grappa, gravid, okapi, Gail, gaped, gapes, gratin, gamin, lapin, okapis, gappy, API, APII, ATAPI, DAPI, GAFI, GAIN, GANIL, GAP, GPIS, Gap, MAPI, ODAPI, TAPI, APIs, GAAP, Ghazi, Gmail] (36791) [lt:en:MORFOLOGIK_RULE_EN_US]">g^{api}}$</span>. This time constant takes into account the conductance with which dendritic compartments leak into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (36901) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span>,</div><div class="clear"></div>
<div class="linenb">578</div><div class="codeline">which is a key driving factor for the speed at which the network relaxes. Any computations that employ or relate to this</div><div class="clear"></div>
<div class="linenb">579</div><div class="codeline">prediction of future network states will henceforth be referred to as <span class="keyword1">\textit</span>{prospective} and denoted with a breve</div><div class="clear"></div>
<div class="linenb">580</div><div class="codeline">($\breve{x}$).</div><div class="clear"></div>
<div class="linenb">581</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">582</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">583</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">584</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">585</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_le_response}</div><div class="clear"></div>
<div class="linenb">586</div><div class="codeline">  <span class="keyword1">\caption</span>{Signal transmission between an input neuron $i$ and a hidden layer pyramidal neuron $j$. Depicted are</div><div class="clear"></div>
<div class="linenb">587</div><div class="codeline">    activations for the original Sacramento model and prospective activation using Latent Equilibrium. <span class="keyword1">\textbf</span>{A:}</div><div class="clear"></div>
<div class="linenb">588</div><div class="codeline">    Current injected into the input neuron. Membrane potential slowly adapts to match this (not shown) <span class="keyword1">\textbf</span>{B:}</div><div class="clear"></div>
<div class="linenb">589</div><div class="codeline">    Activation of the input neuron using instantaneous- $\phi(u_i)$ (blue), and prospective activation</div><div class="clear"></div>
<div class="linenb">590</div><div class="codeline">    $\phi(\breve{u}_i)$ (orange). Note how strongly prospective activation reacts to changes in somatic voltage, leading</div><div class="clear"></div>
<div class="linenb">591</div><div class="codeline">    to 'bursts' in neuron output. After the input neuron has reached its relaxed state ($\dot{u}_i = 0$), both types of</div><div class="clear"></div>
<div class="linenb">592</div><div class="codeline">    neuron evoke the same activation. <span class="keyword1">\textbf</span>{C:} Somatic potential $u_j$ of the pyramidal neuron responding to signals</div><div class="clear"></div>
<div class="linenb">593</div><div class="codeline">    sent from the input neuron (color scheme as in B).}</div><div class="clear"></div>
<div class="linenb">594</div><div class="codeline">  <span class="keyword1">\label</span>{fig-comparison-le}</div><div class="clear"></div>
<div class="linenb">595</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">596</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">597</div><div class="codeline">When employing the default parametrization given by <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (37195) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>\todo{reference parameter table}, $\tau_{eff}$ is</div><div class="clear"></div>
<div class="linenb">598</div><div class="codeline">slightly lower than reported pyramidal neuron time constants \citep{McCormick1985} at approximately $5.26ms$. When</div><div class="clear"></div>
<div class="linenb">599</div><div class="codeline">presynaptic neurons employ prospective dynamics, postsynaptic neurons approach their steady state much more quickly, as</div><div class="clear"></div>
<div class="linenb">600</div><div class="codeline">depicted in Fig. \ref{fig-comparison-le}. In intuitive terms, prospective activation is more strongly dependent on the</div><div class="clear"></div>
<div class="linenb">601</div><div class="codeline">derivative membrane potential compared to the instantaneous activation. This results in drastic changes in activation</div><div class="clear"></div>
<div class="linenb">602</div><div class="codeline">in response to changes in the somatic membrane potential. While this can lead to an overshoot of postsynaptic activity,</div><div class="clear"></div>
<div class="linenb">603</div><div class="codeline">under careful parametrization it strongly decreases response time.   </div><div class="clear"></div>
<div class="linenb">604</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">605</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">606</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">607</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">608</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_le_dendritic_errors}</div><div class="clear"></div>
<div class="linenb">609</div><div class="codeline">  <span class="keyword1">\caption</span>{Effects of Latent equilibrium dynamics on the dendritic error terms from Equations</div><div class="clear"></div>
<div class="linenb">610</div><div class="codeline">    \ref{eq-delta_w_up}-\ref{eq-delta_w_down}. Depicted are error terms for individual spiking neurons in a network with</div><div class="clear"></div>
<div class="linenb">611</div><div class="codeline">    one hidden layer (N=3). The network was fully trained on the Bars dataset (cf. Section \ref{sec-le-tpres}), so</div><div class="clear"></div>
<div class="linenb">612</div><div class="codeline">    errors should ideally relax to zero. Note, that this does not happen here due to the fluctuations inherent to the</div><div class="clear"></div>
<div class="linenb">613</div><div class="codeline">    spiking network variant which was employed. In the original dendritic error network (orange), dendritic errors</div><div class="clear"></div>
<div class="linenb">614</div><div class="codeline">    exhibit longer and more intense deviations, while errors in an identical LE network (blue) relax much sooner.</div><div class="clear"></div>
<div class="linenb">615</div><div class="codeline">    <span class="keyword1">\textbf</span>{A:} Basal dendritic error for a pyramidal neuron at the output layer. <span class="keyword1">\textbf</span>{B:} Dendritic error for a</div><div class="clear"></div>
<div class="linenb">616</div><div class="codeline">    hidden layer Interneuron. <span class="keyword1">\textbf</span>{C:} Proximal apical error for a hidden layer Pyramidal neuron. <span class="keyword1">\textbf</span>{D:} Distal</div><div class="clear"></div>
<div class="linenb">617</div><div class="codeline">    apical error for the same pyramidal neuron. Note that this error term does not converge close to zero for either</div><div class="clear"></div>
<div class="linenb">618</div><div class="codeline">    network after the relaxation period, an issue that will be discussed in Section \ref{sec-feedback-plast}.}</div><div class="clear"></div>
<div class="linenb">619</div><div class="codeline">  <span class="keyword1">\label</span>{fig-error-comp-le}</div><div class="clear"></div>
<div class="linenb">620</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">621</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">622</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">623</div><div class="codeline">When employing prospective dynamics in the dendritic error networks, local error terms of pyramidal- and interneurons</div><div class="clear"></div>
<div class="linenb">624</div><div class="codeline">relax much faster, as shown in Fig. \ref{fig-error-comp-le}. These results highlight the superiority of LE for</div><div class="clear"></div>
<div class="linenb">625</div><div class="codeline">learning in this network, as the relaxation period is almost instantaneous. In contrast, the error terms in the original</div><div class="clear"></div>
<div class="linenb">626</div><div class="codeline">dendritic error network drive random synaptic plasticity even when the network is fully trained on a given dataset and</div><div class="clear"></div>
<div class="linenb">627</div><div class="codeline">is able to make accurate predictions. Thus, both the issue of redundant weight changes, <span class="highlight" title="Probable usage error. Use 'and' after 'both'.. Suggestions: [and] (38404) [lt:en:BOTH_AS_WELL_AS]">as well as</span> the concern over</div><div class="clear"></div>
<div class="linenb">628</div><div class="codeline">response and learning speed can be solved by LE. The authors furthermore show, that learning with this mechanism is </div><div class="clear"></div>
<div class="linenb">629</div><div class="codeline">indifferent to presentation times or effective time constant for rate neurons.</div><div class="clear"></div>
<div class="linenb">630</div><div class="codeline">In addition to using the prospective somatic potential for the neuronal transfer function, it is also used in the</div><div class="clear"></div>
<div class="linenb">631</div><div class="codeline">plasticity rule of LE neurons. The <span class="highlight-spelling" title="Possible spelling mistake found. (38777) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity is therefore updated to compute dendritic error from</div><div class="clear"></div>
<div class="linenb">632</div><div class="codeline">prospective somatic activations and a non-prospective dendritic potential <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PW, cw, MW, CW, EW, KW, NW, SW, TW, W, WW, YW, aw, kW, kw, ow, w, DW, FW, GW, HW, UW, VW, mW] (38930) [lt:en:MORFOLOGIK_RULE_EN_US]">$\dot{w}</span>_{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [IJ, in, is, I, it, if, ID, IQ, id, PJ, RJ, IH, DJ, IP, AIJ, AJ, CIJ, CJ, FIJ, FJ, GJ, IA, IE, IF, IJF, IJM, IL, IN, IPJ, IR, IS, IT, IV, IZ, Ia, In, Io, Ir, It, J, KJ, MJ, NJ, OJ, PIJ, SJ, TJ, VJ, i, ii, iv, ix, j, pj, BJ, IB, IC, ICJ, IG, IM, IU, JJ, kJ] (38933) [lt:en:MORFOLOGIK_RULE_EN_US]">ij}</span>= \eta \ <span class="highlight" title="Don't put a space after the opening parenthesis.. Suggestions: [(] (38944) [lt:en:COMMA_PARENTHESIS_WHITESPACE]">(</span></div><div class="clear"></div>
<div class="linenb">633</div><div class="codeline">\phi(\breve{u}_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [idiom, Epsom, IOM, bosom, besom, IEOM, ISAM, ISCM, ISM, ISO, ISOC, ism, ISOs, ITSM] (38953) [lt:en:MORFOLOGIK_RULE_EN_US]">i^{som}</span>) - \phi(\hat{v}_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [ideas, Abbas, IAS, ICBMs, Iqbal, ibis, IRAs, Incas, tubas, iotas, BAS, IBA, IBAN, IFAS, IGAS, INBS, IRAS, ISAS, Iowas, IBANs, IBS, IPAs, ISBNs, ITAs, IaaS, babas] (38969) [lt:en:MORFOLOGIK_RULE_EN_US]">i^{bas}</span>)<span class="highlight" title="Don't put a space before the closing parenthesis.. Suggestions: [)] (38975) [lt:en:COMMA_PARENTHESIS_WHITESPACE]"> )</span> \ \phi(\breve{u}_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Jason, Epsom, bosom, besom, JSON, JOSM] (38987) [lt:en:MORFOLOGIK_RULE_EN_US]">j^{som}</span>)<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [TO, AT, IT, ITS, ST, ETC, MTV, FT, LTD, BTW, CTV, LT, PT, TT, UTC, ATE, ATC, ATM, ATV, BT, DT, ITF, ITU, MTA, MTR, RTL, RTS, TTC, TV, WTA, WTO, TA, CTA, CTC, DTM, DTS, ETA, FTA, FTC, GTA, GTE, ITC, ITT, KTM, MTS, OTC, PTC, TTL, WTC, CTF, CTI, CTM, CTO, CTP, DTC, DTI, MTB, MTC, MTU, NTP, PTH, PTO, PTT, PTV, RTA, STP, FTP, TS, CTH, DTA, DTP, ITE, KTP, MTO, MTT, RTT, RTU, TTA, NTH, CTT, MTG, OTB, TTF, RT, STY, GT, ETD, YTD, ATP, ITV, LTE, TD, QTY, HTS, 0TH, 1TH, 2TH, 3TH, 4TH, 5TH, 6TH, 7TH, 8TH, 9TH, AT3, ATA, ATB, ATD, ATF, ATG, ATH, ATI, ATK, ATL, ATN, ATO, ATQ, ATR, ATS, ATT, ATU, ATW, ATX, ATY, ATZ, BTA, BTB, BTC, BTF, BTG, BTH, BTI, BTK, BTL, BTM, BTN, BTO, BTP, BTQ, BTR, BTS, BTT, BTU, BTV, BTX, CT, CTB, CTE, CTK, CTR, DTU, DTV, DTW, ET, ETB, ETF, ETL, ETM, ETP, ETS, ETT, ETV, ETZ, FTF, FTG, FTI, FTM, FTO, FTQ, FTV, FTW, GT3, GT4, GT6, GTB, GTG, GTI, GTK, GTM, GTO, GTP, GTQ, GTR, GTS, HT, HTA, HTB, HTF, HTL, HTO, HTP, HTV, ITA, ITB, ITK, ITL, ITP, ITO, JT, JTA, JTC, JTL, KT, KTD, KTS, LTA, LTF, LTI, LTL, LTN, LTR, LTT, LTU, MT, MTD, MTF, MTL, MTN, MTP, MTQ, MTW, MTZ, NT, NTC, NTD, NTL, NTR, NTV, NTW, OT, OTG, OTN, OTO, OTP, OTU, PTA, PTB, PTE, PTF, PTG, PTI, PTM, PTP, PTR, PTU, PTY, PTZ, QT8, RTB, RTC, RTD, RTE, RTF, RTG, RTM, RTN, RTO, RTP, RTR, RTV, RTW, STA, STB, STC, STD, STE, STF, STG, STI, STL, STM, STN, STO, STR, STS, STT, STV, STX, STU, T, TB, TC, TF, TG, TH, TI, TJ, TK, TL, TM, TN, TP, TR, TTD, TTI, TTO, TTP, TTR, TTS, TTT, TTU, TTX, TTY, TU, TW, TX, TY, TZ, TE, UT, UT1, UTA, UTF, UTG, UTL, UTP, UTV, UTE, VT, VTA, VTC, VTP, VTT, WTF, WTH, WTN, WTP, XTC, XTP, YT, YTL, YTO, YTV, YTZ, ZTI, CTN, QT, QTS, WT, Ã‰TP, CTG, CTS, CTU, CTs, DTD, DTT, ETH, ETR, FTD, FTE, FTX, GTC, GTD, GTX, HTC, ITN, JTF, LTC, LTG, LTS, LTV, MTX, NTI, NTS, NTT, OTA, RTI, RTX, RTs, STW, TQ, TTG, TTs, UTI, UTM, ZTE, PTS] (38993) [lt:en:MORFOLOGIK_RULE_EN_US]">^T$</span>. Much like for the transfer function,</div><div class="clear"></div>
<div class="linenb">634</div><div class="codeline">this change serves to increase the responsiveness of the network to input changes.</div><div class="clear"></div>
<div class="linenb">635</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">636</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">section</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Implementation, Implementations] (39119) [lt:en:MORFOLOGIK_RULE_EN_US]">Implementational</span> details}</div><div class="clear"></div>
<div class="linenb">637</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">638</div><div class="codeline">Building on the neuron and plasticity model <span class="highlight-sh" title="Do not use 'from [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">from \cite{</span>Stapmanns2021}, a replicate model of the pyramidal neuron which</div><div class="clear"></div>
<div class="linenb">639</div><div class="codeline">employs spiking communication was developed in NEST. The existing <span class="highlight-spelling" title="Possible spelling mistake found. (39313) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> neuron was expanded to three</div><div class="clear"></div>
<div class="linenb">640</div><div class="codeline">compartments, and storage and readout of dendritic errors were updated  to allow for compartment-specific plasticity</div><div class="clear"></div>
<div class="linenb">641</div><div class="codeline">rules. Interneurons were chosen to be modeled as pyramidal neurons with slightly updated parameters and apical</div><div class="clear"></div>
<div class="linenb">642</div><div class="codeline">conductance $g^{api}=0$. Since membrane dynamics of both neurons follow the same principles and additional compartments</div><div class="clear"></div>
<div class="linenb">643</div><div class="codeline">have minor impact on performance, this was deemed sufficient.</div><div class="clear"></div>
<div class="linenb">644</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">645</div><div class="codeline">After facing some setbacks when attempting to train the first spiking variant of the network, the decision was made to</div><div class="clear"></div>
<div class="linenb">646</div><div class="codeline">also implement a rate-based variant of the neuron in NEST.  While the additional effort required for another</div><div class="clear"></div>
<div class="linenb">647</div><div class="codeline">implementation might be questionable, this model turned out to be <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [indispensable] (40052) [lt:en:MORFOLOGIK_RULE_EN_US]">indispensible</span>. It enabled the identification of both</div><div class="clear"></div>
<div class="linenb">648</div><div class="codeline">errors in the model, <span class="highlight" title="Probable usage error. Use 'and' after 'both'.. Suggestions: [and] (40126) [lt:en:BOTH_AS_WELL_AS]">as well as</span> training mechanisms and parameters that required changes to enable spike-compatible</div><div class="clear"></div>
<div class="linenb">649</div><div class="codeline">learning. The rate version in NEST additionally served to distinguish discrepancies that are due to the novel simulation</div><div class="clear"></div>
<div class="linenb">650</div><div class="codeline">backend from those that were introduced by the spike-based communication scheme.</div><div class="clear"></div>
<div class="linenb">651</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">652</div><div class="codeline">Following NEST convention, the spiking and rate-based neuron models were named \texttt{pp\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40508) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cold, bond, cone, cord, pond, Bond, con, fond, cod, Cong, coed, condo, CND, coned, cont, conk, conj, COD, CON, Cod, Conn, MOND, OND, cons, contd, cony, COPD, Conda] (40519) [lt:en:MORFOLOGIK_RULE_EN_US]">cond</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40526) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>allowbreak</div><div class="clear"></div>
<div class="linenb">653</div><div class="codeline">exp\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40543) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MC, me, my, BC, Mr, cm, mm, PC, MA, Mac, JC, MSC, MV, Mk, QC, ma, ml, BMC, DMC, MBC, MCG, MCP, MDC, MF, MHC, MX, WC, mic, ms, FMC, HMC, IMC, MCB, MCD, MCL, MTC, MVC, MWC, PMC, RMC, MCE, MCF, MCO, MCR, MCT, MCU, mp, AMC, MCA, MD, MW, mg, AC, Ac, C, CC, CMC, DC, EC, EMC, FC, KC, LC, LMC, M, MB, MCH, MCI, MCJ, MCM, MCS, MCZ, ME, MEC, MFC, MG, MH, MI, MJ, MJC, MK, ML, MLC, MM, MMC, MN, MO, MOC, MP, MPC, MR, MRC, MS, MT, MU, MUC, MY, MZ, Mb, Md, Me, Mg, Mn, Mo, Ms, Mt, NC, OMC, RC, SC, SMC, Sc, TC, TMC, Tc, UC, VC, VMC, ZC, ac, c, cc, kc, m, mac, mi, mo, mu, Â°C, GC, GMC, HC, IC, MCC, MCK, MCQ, MCX, MCs, MNC, MSc, NMC, OC, UMC, YC, bc, mCi, mL, mW, mcg, pc] (40554) [lt:en:MORFOLOGIK_RULE_EN_US]">mc</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40559) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [despite, respite] (40570) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr}\footnote{Despite</span> being somewhat cryptic, the name does actually make sense, as it</div><div class="clear"></div>
<div class="linenb">654</div><div class="codeline">describes some key features of the model: It is a <span class="keyword1">\textbf</span>{point process} for <span class="keyword1">\textbf</span>{cond}uctance based synapses and has</div><div class="clear"></div>
<div class="linenb">655</div><div class="codeline">an <span class="keyword1">\textbf</span>{exp}onentially decaying membrane in <span class="keyword1">\textbf</span>{multiple compartments}.} <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [And] (40810) [lt:en:UPPERCASE_SENTENCE_START]">and</span> \texttt{rate\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40821) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>allowbreak</div><div class="clear"></div>
<div class="linenb">656</div><div class="codeline">neuron\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40841) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [per, PR, par, pyre, PMR, PVR, pry, ppr, AYR, BYR, MYR, PAR, PBR, PCR, PER, PFR, PHR, PLR, PNR, POR, PPR, PQR, PRR, PSR, PTR, PUR, PWR, PYF, PYG, PYT, Pr, Pym, RYR, SYR, pr, pyx, yr, p yr, PDR] (40852) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr}</span> respectively. Furthermore, the \texttt{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [per, PR, par, pyre, PMR, PVR, pry, ppr, AYR, BYR, MYR, PAR, PBR, PCR, PER, PFR, PHR, PLR, PNR, POR, PPR, PQR, PRR, PSR, PTR, PUR, PWR, PYF, PYG, PYT, Pr, Pym, RYR, SYR, pr, pyx, yr, p yr, PDR] (40887) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40893) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> synapse} class was defined for spike</div><div class="clear"></div>
<div class="linenb">657</div><div class="codeline">events, and implements the event-based variant of the <span class="highlight-spelling" title="Possible spelling mistake found. (40994) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity described in Section</div><div class="clear"></div>
<div class="linenb">658</div><div class="codeline">\ref{sec-event-urb}. The \texttt{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [per, PR, par, pyre, PMR, PVR, pry, ppr, AYR, BYR, MYR, PAR, PBR, PCR, PER, PFR, PHR, PLR, PNR, POR, PPR, PQR, PRR, PSR, PTR, PUR, PWR, PYF, PYG, PYT, Pr, Pym, RYR, SYR, pr, pyx, yr, p yr, PDR] (41048) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (41054) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> synapse\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (41075) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> rate} model on the other hand transmits rate</div><div class="clear"></div>
<div class="linenb">659</div><div class="codeline">events and updates its weight according to the original plasticity rule.</div><div class="clear"></div>
<div class="linenb">660</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">661</div><div class="codeline">Simulations were managed using the python API \texttt{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Priest, Finest, Nest, Ernest, Honest, Pest, Pines, Panes, Purest, Pyres, Palest, Sanest, Pones, Openest, Piniest, Pinkest, Puniest, Punkest, Pyxes, Yest] (41250) [lt:en:MORFOLOGIK_RULE_EN_US]">PyNEST}</span> \citep{Eppler2009}, which is much more convenient than the</div><div class="clear"></div>
<div class="linenb">662</div><div class="codeline">SLI interface that lies at the core of NEST. An additional advantage of using this language is, that the LE network  is</div><div class="clear"></div>
<div class="linenb">663</div><div class="codeline">also implemented in python. Thus, by including a slightly modified version of that code in my project, it was possible</div><div class="clear"></div>
<div class="linenb">664</div><div class="codeline">to unify all three variants in a single network class and accompanying interface. This allowed for exact alignment of</div><div class="clear"></div>
<div class="linenb">665</div><div class="codeline">network stimulation and readout and enabled in-depth comparative analyses. In <span class="highlight" title="If the text is a generality, 'of the' is not necessary.. Suggestions: [some] (41736) [lt:en:SOME_OF_THE]">some of the</span> upcoming Results, three</div><div class="clear"></div>
<div class="linenb">666</div><div class="codeline">variants of the same network architecture will therefore be compared; The modified python implementation from</div><div class="clear"></div>
<div class="linenb">667</div><div class="codeline">\citep{Haider2021} is termed \texttt{NumPy} based on the framework that is used to compute neuron dynamics and synaptic</div><div class="clear"></div>
<div class="linenb">668</div><div class="codeline">plasticity through matrix multiplication. The two NEST variants will be referred to as <span class="keyword1">\textit</span>{NEST spiking} and</div><div class="clear"></div>
<div class="linenb">669</div><div class="codeline"><span class="keyword1">\textit</span>{NEST rate}. </div><div class="clear"></div>
<div class="linenb">670</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">671</div><div class="codeline"><span class="keyword1">\subsection</span>{Neuron model Adaptations}</div><div class="clear"></div>
<div class="linenb">672</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">673</div><div class="codeline">The neuron model <span class="highlight-sh" title="Do not use 'from [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">from \cite{</span>Stapmanns2021} was modified in some ways in order to match the pyramidal neuron</div><div class="clear"></div>
<div class="linenb">674</div><div class="codeline">implementation more closely. Both the inclusion of nonzero reversal potentials and the flow of currents from the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, LomÃ©, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, SofÃ­a, Sonoma, Souza, TOML, TomÃ¡s, UOM, Zora, hola, momma, sRNA, semÃ©, simp, socs, stomas, Å koda] (42325) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> to</div><div class="clear"></div>
<div class="linenb">675</div><div class="codeline">the dendrites were omitted in my model. Furthermore, the present network requires synapses to be able</div><div class="clear"></div>
<div class="linenb">676</div><div class="codeline">change the sign of their weight at runtime, which is not permitted in the original synapse model. For this reason, the</div><div class="clear"></div>
<div class="linenb">677</div><div class="codeline">strict separation of excitatory and inhibitory synapses had to be removed from the synapse model. In order to compare</div><div class="clear"></div>
<div class="linenb">678</div><div class="codeline">the different implementations exactly, the ODE solver with variable <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [step size] (42740) [lt:en:MORFOLOGIK_RULE_EN_US]">stepsize</span> was replaced with Euler</div><div class="clear"></div>
<div class="linenb">679</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (42773) [lt:en:MORFOLOGIK_RULE_EN_US]">integrations\footnote{This</span> change initially served debugging purposes, but turned out to have no negative effect on</div><div class="clear"></div>
<div class="linenb">680</div><div class="codeline">performance and was therefore kept} with step size $\Delta t$. For the spiking neuron model, dendritic compartments are</div><div class="clear"></div>
<div class="linenb">681</div><div class="codeline">modeled with leaky membrane dynamics in contrast to the rate variant. The choice of dendritic leakage conductance</div><div class="clear"></div>
<div class="linenb">682</div><div class="codeline">$g_l^{dend}=\Delta t=0.1$ is motivated in Section \ref{sec-gl-dend}.</div><div class="clear"></div>
<div class="linenb">683</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">684</div><div class="codeline">A major issue of the spiking network is the fact that under the default parametrization, spikes are too infrequent for</div><div class="clear"></div>
<div class="linenb">685</div><div class="codeline">the network to accurately compute the dendritic error terms. Initial experiments showed that the network is rather</div><div class="clear"></div>
<div class="linenb">686</div><div class="codeline">sensitive to changes in parametrization, which meant that it was desirable to change as few existing parameters as</div><div class="clear"></div>
<div class="linenb">687</div><div class="codeline">possible. Therefore, a novel parameter $\psi$ was introduced. In a spiking neuron $i$, the probability of eliciting a</div><div class="clear"></div>
<div class="linenb">688</div><div class="codeline">spike is linearly increased by this factor ($r_i = \psi \phi(u_i)$). Likewise, all synaptic weights $W$ in a spiking</div><div class="clear"></div>
<div class="linenb">689</div><div class="codeline">network are attenuated by the same factor ($W \leftarrow \frac{W}{\psi}$). These changes cancel each other out, as an</div><div class="clear"></div>
<div class="linenb">690</div><div class="codeline">increased value for $\psi$ elicits no change in absolute compartment voltages of a network. Instead, it serves to</div><div class="clear"></div>
<div class="linenb">691</div><div class="codeline">stabilize these voltages over time, which drastically improves learning performance. One mechanism in which this</div><div class="clear"></div>
<div class="linenb">692</div><div class="codeline">parameter also needs to be considered is the plasticity rule. Weight changes are affected by $\psi$ in three distinct</div><div class="clear"></div>
<div class="linenb">693</div><div class="codeline">ways: Since $\psi$ <span class="highlight" title="The verb form seems incorrect.. Suggestions: [is linearly scaling, it linearly scales, linearly scales] (44126) [lt:en:IS_VBZ]">is linearly scales</span> the activation (spiking or hypothetical), it also increases dendritic error</div><div class="clear"></div>
<div class="linenb">694</div><div class="codeline">linearly, as it does the presynaptic activation. Additionally, since the frequency of weight changes is determined by</div><div class="clear"></div>
<div class="linenb">695</div><div class="codeline">the presynaptic spike rate, $\psi$ increases the strength of plasticity three times. As these influences are</div><div class="clear"></div>
<div class="linenb">696</div><div class="codeline">multiplicative, learning rates are attenuated by $\eta \leftarrow \frac{\eta}{\psi^3}$. The exception to this are the</div><div class="clear"></div>
<div class="linenb">697</div><div class="codeline">weights from interneurons to pyramidal neurons, as these do not depend on dendritic predictions, but on absolute</div><div class="clear"></div>
<div class="linenb">698</div><div class="codeline">dendritic voltage. Hence, in this case $\eta^{pi} \leftarrow\frac{\eta^{pi}}{\psi^2}$. On close investigation of the</div><div class="clear"></div>
<div class="linenb">699</div><div class="codeline">spiking neuron model, one can observe that for $\psi \rightarrow \infty$, it approximates the rate-based implementation</div><div class="clear"></div>
<div class="linenb">700</div><div class="codeline">exactly at the steady state. Unsurprisingly therefore, increasing $\psi$ caused the spiking network to learn</div><div class="clear"></div>
<div class="linenb">701</div><div class="codeline">successfully with fewer samples and to a lower test loss. Yet, the argument against increasing $\psi$ is twofold:</div><div class="clear"></div>
<div class="linenb">702</div><div class="codeline">Initial experiments showed that with $\psi \in [0.1, 0.5]$, pyramidal and interneurons exhibit spike frequencies in</div><div class="clear"></div>
<div class="linenb">703</div><div class="codeline">biologically plausible range of less than $55Hz$ \cite{Kawaguchi2001,Eyal2018} \todo{reevaluate!}. Additionally, each</div><div class="clear"></div>
<div class="linenb">704</div><div class="codeline">transmitted \texttt{<span class="highlight-spelling" title="Possible spelling mistake found. (45210) [lt:en:MORFOLOGIK_RULE_EN_US]">SpikeEvent}</span> is computationally costly, which increases training time (cf. Figure</div><div class="clear"></div>
<div class="linenb">705</div><div class="codeline">\ref{fig-benchmark}) and further makes high spike frequencies undesirable. As a middle ground, $\psi = 100$ proved</div><div class="clear"></div>
<div class="linenb">706</div><div class="codeline">useful during initial tests and will be assumed the default from here on out. Note that this parametrization was chosen</div><div class="clear"></div>
<div class="linenb">707</div><div class="codeline">primarily with efficiency in mind, and makes no claim towards biologically plausible spike frequency.</div><div class="clear"></div>
<div class="linenb">708</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">709</div><div class="codeline">With these adaptations, the network was able to perform supervised learning with spiking neurons, as will be discussed</div><div class="clear"></div>
<div class="linenb">710</div><div class="codeline">in the upcoming sections.</div><div class="clear"></div>
<div class="linenb">711</div><div class="codeline">&nbsp;</div><div class="clear"></div>
</div>
<h2 class="filename">06_notes.tex</h2>

<p>Found 48 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;1</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 99 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Additional] (0) [lt:en:UPPERCASE_SENTENCE_START]">additional</span> notes and questions}</div><div class="clear"></div>
<div class="linenb">&nbsp;2</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;3</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{Questions}</div><div class="clear"></div>
<div class="linenb">&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;5</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;6</div><div class="codeline"><span class="keyword2">\begin{itemize}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;7</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [How] (49) [lt:en:UPPERCASE_SENTENCE_START]">how</span> interested are you in the code? <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Reference] (85) [lt:en:UPPERCASE_SENTENCE_START]">reference</span> it, explain it, or shut up?</div><div class="clear"></div>
<div class="linenb">&nbsp;8</div><div class="codeline">    <span class="keyword1">\item</span> Code <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [barefoot, baleful, baneful, bagful, bagfuls, Bagehot] (132) [lt:en:MORFOLOGIK_RULE_EN_US]">beigefÃ¼gt</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [ALS, as, all, also, eyes, ads, ACS, Alps, ale, APS, TLS, alms, AGS, AHS, Alas, CLS, GLS, NLS, SLS, alas, ales, pals, Hals, abs, alps, alp, awls, gals, ails, albs, Ali, alts, AAS, ABS, ADS, AES, AFS, AIS, AIs, AJS, AKS, AL, ALB, ALC, ALCS, ALD, ALE, ALG, ALH, ALJ, ALK, ALL, ALM, ALN, ALO, ALP, ALQ, ALR, ALV, ALW, ALX, ALZ, AMS, ANS, AOS, AQS, ASS, ATS, AUS, AVS, AWS, AXS, AYS, AZS, Al, Ala, As, Ats, BLS, ELS, FLS, JLS, LS, PLS, RLS, SALS, alb, alt, ans, ass, ls, a ls, ACLs, ADLs, AGs, ALF, ALU, ALUs, APs, AVs, DLS, HLS, ILS, MLS, XLS, ais, pls, a, and, is, was, at, his, an, any, are, has, age, its, last, us, yes, air, art, days, half, law, old, act, arms, call, cars, fall, fans, gas, goals, male, males, mass, pass, says, Hall, Los, able, acts, add, ago, aid, am, arm, arts, ball, bass, bus, calls, eye, falls, fly, hall, laws, lay, plus, sale, sales, talk, walk, wall, walls, ways, AIDS, Wales, ad, ages, aims, ask, asks, axis, balls, bars, deals, else, false, lap, mall, maps, salt, tale, talks, tall, vs, wars, AFC, Alan, Alex, CDs, LA, Mrs, Wall, adds, airs, aka, ally, arc, ash, bats, calm, caps, cats, ears, fails, halls, halt, ill, lab, laps, lb, meals, pale, palm, pays, rails, rats, rays, tales, ups, walks, Acts, Ball, Bass, CSS, Dale, GPS, Hans, Lt, Mars, Mass, NHS, PCs, SAS, SL, Walsh, Walt, Yale, ace, aft, aids, alias, ants, ate, axes, axle, bags, bays, cans, dams, eats, eyed, gaps, hats, labs, lays, lbs, malls, nails, oils, owls, pads, sails, salts, seals, tags, tails, ACM, ADC, ADP, AF, ANSI, AOL, APA, APC, ATC, ATM, ATV, Alec, Alma, Amos, Ana, Atlas, Axis, BBS, CNS, DL, DNS, FRS, Gus, Hale, INS, IRS, LSD, LST, Laos, Male, Mali, RMS, RSS, RTS, SBS, SLR, SMS, SOS, TLC, aces, alto, amp, ant, ape, apes, apse, apt, arcs, atlas, aux, awe, axles, bald, bans, calf, eds, elf, elk, elm, fats, flu, gags, gala, gale, gays, hails, halo, jams, jars, jaws, lag, mails, malt, mats, ml, oaks, palms, pans, pas, ply, salsa, sans, vans, vols, AAU, ACLU, ADR, ADSL, AEC, AEG, AIF, AIG, AMX, APG, ARL, ARP, ASB, ASD, Abe, Ada, Alba, Alta, Ames, Ares, Aug, Ava, Bali, CCS, CFS, CLR, CVS, DLR, DMS, DSS, DTS, ELF, Elsa, FBS, FCS, Gale, Gauls, Haas, Hays, IAS, IDs, IPS, KBS, KLM, Kali, LD, Lars, MTS, Mays, NPS, OAS, OCS, PDS, PRS, RCS, RDS, SARS, SLA, SLC, TFS, TVs, UPS, URLs, VMS, Walls, ah, alum, amps, ark, avg, bales, cabs, cal, cams, coals, dials, earls, eels, foals, gal, gall, halts, heals, ills, jails, jays, lad, lads, lax, ms, oars, oats, ops, ovals, pal, palsy, paws, rags, rams, raps, sacs, saws, sly, tabs, taps, AAT, ABM, ACU, AD, ADD, ADT, AEF, AFM, AFN, AGC, AIT, APD, APM, ARF, ASN, ASSR, Aldo, Alva, Apr, Ara, BDS, BLT, BMS, CGS, CLI, CLP, Cali, DDS, DLA, DLL, ESS, FLP, Gaels, Gall, HFS, HHS, HSS, IHS, IVs, JAL, JCS, LCS, LSA, LSC, LSI, LSM, LSO, LSP, MBS, MDS, MMS, NBS, NFS, PMS, PNS, PSS, SALT, SCS, SLP, SRS, TLD, TSS, Urals, VLSI, Wald, alga, asp, av, ax, aye, bails, bale, balm, balsa, calms, cays, dais, dale, elms, fads, gales, gels, hams, hauls, jabs, lac, lags, lash, lat, malts, pall, pars, pus, reals, talc, ts, vale, vats, vials, yams, ABCs, ACD, ACG, ACH, AGR, APB, APN, AUT, AZT, Adm, Alisa, Alsop, Avis, BRS, CLT, DLM, Dali, ELT, ERS, GDS, GLP, GRS, Hus, JMS, KSS, LFS, LRS, LSB, MLF, MNS, NAL, PVS, RVs, SLT, TLR, VLF, WAIS, Yalu, ably, aim, aloe, alums, amt, auks, balk, cl, dads, dales, ell, fps, galas, galls, halos, ilk, kale, lass, mans, mils, ole, pales, pats, sols, val, yaks, ABMs, ACAS, AFO, CLG, DLI, DPs, DVS, FAQs, HDS, Kalb, LGS, MLD, NLR, SLN, Salk, WLM, adj, ado, aha, ain, aloes, altos, awl, balks, bps, cols, ells, gars, hale, ifs, lg, mags, mars, mauls, mos, naps, nays, opals, pails, peals, rials, sags, saps, sass, sis, tans, tars, vales, wads, wags, wails, wale, wales, zaps, AHQ, BKS, MLI, PMs, RAMs, TAWS, Tass, adv, ail, app, apps, arks, asst, awns, ayes, balms, dabs, elks, lase, nags, nus, obs, pawls, pols, rads, talus, tams, teals, yawls, yaws, Alar, Kans, ODs, asps, baas, cads, caws, ems, haws, hays, macs, maws, nabs, orals, palls, tali, tats, taus, weals, yaps, AAA, ABC, AMC, ATP, Afr, Amy, Ann, CBS, HMS, OKs, PS, SOs, USS, assn, ave, awes, hales, mks, ohs, oms, rps, Hts, Xes, bawls, calks, dds, fays, gabs, gads, jags, lams, oafs, oles, A, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26, A27, A28, A29, A30, A31, A32, A33, A34, A35, A36, A37, A38, A39, A40, A41, A42, A43, A44, A45, A46, A47, A48, A49, A50, A51, A52, A53, A54, A55, A56, A57, A58, A59, A60, A61, A62, A63, A64, A65, A66, A67, A68, A69, A6M, A70, A71, A72, A73, A74, A75, A76, A77, A78, A79, A80, A81, A82, A83, A84, A85, A86, A87, A88, A89, A90, A91, A92, A93, A94, A95, A96, A97, A98, A99, A9C, AA, AAB, AAC, AAD, AAE, AAF, AAG, AAH, AAI, AAJ, AAK, AAL, AAM, AAN, AAO, AAP, AAQ, AAR, AAV, AAW, AAX, AAY, AAZ, AB, ABA, ABB, ABD, ABES, ABF, ABG, ABH, ABIS, ABJ, ABK, ABL, ABN, ABP, ABQ, ABR, ABT, ABU, ABV, ABW, ABX, ABZ, AC, ACA, ACB, ACC, ACF, ACI, ACJ, ACK, ACL, ACN, ACO, ACP, ACR, ACT, ACV, ACW, ACX, ACY, ACZ, ADA, ADB, ADBS, ADF, ADG, ADH, ADJ, ADK, ADL, ADM, ADN, ADQ, ADSI, ADV, ADX, ADY, ADZ, AE, AEA, AEB, AED, AEE, AEI, AEJ, AEK, AEL, AELE, AEM, AEN, AEO, AEP, AEQ, AER, AESA, AESV, AET, AEU, AEV, AEW, AEX, AEZ, AFA, AFB, AFD, AFE, AFF, AFG, AFH, AFI, AFIS, AFJ, AFK, AFL, AFLD, AFLP, AFPS, AFQ, AFR, AFT, AFU, AFV, AFW, AFX, AFZ, AG, AGB, AGCS, AGD, AGE, AGF, AGG, AGH, AGI, AGJ, AGK, AGL, AGLH, AGM, AGN, AGO, AGP, AGQ, AGT, AGU, AGV, AGW, AGX, AGY, AGZ, AHA, AHB, AHC, AHD, AHE, AHF, AHG, AHH, AHI, AHJ, AHK, AHL, AHM, AHN, AHO, AHR, AHSI, AHT, AHU, AHV, AHW, AHX, AHY, AHZ, AI, AIB, AIBS, AIC, AID, AIE, AIH, AII, AIJ, AIK, AIM, AIO, AIP, AIQ, AIU, AIV, AIW, AIY, AIZ, AJ, AJB, AJC, AJF, AJG, AJH, AJI, AJJ, AJK, AJL, AJN, AJO, AJP, AJQ, AJR, AJT, AJU, AJV, AJW, AJY, AK, AKB, AKC, AKD, AKE, AKF, AKG, AKJ, AKK, AKL, AKM, AKN, AKO, AKP, AKQ, AKR, AKT, AKU, AKV, AKX, AKY, ALAT, ALBA, ALCM, ALFA, ALMA, ALPA, AM, AMA, AMD, AME, AMF, AMG, AMH, AMIS, AMJ, AMK, AML, AMLA, AMM, AMN, AMO, AMP, AMQ, AMR, AMSL, AMST, AMT, AMU, AMV, AMW, AMY, AMZ, AN, ANB, ANC, AND, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANPS, ANQ, ANR, ANSM, ANT, ANTS, ANV, ANW, ANX, ANY, ANZ, AO, AOA, AOB, AOC, AOD, AOE, AOF, AOG, AOH, AOI, AOJ, AOK, AOM, AON, AOO, AOP, AOQ, AOR, AOT, AOU, AOV, AOW, AOX, AP, APE, APF, APH, API, APJ, APK, APL, APO, APP, APQ, APR, APRS, APT, APU, APV, APW, APX, APY, APZ, AQA, AQB, AQC, AQF, AQG, AQH, AQI, AQK, AQL, AQM, AQP, AQR, AQT, AQY, AR, ARB, ARC, ARD, ARE, ARH, ARI, ARJ, ARK, ARLE, ARM, ARN, ARQ, ARR, ARSI, ART, ARTS, ARU, ARV, ARW, ARX, ARY, ASA, ASC, ASE, ASF, ASG, ASGS, ASH, ASJ, ASK, ASL, ASLM, ASM, ASP, ASQ, ASR, ASSE, AST, ASU, ASV, ASW, ASX, ASY, ASZ, AT, AT3, ATA, ATB, ATD, ATE, ATF, ATG, ATH, ATI, ATK, ATL, ATN, ATO, ATQ, ATR, ATT, ATU, ATW, ATX, ATY, ATZ, AU, AUA, AUC, AUD, AUE, AUF, AUG, AUI, AUJ, AUL, AUM, AUN, AUO, AUP, AUQ, AUR, AUU, AUW, AUX, AUY, AUZ, AV, AVA, AVB, AVC, AVE, AVF, AVG, AVH, AVI, AVK, AVL, AVN, AVO, AVP, AVR, AVSF, AVU, AVV, AVW, AVX, AVY, AVZ, AWA, AWB, AWD, AWE, AWG, AWK, AWM, AWN, AWP, AWR, AWU, AWZ, AX, AXB, AXC, AXD, AXG, AXK, AXL, AXM, AXN, AXP, AXR, AXV, AXX, AYA, AYC, AYD, AYE, AYG, AYH, AYI, AYJ, AYL, AYN, AYO, AYP, AYQ, AYR, AYU, AYY, AYZ, AZ, AZA, AZB, AZD, AZE, AZF, AZI, AZN, AZO, AZW, AZZ, Ac, Adas, Ag, Alpo, Am, Ar, Ark, Art, At, Au, Av, Ave, BAES, BAS, BCS, BES, BFS, BGS, BHS, BIS, BJS, BLA, BLB, BLC, BLD, BLE, BLF, BLG, BLI, BLL, BLM, BLP, BLR, BLTs, BLU, BLV, BLW, BLX, BLY, BLZ, BNS, BOS, BPS, BS, BSS, BTS, BVS, BWS, BXS, BYS, BZS, Baals, Blu, CAES, CAL, CALR, CALT, CATS, CDS, CES, CHS, CJS, CKS, CL, CLA, CLAS, CLB, CLC, CLD, CLE, CLH, CLIS, CLJ, CLL, CLM, CLN, CLQ, CLSC, CLU, CLV, CLW, CLX, CLY, CLZ, CMS, CPS, CQS, CRS, CS, CZS, Cal, Cl, Cs, DAL, DALF, DCS, DES, DFS, DGS, DHS, DLC, DLF, DLG, DLH, DLP, DLT, DOS, DPS, DRS, DS, DUS, Dis, EADS, EAL, EAS, EBS, ECS, EDS, EFS, EHS, EIS, ELA, ELD, ELI, ELN, ELO, ELP, ELQ, ELU, ENS, EPS, ES, ETS, EUS, EVS, EWS, EXS, EZS, Eli, Es, FDS, FES, FFS, FL, FLA, FLB, FLE, FLH, FLK, FLL, FLM, FLR, FLSH, FLU, FMS, FMs, FNS, FPS, FS, FSS, Fla, Flo, GAL, GAS, GCS, GES, GFS, GGS, GHS, GIS, GLA, GLM, GLN, GLT, GMS, GTS, HALE, HBS, HCS, HES, HL, HLA, HLM, HLP, HLR, HNS, HPS, HS, HTS, Hal, IDS, IES, IFS, IGS, IIS, IL, IL2, ILC, ILG, ILM, ILN, ILP, IMS, IOS, IS, ISS, ITS, Ila, Ill, JBS, JHS, JLA, JLD, JLM, JWS, Japs, KAL, KDS, KES, KHS, KKS, KLC, KLE, KMS, KOS, KS, KTS, Ks, L, LABS, LC, LE, LES, LFLS, LG, LH, LHS, LI, LL, LLB, LLC, LLD, LLG, LLN, LLR, LMS, LN, LO, LP, LR, LS1, LS2, LS3, LS4, LSE, LSF, LSJ, LSL, LSN, LSQ, LSW, LSX, LT, LU, LVS, LY, La, Las, Le, Les, Li, Ln, Lr, Lu, MACS, MAL, MALE, MARS, MAS, MATS, MAWS, MCS, MES, MFS, MGS, MKS, ML, MLA, MLC, MLM, MLP, MLT, MLV, MLW, MOS, MPS, MRS, MS, MSS, MUS, Ms, NADS, NAS, NCS, NDS, NGS, NL, NLB, NLD, NLG, NLH, NLO, NOS, NS, NSS, NUS, Nos, OAL, OBS, ODS, OFS, OLA, OLF, OLO, OLP, OLT, OMS, ONS, ORS, OS, OSS, OVS, OWS, Ola, Os, PACS, PAES, PAL, PALU, PAPS, PAS, PBS, PCS, PES, PFS, PGS, PHS, PIS, PKS, PL, PLA, PLB, PLD, PLE, PLF, PLG, PLH, PLK, PLL, PLM, PLN, PLO, PLP, PLR, PLT, PLU, PLW, PLX, POS, PPS, PUS, Pl, RAMS, RAS, RBS, RES, RFS, RGS, RHS, RIS, RLC, RLD, RLE, RLI, RLL, RNS, ROS, RPS, RRS, RS, S, SAVS, SES, SFS, SGS, SIS, SLB, SLF, SLG, SLI, SLL, SLO, SLQ, SLSI, SLV, SLZ, SNS, SPS, SS, SSS, STS, SUS, Saks, Sal, Salas, TALC, TASS, TBS, TDS, TES, TGS, THS, TIS, TJS, TKS, TL, TLA, TLB, TLE, TLF, TLH, TLM, TLP, TLT, TLV, TMS, TNS, TOS, TPS, TQS, TRS, TS, TTS, TVS, TWS, TZS, Tl, UAL, UAS, UBS, UDS, UES, UFS, UGS, UL, ULC, ULG, ULK, ULM, ULR, ULSI, ULT, ULX, UMS, URS, US, UUS, VBS, VES, VGS, VHS, VLA, VLB, VLC, VLE, VLG, VLM, VLN, VLSM, VPS, VRS, VSS, VVS, Val, WATS, WCS, WDS, WGS, WHS, WLF, WLG, WLT, WLW, WMS, WNS, WPS, WSS, Wis, XL, XLG, XMS, XPS, XS, YBS, YHS, YL, ZLSD, Zs, aah, ab, ac, ang, anus, ares, arr, auk, aw, awn, balds, baps, bis, bl, bxs, calk, cis, cos, cps, cs, dags, dis, dos, ens, fags, fl, flt, gs, hags, hes, hols, hos, hrs, iOS, ids, ilks, ins, isl, kl, ks, l, la, lam, lats, lav, lavs, ll, lo, mads, mams, mas, mes, mus, mys, nos, paps, pis, pl, qts, rale, rales, res, rs, s, tads, tbs, ult, vacs, vars, vlf, xis, yrs, A&amp;A, A&amp;D, A&amp;E, A&amp;M, A&amp;P, A&amp;R, ABI, ACE, ACMs, ACST, ACTs, ACh, ADI, ADU, AEST, AFP, AGA, AGAs, AIWS, AIX, AMI, AMLO, AOSP, APFS, APIs, APKs, APNs, ARDS, ASOS, ASUS, ATMs, ATVs, AVM, AWC, AWF, AWST, AXA, Aalst, Abi, Abu, Albus, Alby, Aldi, Alois, Alok, Alves, Aon, Aras, Asus, Atos, Ayla, BJs, CAS, CCs, CIS, CKs, CLSA, CTS, CTs, CVs, Calo, DAUs, DBS, DJs, DLK, DMs, DalÃ­, DoS, EGS, ELR, EMS, EOLs, EOS, EPs, EQS, EQs, EVs, El, FACS, FAS, FATs, FCs, FHS, FOS, FPs, GBS, GCs, GIs, GL, H&amp;S, HCs, HLC, HLSS, HWs, I&amp;S, IBS, ICS, IMs, IPs, IQs, IaaS, JS, KLB, KWS, LDS, LF, LLP, LM, LMs, LPS, LPs, LQ, LSU, LTS, LV, LX, MAUs, MCs, MDs, MIS, MLB, MLE, MLIS, MLK, MPLS, MPs, MQLs, MWS, NAKs, NLP, NLT, NLU, NRS, NTS, NYS, Nils, PAAs, PACs, PAs, PLC, PLCs, PRs, PaaS, QBs, QLD, QMS, QS, QTS, RL, RTs, Ralf, SATs, SDS, SEALs, SHS, SLAs, SLRs, SQS, SaaS, T&amp;S, TAS, TCS, TLDs, TLN, TTs, ToS, UATs, UAVs, UCS, UIs, ULB, UOS, Ulm, Urs, V6s, V8s, VCS, VCs, VFS, VMs, VPs, VWs, WBS, WS, XLSX, XSS, ZDs, ZFS, ahh, ai, amu, amus, aww, axe, cts, dms, mL, ol', olds, pHs, pcs, pgs, plc, pms, pts, Ã€lex] (142) [lt:en:MORFOLOGIK_RULE_EN_US]">als</span> CD? GitHub link? <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Egal] (163) [lt:en:UPPERCASE_SENTENCE_START]">egal</span>?</div><div class="clear"></div>
<div class="linenb">&nbsp;9</div><div class="codeline">    <span class="keyword1">\item</span> Citations in my motivation or not?</div><div class="clear"></div>
<div class="linenb">10</div><div class="codeline">    <span class="keyword1">\item</span> No defense? <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Is] (224) [lt:en:UPPERCASE_SENTENCE_START]">is</span> that correct?</div><div class="clear"></div>
<div class="linenb">11</div><div class="codeline">    <span class="keyword1">\item</span> Mathematical notation for <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>variable is changed to<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> ($W \leftarrow \frac{W}{\psi}$)</div><div class="clear"></div>
<div class="linenb">12</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Caleche, CalÃ¨che, Sclerite] (304) [lt:en:MORFOLOGIK_RULE_EN_US]">Schlechte</span> <span class="highlight-spelling" title="Possible spelling mistake found. (314) [lt:en:MORFOLOGIK_RULE_EN_US]">ergebnisse</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Oder, over, older, order, odor, ode, odes, coder, doer, odder, ODEM, ODR, o'er, one, or, her, other, under, does, modern, open, power, border, code, cover, ever, idea, lower, model, offer, our, Dr, door, mode, ones, opera, orders, owner, tower, user, beer, codes, elder, odd, outer, wider, Roger, boxer, deer, holder, lover, modes, node, nodes, odds, peer, per, powder, rider, rode, tier, voter, Del, Owen, Wonder, coded, den, obey, oven, owed, owes, pier, poker, wonder, Adler, Boer, DDR, DEC, Dec, Dee, Dover, Eden, GDR, Homer, OEM, Omar, Rover, Ryder, codec, codex, colder, dear, fodder, folder, homer, loader, loser, modem, mover, ores, otter, overt, owe, oxen, rodeo, rover, rower, sober, ADR, Aden, Boyer, CDR, DLR, Dyer, FDR, MDR, OPEC, Odin, Ogden, Opel, alder, cider, dew, foyer, hover, loner, louder, oar, omen, seer, solder, DNR, DVR, Dir, HDR, Holder, Nader, OCR, OE, SDR, UDR, Vader, adder, bolder, deg, doe, dour, dower, idler, joker, lodger, mower, odors, ogre, olden, ponder, toner, veer, DDE, DSR, Odell, Odom, Rodger, Seder, bier, bode, borer, bower, coders, comer, doers, goer, lode, ole, udder, wader, yonder, Adar, Odets, Orem, Oreo, cower, dodger, dyer, eider, gofer, hider, idem, jeer, lodes, oleo, roper, yodel, Olen, boded, bodes, dodder, edger, fonder, ides, leer, molder, ocher, osier, poser, roger, ruder, sower, ODs, codger, corer, goner, ocker, wooer, ceder, doper, ore, Odis, moper, oles, sorer, ADMR, AER, AFER, ATER, Amer, BDE, BDR, BER, CDE, CER, CFER, DAR, DBR, DCR, DDEA, DE, DEA, DEB, DEG, DEI, DEL, DEM, DEP, DEQ, DES, DET, DEU, DHR, DIR, DKR, DMR, DR, Dem, EER, ER, Er, FDES, FEDER, FER, FSER, GDE, GDPR, Ger, Godel, HDE, HER, IDE, IDES, IDR, IPER, JDE, KER, LDE, LER, MDE, MER, MODEM, NDLR, NDR, NER, OAE, OAR, OBE, OCDE, OD, ODAS, ODB, ODBC, ODC, ODD, ODF, ODG, ODI, ODM, ODMRP, ODN, ODP, ODS, OEA, OEB, OEC, OED, OEP, OGE, OGR, OKed, OLED, OMR, ONDAR, ONE, ONEM, ONERA, ONERC, OPEP, OR, OSEF, OSR, OSes, Odesa, Ore, Oreg, Orr, PER, QDR, RDAR, REER, RER, SDEI, SER, TDE, TDR, TER, UDSR, UER, aver, boner, coyer, dded, deb, def, derv, doter, er, ewer, fer, hoer, honer, nuder, ogler, overs, weer, yer, Acer, Adel, CDEC, Cher, DEX, EDPR, EDR, Godber, GÃ¶del, Hofer, IDEs, IDed, Iker, KDE, ODHA, ODMs, ODT, OKR, ONR, OPEX, Oleg, Opera, Otero, Owler, PDR, SDE, UDHR, Uber, VDE, Zoner, aider, dep, dev, e'er, fader, oiler, opex, polder, rodes, suer] (325) [lt:en:MORFOLOGIK_RULE_EN_US]">oder</span> gar <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Kane, lane, Jane, Lane, Maine, Kate, cane, Seine, Dane, Kant, Kaye, Paine, kana, mane, pane, sane, vane, wane, Heine, Kline, Zane, bane, seine, kale, Taine, Kans, Kan, Kano, kine, Caine, Kanye, Keane] (334) [lt:en:MORFOLOGIK_RULE_EN_US]">keine</span> <span class="highlight-spelling" title="Possible spelling mistake found. (340) [lt:en:MORFOLOGIK_RULE_EN_US]">ergebnisse</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [reported, reporter] (351) [lt:en:MORFOLOGIK_RULE_EN_US]">reporten</span>?</div><div class="clear"></div>
<div class="linenb">13</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Algae, Ablaze, Mugabe, Agate, Agave, Abate, Agape, Babe, Ablate, Abrade, Abase, Abbe, Gabe, AbbÃ©] (365) [lt:en:MORFOLOGIK_RULE_EN_US]">Abgabe</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [postalize, postalized, postalizes] (372) [lt:en:MORFOLOGIK_RULE_EN_US]">postalisch</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Moloch] (383) [lt:en:MORFOLOGIK_RULE_EN_US]">mÃ¶glich</span>/<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [sinkhole, Linnell, SIGPOLL] (391) [lt:en:MORFOLOGIK_RULE_EN_US]">sinnvoll</span>?</div><div class="clear"></div>
<div class="linenb">14</div><div class="codeline">    <span class="keyword1">\item</span> 4 <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [eaten, Satan, satin, sate, sated, sateen, oaten, sates] (407) [lt:en:MORFOLOGIK_RULE_EN_US]">seiten</span> <span class="highlight-spelling" title="Possible spelling mistake found. (414) [lt:en:MORFOLOGIK_RULE_EN_US]">probekapitel</span> ok?</div><div class="clear"></div>
<div class="linenb">15</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Dark, Dwarf, Dare, Dart, ARF, Darn, Barf, CARF, DAAF, DAF, DALF, DAR, DARE] (435) [lt:en:MORFOLOGIK_RULE_EN_US]">Darf</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [ICH, ice, rich, inch, ICT, Rich, ICU, icy, ICL, ICP, ICM, ICR, IH, itch, ACH, ICN, ICO, Mich, RCH, sch, ICC, BCH, CCH, CH, Ch, IAH, ICA, ICB, ICD, ICE, ICI, IDH, IGH, IMH, INH, Ice, LCH, MCH, TCH, ch, och, i ch, ACh, IC, ICF, ICJ, ICQ, ICS, in, is, I, with, it, which, such, each, high, if, its, much, nice, act, BC, CD, cm, fish, pick, vice, IBM, PC, kick, mph, pitch, rice, wish, FCC, IRA, Nick, Rico, arch, ash, dish, icon, ill, ink, ion, mice, sick, tech, ID, IDF, IOC, IQ, LCD, Mick, PCI, PCs, Rice, Rick, ace, cf, dice, ditch, echo, id, inn, niche, oh, pH, pinch, sic, witch, ACM, ACS, Bach, CCC, CCD, CV, Erich, IEC, IIT, IMF, INS, IPA, IPO, IRS, ISP, ITF, ITU, JC, Mitch, Nice, Oct, PCB, PCC, QC, Reich, Sikh, birch, ire, sci, tick, CCA, CCI, CCP, CCS, CIC, Che, DCC, DCI, ECG, Eco, FCS, Finch, Fitch, GCB, GCC, GH, IAS, IAU, ICBM, IDs, IFA, IFC, IMG, IMO, IPC, IPCC, IPS, IRL, ITC, ITT, Inca, LCC, MCG, MCP, MPH, Mach, NCA, NCO, NCR, OCS, PCM, PCP, RCD, RCN, RCS, SCC, UCL, VCR, VH, Vichy, WC, ah, finch, hitch, iced, ivy, kWh, lice, lick, mic, mica, sh, wick, winch, ACTH, ACU, BCD, CCG, CCR, DCE, DCL, DCM, FCA, FCI, FSH, HCA, HCC, IAA, IAC, IAP, IDC, IEA, IED, IEP, IFL, IFP, IFR, IGA, IHS, IMA, IMC, IMT, IMU, IOM, IPN, IPP, IPR, IVB, IVs, JCB, JCR, JCS, LCL, LCR, LCS, LCT, MCB, MCD, MCL, Micah, OCR, PTH, RCM, RCP, SCS, TCA, TCM, TCR, UCA, cit, eh, etch, imp, nick, nigh, pic, pith, sigh, tics, uh, ACD, ACG, CCB, CCU, CNH, CTH, DCB, DCF, EIC, FCE, FICA, Foch, GCA, Gish, HGH, IAB, IBP, IDG, IDL, IPI, ITE, IUD, Ina, Iva, JCC, JCL, LCM, MCE, MCF, MCO, MCR, MCT, MCU, RCI, SCU, ache, cl, huh, ices, ilk, inf, itchy, mach, mics, nth, pica, rah, CSH, DCD, HIC, IOU, IRQ, Icahn, TCO, bah, cinch, duh, hick, ifs, kith, ooh, pah, pics, ssh, cw, icky, incl, kph, ouch, pct, rick, ugh, IMHO, Ind, Pict, achy, aitch, ecu, fiche, irk, mick, zilch, CHG, ECW, IP, ITV, MCA, Vic, chm, filch, jct, sics, fichu, 0th, 1th, 2th, 3CB, 3th, 4th, 5th, 6th, 7th, 8th, 9th, AAH, ABH, AC, ACA, ACB, ACC, ACF, ACI, ACJ, ACK, ACL, ACN, ACO, ACP, ACR, ACT, ACV, ACW, ACX, ACY, ACZ, ADH, AFH, AGH, AHH, AIC, AIH, AJH, ALH, AMH, ANH, AOH, APH, AQH, ARH, ASH, ATH, AVH, AYH, Ac, BAH, BBH, BCA, BCB, BCC, BCE, BCF, BCG, BCI, BCJ, BCK, BCL, BCLH, BCM, BCN, BCO, BCP, BCR, BCS, BCT, BCU, BCV, BCX, BCY, BCZ, BDH, BEH, BGH, BIH, BJH, BKH, BOH, BPH, BRH, BSH, BTH, BUH, BVH, BWH, BXH, BYH, BZH, Bic, C, CA, CAH, CB, CBH, CC, CCE, CCF, CCJ, CCK, CCL, CCM, CCN, CCO, CCQ, CCT, CCV, CCW, CCX, CCZ, CDH, CE, CF, CFH, CG, CGH, CHB, CHC, CHD, CHF, CHH, CHI, CHK, CHL, CHM, CHN, CHO, CHP, CHQ, CHR, CHS, CHT, CHV, CHW, CHX, CHY, CHZ, CI, CICE, CICM, CICR, CICS, CIH, CJ, CJH, CK, CL, CLH, CM, CN, CO, CPH, CR, CRH, CS, CT, CVH, CW, CY, CZ, Ca, Cb, Cd, Ce, Cf, Chi, Ci, Cl, Cm, Co, Cr, Cs, Ct, Cu, DC, DCA, DCK, DCN, DCO, DCP, DCR, DCS, DCT, DCU, DDH, DGH, DH, DHH, DIC, DLH, DPH, DRH, DWH, Dick, EAH, EC, EC1, EC4, ECA, ECB, ECC, ECE, ECHL, ECL, ECM, ECN, ECP, ECR, ECS, ENH, EOH, ESH, FAH, FC, FCD, FCG, FCL, FCM, FCO, FCP, FCR, FCW, FIC, FICP, FLH, FMH, FOH, FPH, GCF, GCG, GCI, GCM, GCP, GCR, GCS, GCU, GEH, GICL, GIH, GRH, GSH, H, HCB, HCG, HCI, HCL, HCS, HHH, HRH, I10, I11, I12, I15, I16, I19, I20, I21, I22, I24, I25, I26, I27, I29, I30, I35, I37, I39, I3P, I40, I42, I43, I44, I45, I49, I53, I64, I65, I66, I68, I69, I70, I71, I72, I73, I74, I77, I78, I79, I80, I81, I82, I83, I84, I85, I86, I87, I88, I89, I95, I97, I99, IA, IA4, IA8, IAD, IAE, IAF, IAG, IAI, IAN, IAO, IAV, IB1, IBA, IBB, IBC, IBE, IBL, IBO, IBR, ICAM, ICAP, ICCA, ICCF, ICGA, ICGG, ICNA, ICNE, ICPE, ID3, IDE, IDK, IDM, IDN, IDR, IDS, IDT, IE, IECA, IEF, IEG, IEM, IES, IF, IFD, IFF, IFI, IFM, IFN, IFO, IFS, IFU, IGC, IGCT, IGE, IGF, IGI, IGM, IGN, IGP, IGS, IHP, IHT, IHVH, IIA, IIC, IIF, IIM, IIN, IIS, IJ, IJF, IJM, IKB, IL, IL2, ILC, ILG, ILM, ILN, ILP, IMB, IMCC, IMD, IME, IMK, IMN, IMP, IMS, IMV, IN, INA, INB, INC, INE, INF, ING, INM, INP, INR, INT, IOB, IOF, IOI, IOP, IOR, IOS, IOT, IOV, IPB, IPD, IPE, IPF, IPG, IPJ, IPM, IPT, IPY, IR, IRB, IRC, IRCB, IRD, IRE, IRG, IRI, IRM, IRN, IRP, IRR, IRT, IRU, IS, ISA, ISB, ISC, ISCM, ISD, ISF, ISG, ISI, ISK, ISL, ISM, ISN, ISO, ISR, ISS, IST, ISV, IT, ITA, ITB, ITECH, ITK, ITL, ITP, ITS, IUF, IV, IVD, IVF, IVG, IVI, IVR, IWC, IWF, IXC, IXV, IZ, Ia, Ian, Ibo, Ida, Ike, Ila, Ill, In, Inc, Io, Ir, Ira, It, Ito, Ivy, JCA, JCE, JCP, JIC, KC, KCB, KCL, KMH, KOH, KWH, Koch, LAH, LC, LC9, LCF, LCN, LCP, LCY, LDH, LFH, LH, LIH, LNH, LZH, MAH, MC, MCI, MCJ, MCM, MCS, MCZ, MH, MMH, MOH, MSH, NAH, NC, NCB, NCC, NCE, NCM, NCP, NCQ, NCS, NCW, NCY, NDH, NEH, NH, NIC, NIH, NLH, NRH, NYH, OCB, OCC, OCD, OCG, OCHA, OCI, OCK, OCL, OCN, OCO, OCP, OCV, OCX, OH, OIC, OICS, PAH, PCA, PCD, PCE, PCF, PCG, PCL, PCN, PCR, PCS, PCT, PCV, PH, PIC, PICO, PLH, PMH, PNH, POH, PPH, QCM, QNH, RC, RC5, RCA, RCB, RCC, RCF, RCK, RCR, RCW, RIC, RILH, RSH, Rh, SAH, SC, SC2, SCA, SCB, SCD, SCI, SCJ, SCL, SCM, SCN, SCO, SCP, SCR, SCV, SDH, SFH, SGH, SH, SIC, SMH, SPH, SSH, Sc, TAH, TBH, TC, TCC, TCD, TCF, TCG, TCI, TCK, TCL, TCN, TCV, TCW, TH, TIC, TLH, TSH, TVH, Tc, Th, UAH, UC, UCB, UCC, UCD, UCF, UCM, UCU, UIC, UICN, UMH, UNH, VC, VCC, VCF, VCI, VCM, VCO, VCP, VCT, VPH, VVH, WCC, WCF, WCG, WCS, WCT, WIC, WTH, XCD, XIC, YCG, YCO, YCW, YWH, ZC, ZCR, ZH, ZICO, ZKH, aah, ac, bitch, c, ca, cc, cg, chg, chi, cir, cis, ck, co, cs, ct, cu, dick, dict, h, i, iOS, ids, ii, iii, inc, incs, ind, ins, int, isl, ism, isn, ite, iv, ix, kc, lech, meh, milch, nah, shh, tic, titch, xci, Â°C, ACE, BH, BIC, BICs, Boch, C3, CCs, CP, CX, Chu, DACH, DOH, ECD, ECF, ECHR, ECO, ECT, ETH, FCB, FCU, FCs, FH, FICO, GC, GCHQ, GCT, GCs, HC, HCP, HCT, HCl, HCs, HH, HMH, I&amp;S, IACP, IAM, IAT, IB, IBD, IBS, ICAO, ICBC, ICEs, ICMP, ICMR, ICRA, ICSE, ICUs, IDB, IDP, IDQ, IFCA, IG, IGO, ILS, IM, IMs, IND, INI, ION, IPs, IQR, IQs, IRCC, ISU, ITN, IU, IUCN, IXL, IoT, JCI, JHH, JMH, KCC, KCl, LCA, LIC, MCC, MCK, MCQ, MCX, MCs, MICR, MWh, NCD, NCI, NCL, NCT, NICU, Nico, OC, OCF, OVH, PCHA, PCU, PDH, PICC, RCHK, RUH, Ricoh, Sith, TCHS, TCP, TCS, TCU, UCI, UCP, UCR, UCS, VCS, VCs, WCHS, WCM, WCU, WFH, WH, Wish, Xico, YC, Zach, ahh, bc, cx, eco, fiqh, hah, mAh, mCi, mcg, pc, pcs, uhh, Ã©cu] (440) [lt:en:MORFOLOGIK_RULE_EN_US]">ich</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [such, much, arch, ACH, ouch, AUC, ACh, auth] (444) [lt:en:MORFOLOGIK_RULE_EN_US]">auch</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [an, EIN, and, in, as, at, any, win, am, ad, bin, pin, tin, vein, min, Lin, ant, din, gin, kin, Erin, ah, rein, yin, av, ax, eon, EIC, ESN, ain, fin, sin, BIN, DIN, EAN, ECN, EI, EIA, EIB, EID, EIE, EIG, EIP, EIS, EIT, ELN, EMN, EN, EUN, EVN, FIN, IIN, IN, In, KIN, Min, NIN, OIN, PIN, SIN, VIN, ab, ac, ang, ans, aw, awn, en, Jin, Qin, ai] (449) [lt:en:MORFOLOGIK_RULE_EN_US]">ein</span> <span class="highlight-spelling" title="Possible spelling mistake found. (453) [lt:en:MORFOLOGIK_RULE_EN_US]">Probekapitel</span> <span class="highlight" title="Use 'a' instead of 'an' if the following word doesn't start with a vowel sound, e.g. 'a sentence', 'a university'.. Suggestions: [a] (466) [lt:en:EN_A_VS_AN]">an</span> Johan <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [chicken] (475) [lt:en:MORFOLOGIK_RULE_EN_US]">schicken</span>?</div><div class="clear"></div>
<div class="linenb">16</div><div class="codeline">    <span class="keyword1">\item</span> Reconsider title?</div><div class="clear"></div>
<div class="linenb">17</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight-spelling" title="Possible spelling mistake found. (511) [lt:en:MORFOLOGIK_RULE_EN_US]">Begutachtungsbogen</span>: 2.1 <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Ope rationalizations, Operationalization] (535) [lt:en:MORFOLOGIK_RULE_EN_US]">Operationalizations</span>?</div><div class="clear"></div>
<div class="linenb">18</div><div class="codeline"><span class="keyword2">\end{itemize}</span></div><div class="clear"></div>
<div class="linenb">19</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">20</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">21</div><div class="codeline"><span class="highlight-sh" title="A section title should not be written in all caps. The LaTeX stylesheet takes care of rendering titles in caps if needed. [sh:003]"></span>\<span class="highlight-sh" title="This section is very short (about 142 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [To-dos, DODOS, TO DOS, TO-DOS] (559) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>TODOS}</div><div class="clear"></div>
<div class="linenb">22</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">23</div><div class="codeline"><span class="keyword2">\begin{itemize}</span></div><div class="clear"></div>
<div class="linenb">24</div><div class="codeline">    <span class="keyword1">\item</span> I can <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>cheat<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> the apical voltage constraint for self prediction by increasing apical leakage conductance. How</div><div class="clear"></div>
<div class="linenb">25</div><div class="codeline">    does this influence my model?</div><div class="clear"></div>
<div class="linenb">26</div><div class="codeline">    <span class="keyword1">\item</span> Does the network still learn when neurons have a refractory period?</div><div class="clear"></div>
<div class="linenb">27</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Talk] (791) [lt:en:UPPERCASE_SENTENCE_START]">talk</span> about feedback plasticity</div><div class="clear"></div>
<div class="linenb">28</div><div class="codeline">    <span class="keyword1">\item</span> redo the spike rate experiment, but investigate whether novel stimuli cause bursts.</div><div class="clear"></div>
<div class="linenb">29</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Replace] (914) [lt:en:UPPERCASE_SENTENCE_START]">replace</span> cite with <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cited, cite, cites, CIEP] (932) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>citep</div><div class="clear"></div>
<div class="linenb">30</div><div class="codeline">    <span class="keyword1">\item</span> investigate <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [examine, exciting, etching, excise, exiting, exacting, excite, exiling, excising, excusing, exuding, exocrine] (954) [lt:en:MORFOLOGIK_RULE_EN_US]">exc-inh</span> split biologically</div><div class="clear"></div>
<div class="linenb">31</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight-spelling" title="Possible spelling mistake found. (985) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-senn</span> has little empirical background. <span class="highlight" title="This word is considered offensive. (1033) [lt:en:PROFANITY]">shits</span> for the outlook</div><div class="clear"></div>
<div class="linenb">32</div><div class="codeline"><span class="keyword2">\end{itemize}</span></div><div class="clear"></div>
<div class="linenb">33</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">34</div><div class="codeline">squared error / variance of training output = explained variance</div><div class="clear"></div>
<div class="linenb">35</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">36</div><div class="codeline"><span class="keyword1">\subsection</span>{Interneurons and their jobs}</div><div class="clear"></div>
<div class="linenb">37</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">38</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">39</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">40</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">41</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Reciprocal] (1155) [lt:en:UPPERCASE_SENTENCE_START]">reciprocal</span> inhibition of SST+ neurons. In agreement,</div><div class="clear"></div>
<div class="linenb">42</div><div class="codeline">recent experiments show that feedback input can gate</div><div class="clear"></div>
<div class="linenb">43</div><div class="codeline">plasticity of the feedforward synapses through VIP+</div><div class="clear"></div>
<div class="linenb">44</div><div class="codeline">neuron mediated disinhibition 176 and that <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [pyramidal] (1356) [lt:en:MORFOLOGIK_RULE_EN_US]">p</span>yrami</div><div class="clear"></div>
<div class="linenb">45</div><div class="codeline">dal neurons indeed recruit inhibitory populations to</div><div class="clear"></div>
<div class="linenb">46</div><div class="codeline">produce a predictive error177 \citep{Poirazi2020}</div><div class="clear"></div>
<div class="linenb">47</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">48</div><div class="codeline"><span class="highlight-sh" title="A section title should start with a capital letter. [sh:001]"></span>\<span class="highlight-sh" title="This section is very short (about 6 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsection</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [tires, Ypres, Pres, pres, tares] (1451) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>tpres}</div><div class="clear"></div>
<div class="linenb">49</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">50</div><div class="codeline">$t_{pres} 10 - 50 \tau$</div><div class="clear"></div>
</div>
<hr/>
Output produced by TeXtidote v0.8.2, &copy; 2018-2020 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/textidote">TeXtidote website</a> for more information.
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,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);
}
h2.filename {
  font-family: monospace;
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.linenb {
  font-style: italic;
  color: lightgrey;
  width: 30pt;
  float: left;
  margin-top: 1pt;
  margin-bottom: 1pt;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.codeline {
  margin-left: -30pt;
  padding-left: 60pt;
  margin-top: 1pt;
  margin-bottom: 1pt;
}
.no-text {
  display: none;
}
.clear {
  clear: both;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your file(s) with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<p>Found 45 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Discussion}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section{</span>Contribution}<span class="keyword1">\label</span>{sec-contribution}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">In this project, the capabilities and limitations of the dendritic error network and its underlying plasticity rule were</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">further tested. While sensitive to certain parameter changes, the network was shown to be exceptionally capable of</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">handling various constraints that affect biological neural networks. Furthermore, the performed experiments can be</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">interpreted as dispelling some criticisms aimed at the model's biological plausibility. In the following section, </div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">many of the major questions about the biological plausibility of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (559) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> from the relevant literature are summarized.</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline"><span class="keyword1">\subsection</span>*{Evaluation of biological plausibility}</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">The original dendritic error network by design solves several biologically implausible mechanisms of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (755) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. It</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline"><span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{locally computes prediction errors}} and encodes them within membrane potentials of pyramidal neuron</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">apical dendrites. Furthermore, it provides two separate solutions to the <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{weight transport problem}}:</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">First, it is capable of learning through Feedback Alignment, as was done in all present simulations. Secondly,</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">experiments employing steady-state-approximations have been successful in training feedback weights through variants of</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">the dendritic plasticity rule. An often overlooked property of biological networks is that feedback signals have an</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">immediate impact on a neurons output <span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\citep{</span>Larkum2009,Gilbert2013}. This does not occur in classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (1388) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>, but is</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">an essential feature of the dendritic error network. Finally, the network relies strictly on <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{Local</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">plasticity}} \citep{Whittington2017}, and models a (somewhat limited) variability in cell types</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">\citep{Bartunov2018}.</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">The present work further improves on the neuron model by showing that spike-based communication does not interfere with</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">the dendritic plasticity rule, or the intricate balance of excitation and inhibition demanded by the network.</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">Experiments also showed that the network can be trained with absolutely <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{minimal external control}}</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">\citep{Whittington2017}. The network requires no external interference such as manual resets or phased plasticity, and</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">can handle background noise in between sample presentations. Exploratory experiments were conducted in support of the</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">hypothesis that the plasticity rule is capable of credit assignment when the network conforms strictly to</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline"><span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{Dale's law}} \citep{Bartunov2018}. In related experiments, the network proved very capable of</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (2309) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop-like</span> learning when constrained by a more <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{plausible architecture}} \citep{Whittington2017} in</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">which neuronal populations were connected imperfectly. Finally, the criticism that the network requires pre-training</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">\citep{whittington2019theories} was found to be largely immaterial. The sum of these observations arguably makes the</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">dendritic error model one of the most biologically plausible approximations of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (2674) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> yet. </div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline"><span class="keyword1">\section</span>{Limitations}</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">In spite of these advances, several critical limitations remain. Some general concerns regarding <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (2801) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> were</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">deliberately not addressed in this thesis; It still remains unclear how an agent would come by the labels with which it</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">might perform this type of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (2962) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> approximation \citep{Bengio2015}. For this reason, some researchers question whether</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">brains engage in any kind of supervised learning at all \citep{magee2020synaptic}. </div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">No attempts have yet been made to train the model on anything other than static inputs presented for $10-500ms$.</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">Therefore, it remains to be seen whether the model is capable of handling the kind of temporal variations or sequences</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">of patterns which the cortex is required to process \citep{Yamins2016}.</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Nudging signals}</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">The requirement for one-to-one connections between pyramidal- and interneurons could not be overcome in this thesis.</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">Thus, one of the major criticisms from \citep{whittington2019theories} remains unaccounted for. It should also be noted</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">that these nudging connections transmit somatic activation without any kind of nonlinearity or delay, further making</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">them biologically questionable. This property is likely the largest limitation of the model, and further work is</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">required if it is to be addressed. While most general criticisms of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (3910) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> do not apply, the model in its current</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">state does not plausibly conform to cortical connectivity due to this singular feature.</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Response to unpredicted stimuli}</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">One of the predictions about cortical activity made by predictive coding is an increased network activity in response to</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">sensory input that violates expectations. A diverse set of studies has since reported <span class="highlight-spelling" title="Possible spelling mistake. 'behaviour' is British English.. Suggestions: [behavior] (4286) [lt:en:MORFOLOGIK_RULE_EN_US]">behaviour</span> consistent with this in</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">various primate cortical neurons (see <span class="highlight-sh" title="Do not refer to tables using hard-coded numbers. Use \ref instead. [sh:hctab]">Table 1</span> in \citep{bastos2012canonical} for a review). As the dendritic error</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">network encodes errors in dendritic potentials instead of neuron activations, experiments showed that it does not</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">exhibit this property in any of its populations. In fact, overall network activity in response to a stimulus seems to</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">increase after training. In this way, the model conflicts with empirical data on cortical activity.</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Spike frequencies}</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">As discussed in Section \ref{sec-c-m-api}, the network in its current state is unable to learn efficiently for low</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">values of $\psi$. As a result, the implementation demands physiologically impossible spike frequencies from both</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">pyramidal- and interneurons. While increasing membrane <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [capacitance] (5025) [lt:en:MORFOLOGIK_RULE_EN_US]">capacitances</span> did relax this constraint somewhat, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [this, Thai, AHSI, IHSI, MHSI, THS, TSI] (5074) [lt:en:MORFOLOGIK_RULE_EN_US]">thsi</span> change in</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">turn demands an increase in presentation time per stimulus. Further work is required to determine if the network is</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">capable of learning when spike frequencies are as low as reported for cortical neurons.</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Benchmark datasets}</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">Training on a benchmark dataset would have been very desirable for comparing the spiking implementation to previous</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">iterations of the dendritic error network. Yet as noted previously, the full network dynamics are prohibitively</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">expensive for simulations of large networks. Extrapolating the results from Fig. \ref{fig-benchmark-threads-psi} shows</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">that training on the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MOIST, MIST, MONIST, MN IST, NIST] (5652) [lt:en:MORFOLOGIK_RULE_EN_US]">MNIST</span> dataset is currently unfeasible. A full training with<span class="highlight" title="In English-speaking countries (except for South Africa), the correct thousands separator is a comma.. Suggestions: [5,000,000] (5712) [lt:en:COMMA_PERIOD_CONFUSION]"> $5.000.000$</span> sample</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">presentations (cf. \cite{Haider2021}) would require over one year on 32 threads (excluding testing and validation) with</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">the current configuration of parameters.</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">Experiments with <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [downscale, down scaled] (5894) [lt:en:MORFOLOGIK_RULE_EN_US]">downscaled</span> images and smaller network sizes were conducted. Yet, training for these still took on the</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">order of several days, making the search for suitable parameters very costly. While I am optimistic about the network's</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">capability in general, no parametrization was found in time under which the network was capable of learning <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MOIST, MIST, MONIST, MN IST, NIST] (6224) [lt:en:MORFOLOGIK_RULE_EN_US]">MNIST</span>. </div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">The challenge of identifying adequate parameters was hindered by training speed in multiple experiments. As the</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">implementation of the spiking neuron model took much longer than initially anticipated, time was a limiting factor for</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">all present experiments. Particularly the simulations described in Sections \ref{sec-func-approx}, \ref{sec-c-m-api} and</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">\ref{sec-dales-law} should be repeated with much more varied sets of parameters. The reported results are expected</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">improve from this, and insights to be applicable to experiments on more complex learning tasks. Thus, computational</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">efficiency remains a serious drawback of this model, and has been a major limiting factor for this thesis.</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline"><span class="keyword1">\section</span>{Future directions}</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Computational efficiency}</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">The high computational demands of the network were first reported in the original paper \citep{sacramento2018dendritic}. They were largely alleviated</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">through steady-state approximations and the addition of Latent Equilibrium. The present SNN implementation reintroduces</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">this issue, and regrettably exacerbates it considerably. Some improvements can be expected by parameter optimizations</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">such as lowering stimulus presentation time or spike frequencies. Yet to a degree, decreased speed is an <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [invertible] (7382) [lt:en:MORFOLOGIK_RULE_EN_US]">inadvertible</span></div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">price to be paid for a more exact modelling of neuronal processes. Therefore, any attempt at introducing new properties</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">of biological neurons must be expected to further increase computational demands. Given the (still very high) level of</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">abstraction of the developed model paired with its poor speed, this perspective is slightly concerning. Hence, the</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">model requires rigorous optimization.</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">Some initial directions for this are provided by the benchmarks performed in this study. The spike-based plasticity rule</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">for example is highly costly. One possible optimization was already provided by \citep{Stapmanns2021}. The authors</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">discuss an alternative variant for implementing the dendritic plasticity rule. Instead of a strictly event-based or</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">time-based update rule, a hybrid algorithm called ``Event-based update with compression`` is presented. This variant</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">tolerates an increased number of synapse updates, but in exchange removes redundant computations. In initial tests, it</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">proved particularly advantageous for networks in which neurons had a large in-degree. Therefore, this alternative</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">integration scheme can be expected to perform well for training in the larger networks demanded by more complex</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">datasets. Regrettably, it was not available in time for this thesis, so potential gains remain speculative.</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">An alternative improvement to efficiency is approximating the plasticity rule with the instantaneous error at the time</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">of a spike. This would eliminate the requirement for both frequent updates, and for storing and reading a history of</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">dendritic error. Thus, a network employing this simplified plasticity rule would be much less computationally costly. As</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">shown in Fig. \ref{fig-error-comp-le}, error terms in LE networks relax after only a few simulation steps. Thus,</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">under the condition that only static inputs are considered, this crude approximation is expected to perform fairly well.</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">The neuron model should likewise be investigated for potential improvements in terms of efficiency. Modeling</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">interneurons without an apical compartment might yield some improvements (although initial experiments have dampened</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">expectations for this). It is also possible, that the network does not require integration <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [time steps] (9580) [lt:en:MORFOLOGIK_RULE_EN_US]">timesteps</span> as low as $0.1ms$,</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">which has not been investigated yet.</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">Finally, further tuning the network's hyperparameters is expected to yield settings that are less computationally</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">costly. Network relaxation time, stimulus presentation time, spiking frequencies and learning rates form a complex</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">interplay in this model which has not yet been fully explored. It is therefore to be expected that further optimization</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">of these might yield networks that can be trained with both lower computational time, and fewer training samples.</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Meromorphic, Neurotrophic] (10110) [lt:en:MORFOLOGIK_RULE_EN_US]">Neuromorphic</span> hardware}</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">A different approach which likely would vastly improve simulation speed is a full re-implementation of the model on</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (10249) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphic</span> hardware. This network fits the self-described niche of such systems almost perfectly; it employs strictly</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">local plasticity rules, its nodes use leaky membrane dynamics and communicate through binary spikes. By a rough</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">estimation, even the first generation of Intel's <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (10530) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> chips \citep{davies2018loihi} should be capable of simulating</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">this neuron model. The chip is capable of modelling multiple dendritic trees per neuron, and the learning engine appears</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">capable of <span class="highlight-spelling" title="Possible spelling mistake found. (10710) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn-like</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [plasticity It] (10730) [lt:en:MORFOLOGIK_RULE_EN_US]">plasticity\footnote{It</span> is possible that the plasticity rule would need to be approximated</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">somewhat for <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (10823) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> 1. The publicly available information about the follow-up chip <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (10892) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> 2 \citep{Davies2021} is still</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">somewhat sparse, but it claims to support a much more diverse set of learning rules<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (10996) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Of course, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (11010) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> is only one of</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">many <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (11036) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphic</span> systems. Another popular system is <span class="keyword1">\textit</span>{BrainScaleS-2}, which appears to be spearheading the field</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">with regard to simulating segregated dendrites \citep{Kaiser2022}. Regardless of the exact system used, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (11231) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphic</span></div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">hardware promises to reduce the high computation time which currently obstructs further research into the model.</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Neuron model}</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">Two properties that are part of most spiking neuron models, but have not been investigated here, are membrane reset and</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">refractory periods. Combined, these modifications would change the spike generation process to that of a stochastic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [IF, LIFE, LIE, LIFT, LID, LIP, LIN, LIZ, AIF, LEIF, LIB, LIFO, RIF, LIT, LIQ, BIF, CIF, DIF, FIF, GIF, HIF, IIF, LBF, LCF, LDIF, LEF, LFI, LGF, LI, LIA, LIH, LIM, LIR, LIX, LOF, LPF, LRF, LSF, LTF, LUF, MIF, NIF, PIF, WIF, ZIF, LIEF, LII, L IF, LI F, L&amp;F, LDF, LF, LIC, LIU, LIV, TIF] (11609) [lt:en:MORFOLOGIK_RULE_EN_US]">LIF</span></div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">neuron. Such neuron models have previously performed well for modelling sensory representations in the cortex</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">\citep{Pillow2008}. Another neuron property considered in that study is <span class="keyword1">\textit</span>{spike-frequency-adaptation}. Neurons</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">with this mechanism increase their threshold potential in response to previous activity. Such adaptability has been</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">observed in $\sim 20 \% $ of neurons in the mouse visual cortex \citep{allen2018}, and has been shown to significantly</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">improve performance in recurrent SNN \citep{bellec2018long,bellec2020solution}. These three changes together would</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">significantly improve correspondence of the neuron model to physiological data, while potentially also improving their</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">computational power.</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">Another point which has not yet been reviewed in terms of biological plausibility is the prospective firing rate in LE</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">neurons. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (12371) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> et al.\ claim that it ``represents a known, though often neglected feature of (single) biological</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">neurons'' \citep{Haider2021}. They partly base this assessment on the fact that neurons show an increased sensitivity to</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">coincident spikes through Na channel responses \citep{Platkiewicz2011}. Further work is required to determine <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [whether, weather, ether, tether, wetter, wither, nether, w ether, wet her] (12672) [lt:en:MORFOLOGIK_RULE_EN_US]">wether</span></div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">prospective activations - particularly as a basis for spike probabilities - appropriately model such processes.  </div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Plasticity rule}</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">The model of the dendritic trees described here is very rudimentary, which has implications for the plasticity</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">mechanism. While the <span class="highlight-spelling" title="Possible spelling mistake found. (12943) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity has been argued to be a type of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Serbian, Lesbian, Debian, Hessian] (13001) [lt:en:MORFOLOGIK_RULE_EN_US]">Hebbian</span> learning</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">\citep{gerstner2018eligibility,urbanczik2014learning}, it leads to substantially different weight changes. Reviewing the</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">literature further yielded no arguments that the <span class="highlight-spelling" title="Possible spelling mistake found. (13138) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity relies on any biologically implausible</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">mechanisms</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">\citep{magee2020synaptic,Lillicrap2020,Poirazi2020,sacramento2018dendritic,guerguiev2017towards,Marblestone2016}. Yet</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline">given the extensive amount of data in support of STDP</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">\citep{magee2020synaptic,gerstner2018eligibility,Bengio2015,Marblestone2016}, the burden of proof is on the dendritic</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">plasticity rule to override this dogma. This includes finding brain networks which can be modeled by using it, as have</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">been identified repeatedly for STDP. A fruitful approach might be to investigate STDP in multi-compartment models which</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">simulate dendritic spikes and plateau potentials in search of similar plasticity dynamics. Exciting advances in this</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">direction have recently been reported \citep{Bono2017,Schiess2016,magee2020synaptic}, and could potentially be</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">integrated into the present neuron model.</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline"><span class="keyword1">\subsection</span>*{Cortical circuitry}</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">The circuitry surrounding the dendritic error network is very simplistic compared to the intricate networks of cortical</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">microcircuits - not to mention its numerous connections to other parts of the brain. In this regard, the network still</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">holds much room for improvement. Furthermore, a seminal model for how the cortical microcircuit might be able to perform</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">predictive coding \citep{bastos2012canonical} appears to conflict with the dendritic error model. While the resulting</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">network has not yet been computationally modeled, it is backed by a vast amount of empirical data and regarded highly in</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">the literature \citep{Lillicrap2020,Park2013,whittington2019theories}. One important hypothesis made by it, is that prediction errors are encoded in separate neuronal</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">populations, rather than dendritic compartments. This claim is shared by other works</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">\citep{Hertaeg2022,Whittington2017}, and further work is required to find out if the two hypotheses can be reconciled.</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">A first step towards an integration of the dendritic error model and the cortical circuitry is provided in this thesis.</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">Present experiments show that the plasticity rule is capable of assigning credit indirectly when dale's law is upheld</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">via additional interneuron populations. Extending this principle to all sets of synapses in the network would introduce</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">novel interneuron populations demanding to be identified. The extent to which the resulting network is compatible with</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">cortical circuitry could prove valuable for further judging the plausibility of the dendritic error model.</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Deep Feedback Control}</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">A completely novel solution to the credit assignment problem is provided by <span class="keyword1">\textit</span>{Deep Feedback control}</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">\citep{Meulemans2021,Meulemans2022}. Instead of approximating <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (15410) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>, this algorithm performs Gauss-Newton</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">optimization, thus employing a previously unexplored approach to training deep neural networks. While originally</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">described as a purely mathematical model, it might be considered even more biologically plausible than the dendritic</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">error network. It considers many features of pyramidal neuron dendrites without being held back by any of the common</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (15804) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> criticisms or the constrained connectivity of the present model. The authors also show that it shares a close</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">mathematical relationship to the dendritic error network, incorporating some interneuron computations into the pyramidal</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">neurons. Its connectivity however is therefore even further abstracted from the cortical circuitry than the one</div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline">discussed here. Regardless of its exact details, this algorithm provides an important insight which was largely</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">neglected in this thesis (and perhaps the surrounding niche of the neuroscience community) so far: <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (16367) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> is not the</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">only <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [competitive] (16392) [lt:en:MORFOLOGIK_RULE_EN_US]">competetive</span> mechanism for assigning synaptic credit in neural networks. Therefore, focussing too narrowly on this</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">singular solution might prevent us from considering viable alternatives.</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline"><span class="keyword1">\section</span>{Conclusion}</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">This project further establishes the dendritic error network as one of the most biologically plausible mechanisms for</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">approximating the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (16730) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors algorithm. As hypothesized, the spiking variant of the <span class="highlight-spelling" title="Possible spelling mistake found. (16811) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span></div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">plasticity is capable of performing well in a much more demanding setting than previously shown. The model furthermore</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">proved to be largely unhindered by the vast majority of biological constraints which were imposed on it. Particularly</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">constraints on connectivity and synaptic polarity were shown to impede learning to only minor degrees. The model</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">overcomes all but a few of the general arguments for claiming that the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (17247) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> algorithm could not plausibly be</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">implemented by networks of biological neurons. Only when investigating the correspondence to cortical microcircuits more</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">closely does the network exhibit serious limitations. </div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">The predictive coding hypothesis has had a substantial impact on the recent developments in cognitive science. The</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">dendritic error network is a promising model for explaining how individual computations of this hypothesis could be</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">distributed across cortical neurons. Finding a biologically plausible mechanism that replaces (or alternatively</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">explains) the <span class="highlight" title="This word is normally spelled as one.. Suggestions: [interlayer] (17830) [lt:en:EN_COMPOUNDS]">inter-layer</span> nudging signals would arguably elevate it to a prime contender for explaining supervised</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">learning in the cortex. Nevertheless, such optimism must be paired with restraint given the vast complexity of the</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">cortex - and associated areas - which a general model would have to encompass. Either way, further research into the</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">dendritic error network is likely to help us better understand the intriguing capabilities of the human brain.</div><div class="clear"></div>
</div>
<hr/>
Output produced by TeXtidote v0.8.4, &copy; 2018-2022 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/textidote">TeXtidote website</a> for more information.
</body>
</html>

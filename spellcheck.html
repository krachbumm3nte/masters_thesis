<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,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);
}
h2.filename {
  font-family: monospace;
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.linenb {
  font-style: italic;
  color: lightgrey;
  width: 30pt;
  float: left;
  margin-top: 1pt;
  margin-bottom: 1pt;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.codeline {
  margin-left: -30pt;
  padding-left: 60pt;
  margin-top: 1pt;
  margin-bottom: 1pt;
}
.no-text {
  display: none;
}
.clear {
  clear: both;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your file(s) with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<h2 class="filename">03_results.tex</h2>

<p>Found 75 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Results}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">The following results are exploratory in nature, and After some poor initial results the focus was laid on proving that</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">the network can perform at all, rather than fine-tuning hyperparameters towards optimal performance. This decision was</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">in part motivated by a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [prioritization, periodization] (272) [lt:en:MORFOLOGIK_RULE_EN_US]">priorization</span> of gaining neuroscientific insights over targeting minimal loss. It should be noted,</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">that training the network is computationally quite costly (c.f. Section \ref{sec-benchmark}) which turned parameter</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">studies into a time-consuming process.</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">Early experiments showed that the network is rather sensitive to parameter changes. The search for default parameters</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">took some effort, as a certain heterogeneity exists in the two existing implementations</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">\citep{sacramento2018dendritic,Haider2021}, both in hyperparameters as in the simulation environment. This model</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">includes properties of both variants, while relying more strongly on the LE implementation. Unless stated otherwise,</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">neurons employ prospective activation functions in all simulations. So far, no drawbacks to this mechanism have</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">presented <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [themselves] (1027) [lt:en:MORFOLOGIK_RULE_EN_US]">themseleves</span>, and learning speed can be increased drastically compared to the original implementation. The full</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">default parametrization is shown in Supplementary Table \ref{tab-params}. Since it was anticipated that the spiking</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">implementation would perform worse than the rate-based variant, the first goal was to measure how big this difference in</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">performance is. Furthermore, a relevant question was to what degree the synaptic delays enforced by NEST would influence</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">performance of the rate model. These questions will be answered in the upcoming sections. Note that not all experimental</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">results were given their own Figures. In these cases, plots can be found in the electronic supplementary material.</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline"><span class="keyword1">\section</span>{The self-predicting state}</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">As a first comparison between the three <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [implementations] (1786) [lt:en:MORFOLOGIK_RULE_EN_US]">implmementations</span>, the pre-training towards a self-predicting state (cf.</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">\citep{sacramento2018dendritic}[Fig. S1]) was performed. For this experiment, no target signal is provided at the output</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">layer, and the network is tasked with learning to self-predict top-down input. The network is initialized with fully</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">random weights and stimulated with random inputs from a uniform distribution between 0 and 1. A comparison of the four</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">error metrics between implementations is shown in Fig. \ref{fig-self-pred}.</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_self_prediction}</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">    <span class="keyword1">\caption</span>[Training towards the self-predicting state]{Training towards the self-predicting state. All implementations</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">        learn to predict self-generated top-down signals. Networks were initialized with the same random weights for</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">        dimensions $[5, 8, 3]$, and stimulated with $5000$ samples of random input for $100ms$ each. As described in</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">        \citep{sacramento2018dendritic}, during this phase only $Pyr \rightarrow Intn$ and $Intn \rightarrow Pyr$</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">        weights are plastic ($\eta^{pi}=0.05, \eta^{ip}=0.02375, \eta^{up}_0=\eta^{up}_1=\eta^{down}=0$).}</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">    <span class="keyword1">\label</span>{fig-self-pred}</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">Both rate neuron implementations were able to reach comparable values for all error metrics after roughly the same time.</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">The exact values that errors converge on differs slightly between implementations, with no implementation being clearly</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">superior. This is an important result for upcoming experiments, as it indicates that both training environment (current</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">injections, simulation time, membrane reset, readouts, etc.) and the actual neuron model of the NEST version adequately</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">replicate the original model.</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">For the spiking variant, Interneuron- and its corresponding <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [feedforward] (2821) [lt:en:MORFOLOGIK_RULE_EN_US]">feedfworward</span> weight error are comparable to the other</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">implementations. In fact these metrics appear to converge slightly faster to comparable values. The primary limitation</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">of this version are the apical error and the closely correlated FB error. After appearing to converge very</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">quickly, the two metrics stagnate at very high levels. These high errors correlate with strong fluctuations of the</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">apical compartment. These fluctuations can likely at least in part be attributed to low spike frequencies. This was</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">confirmed by repeating the experiment with $\psi=1500$, which alleviated the issue to a degree (results not shown). Yet,</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">error values were still inferior to the rate models, and this change came at the cost of substantially increased</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">training time. Therefore, this approach was not pursued much further. A different possible solution is, to increase the</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">membrane capacitance of the apical compartment in order to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [smooth, smoother, smoothed, soothe, smoothie, smooths, smoothen] (3735) [lt:en:MORFOLOGIK_RULE_EN_US]">smoothe</span> out the fluctuations induced by individual spikes.</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">This will be discussed in Section \ref{sec-c-m-api}.</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">In most simulations in the literature, the network is initialized to an ideal self-predicting state. <span class="highlight" title="A comma may be missing after the conjunctive/linking adverb 'Furthermore'.. Suggestions: [Furthermore,] (3933) [lt:en:SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA]"></span>Furthermore</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">feedback weights are non-plastic in many experiments ($\eta^{pi}=\eta^{down}=0$). Therefore, a failure to perfectly</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">learn this weight symmetry should not fundamentally hinder learning. For the time being, showing that the network</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">approaches a self-predicting state was deemed a sufficient result.</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline"><span class="keyword1">\section</span>{Presentation times and latent equilibrium}<span class="keyword1">\label</span>{sec-le-tpres}</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">In order to validate the performance of the NEST implementations on a learning task, the parameter study from</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">\citep{Haider2021}[Fig. 3] was replicated. In this experiment, the network is trained with different stimulus</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">presentation times $t_{pres} \in \{0.3,\ 500\}ms$. Performance of the original Dendritic error network is compared to</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">the improved model which employs LE. Due to the costly computation of the network under such long $t_{pres}$, a simple</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">artificial classification dataset was used. The <span class="keyword1">\textit</span>{Bars-dataset} is defined for $3\times3$ input- and $3$ output</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">neurons. <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [It] (4774) [lt:en:UPPERCASE_SENTENCE_START]">it</span> consists of three horizontal, three vertical, and two diagonal bars in the $3\times3$ grid, which are to be</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">encoded in a 'one-hot-vector' at the output layer. In the experiment, networks of $9-30-3$ pyramidal neurons per layer</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">were trained for 1000 Epochs of 24 samples each. Networks were initialized to the self-predicting state and only</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">feedforward $Pyr\rightarrow Pyr$ and $Pyr \rightarrow Intn$ synapses were plastic. Learning rates scaled inversely with</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">presentation times: $\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Étaín] (5203) [lt:en:MORFOLOGIK_RULE_EN_US]">eta^{ip}</span>_0 = \frac{0.2}{t_{pres}}, \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [teacup] (5226) [lt:en:MORFOLOGIK_RULE_EN_US]">eta^{up}</span>_0 = \frac{0.5}{t_{pres}}, \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [teacup] (5249) [lt:en:MORFOLOGIK_RULE_EN_US]">eta^{up}</span>_1 =</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">    \frac{0.1}{t_{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [press, Pres, pres, PRESO] (5269) [lt:en:MORFOLOGIK_RULE_EN_US]">pres}}$</span>. The results for the spiking NEST network are shown in Fig. \ref{fig-bars-le-snest}, while the</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">results for NumPy and Rate NEST variants are depicted in Supplementary Figures \ref{fig-bars-le-numpy} and</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">\ref{fig-bars-le-rnest}, respectively.</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_3_snest}</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">    <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>[Replication of Fig. 3 from \citep{Haider2021</span>}]{Replication of Fig. 3 from \citep{Haider2021} using networks</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">        of spiking neurons in the NEST simulator. <span class="keyword1">\textbf</span>{A:} Comparison between Original dendritic error network by and</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">        an identical network employing Latent equilibrium. Shown is the training of networks with 9-30-3 neurons on the</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">        Bars-dataset from with three different stimulus presentation times. <span class="keyword1">\textbf</span>{B:} Test performance after 1000</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">        Epochs as a function of stimulus presentation time.}</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">    <span class="keyword1">\label</span>{fig-bars-le-snest}</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">For the original dendritic error model, performance in all three implementations is close to being identical. <span class="highlight" title="A verb may be missing.. Suggestions: [This is] (5563) [lt:en:THIS_MISSING_VERB]">This</span> an</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">important finding as it answers two open questions: Changes made for a NEST-compatible implementation were adequate and</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">result in identical learning <span class="highlight-spelling" title="Possible spelling mistake. 'behaviour' is British English.. Suggestions: [behavior] (5720) [lt:en:MORFOLOGIK_RULE_EN_US]">behaviour</span> between the rate-based implementations. Learning <span class="highlight-spelling" title="Possible spelling mistake. 'behaviour' is British English.. Suggestions: [behavior] (5779) [lt:en:MORFOLOGIK_RULE_EN_US]">behaviour</span> of the spiking model</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">is <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [competitive] (5813) [lt:en:MORFOLOGIK_RULE_EN_US]">competetive</span>, confirming the hypothesis that the spike-based dendritic plasticity model is capable of more complex</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">credit assignment tasks than previously shown. In this regard, the implementation can be considered a success.</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">The results for the LE network experiments are somewhat more interesting. For very long $t_{pres}$, both rate</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">implementations behave the same. <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Yet] (6173) [lt:en:UPPERCASE_SENTENCE_START]">yet</span> the NEST implementation requires considerably more epochs for training, as</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">$t_{pres}$ is reduced. For very low presentation times, this behavior was somewhat expected, due to the synaptic delay</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">enforced by NEST. The NumPy variant computes a full forward pass of the network during a single simulation step, as all</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">layers are processed in sequence. Only feedback signals from pyramidal neurons are delayed by one <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [time step] (6580) [lt:en:MORFOLOGIK_RULE_EN_US]">timestep</span> in this</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">simulation backend. In NEST, all connections have a minimum synaptic delay of $\Delta t$. Therefore, for very short</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">presentation times the NEST network can not be expected to perform well, as signals have no time to traverse the</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">network. It remains an open question whether this feature alone explains the gradual decrease in performance observed</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">here, or if there is an undiscovered error within the novel neuron models or simulation environment. The exceptionally</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">short stimulus presentation times investigated by \citep{Haider2021} are themselves questionable in terms of biological</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">plausibility, as they are much lower than pyramidal neuron time constants \citep{McCormick1985}. Thus, no attempts were</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">made to improve performance for very low $t_{pres}$.</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">The spiking variant proved similarly sensitive to presentation times as the other NEST variant. While obtaining similar</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">final accuracy, it required - at best - twice as many stimulus presentations as its direct competitor. This result,</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">while somewhat expected, shows that for low $t_{pres}$, spiking communication leads to worse learning performance. It</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">also shows that the relaxation problem affects all communication schemes equally. While the utility of LE is</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">substantially higher for rate neurons, it does improve performance and efficiency of the spiking variant. For this</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">reason, LE will be turned on in all upcoming simulations.</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline"><span class="keyword1">\section</span>{Approximating arbitrary functions}<span class="keyword1">\label</span>{sec-func-approx}</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">To confirm that the spiking network is capable of learning more complex tasks, it was trained to match the input-output</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">mapping of a separate teacher network. This is an established method for showing that a network can approximate</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">arbitrary functions. A comparison between different </div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.95\textwidth]{fig_function_approximator}</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">    <span class="keyword1">\caption</span>[SNN learns to match separate teacher network]{SNN learns to match separate teacher network. Networks of</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">    size $15-n_{hidden}-5$ neurons per layer were trained on input-output pairings of a randomly initialized feedforward</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">    network of size $15-15-5$. Each network was trained on 300000 samples of the same teacher network while all somatic</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">    compartments received background noise with a standard deviation of $0.01$.}</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">    <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-func-approx is never referenced in the text [sh:figref]">fig-func-approx</span>}</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">As the task was too easy when inputs to the teacher network were strictly positive (as is the case in the</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">self-prediction paradigm), inputs were drawn from a uniform distribution $U(-1,1)$. To facilitate inhibitory stimulation</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">to the spiking network, a separate population was required at the input layer. This population was initialized with</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">inverted weights compared to the excitatory population and received all negative values of a given input vector. Due to</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">this necessity, the spiking network had to learn an additional set of weights, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [with, which, rich, wish, Rich, witch, wick, winch, Mich, ICH, WIC, Wish] (8790) [lt:en:MORFOLOGIK_RULE_EN_US]">wich</span> might be one of the reasons why</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">overall performance stayed behind expectations.</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">\todo{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Complete] (8876) [lt:en:UPPERCASE_SENTENCE_START]"></span>complete}</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline"><span class="keyword1">\section</span>{Apical compartment capacitance}<span class="keyword1">\label</span>{sec-c-m-api}</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">Next, an investigation was made into lowering apical- and FB errors of the SNN. The hypothesis was that a</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">smoothing of apical compartment voltage would lead to a decrease in both errors. To test this, the Self-predicting</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">experiment was repeated with numerous values for apical compartment capacitance $C_m^{api} \in \{ 1, 250 \} pF$ (results</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline">not shown). The experiment showed that for $C_m^{api} = 50pF$, apical error is almost halved ($0.0034 \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (9316) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>rightarrow</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">0.0019$), and FB error is decreased by $80\%$ ($0.15 \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (9345) [lt:en:MORFOLOGIK_RULE_EN_US]">rightarrow</span> <span class="highlight" title="The currency mark is usually put at the beginning of the number.. Suggestions: [$0.027] (9356) [lt:en:CURRENCY]">0.027$</span>). These values are still at least an order of</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">magnitude higher than the rate implementations, but mark a substantial improvement. Increasing the parameter beyond this</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">point further decreased apical error, but came at the cost of slower convergence. Higher membrane <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [capacitance] (9628) [lt:en:MORFOLOGIK_RULE_EN_US]">capacitances</span> in</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">general increase the relaxation period of the entire network. Thus, <span class="highlight" title="A verb seems to be missing. Did you mean 'they're requiring', 'they are requiring', or 'they were requiring'?. Suggestions: [they're requiring, they are requiring, they were requiring] (9712) [lt:en:PRP_VBG]">they requiring</span> a highly undesirable increase in</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">$t_{pres}$ for successful learning.</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_c_m_psi}</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">    <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>[Comparison of performance for different configurations of $\psi$ and $C_m^{api</span>}$]{Comparison of performance</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">    for different  configurations of $\psi$ and $C_m^{api}$.}</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">    <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-c-m-psi is never referenced in the text [sh:figref]">fig-c-m-psi</span>}</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">A secondary objective of training with different apical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [capacitance] (9845) [lt:en:MORFOLOGIK_RULE_EN_US]">capacitances</span> was to enable learning with lower $\psi$ and</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">therefore approach biologically plausible firing rates. To test this, training on the <span class="highlight" title="An apostrophe may be missing.. Suggestions: [Bars', Bar's] (9984) [lt:en:POSSESSIVE_APOSTROPHE]">Bars</span> dataset was performed under</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">different combinations of apical capacitance and $\psi$. \todo{describe <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [paras, prams, pa rams, para ms] (10078) [lt:en:MORFOLOGIK_RULE_EN_US]">params</span> and results}</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">Hence, another tradeoff between performance and training duration is introduced by this parameter.</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline"><span class="keyword1">\section</span>{Imperfect connectivity}</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">Connectivity within cortical circuits, while structured, appears to subject to a high degree of randomness</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">\citep{potjans2014cell} As noted before, one-to-one connections between pairs of neurons are therefore highly unlikely</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline">\citep{whittington2019theories}. On the other hand, 'fully connected' populations of neurons likewise have not been</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">observed in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [electrophysiologist, electrophysiologists] (10531) [lt:en:MORFOLOGIK_RULE_EN_US]">electrophysiological</span> \citep{thomson2002synaptic} and <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [invited, intro, invite, invites, nitro, invitee, initio, inviter, inviters] (10560) [lt:en:MORFOLOGIK_RULE_EN_US]">in-vitro</span> \citep{binzegger2004quantitative} analyses of</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (10585) [lt:en:MORFOLOGIK_RULE_EN_US]">cortico-cortical</span> connectivity. Therefore, any network proclaiming to model the cortex must invariably be capable of</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">handling this imperfect connectivity.</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_dropout}</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">    <span class="keyword1">\caption</span>[Error terms after training with synapse dropout]{Error terms after training with synapse dropout. Networks</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">        with $8-8-8$ neurons per layer were trained towards the self-predicting state, with different percentages of</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline">        synaptic connections randomly removed. Experiments were performed with the rate-based network in NEST, each</div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">        network was trained for 2000 epochs of 50ms each. Errors are averaged over 6 independent runs for each</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">        configuration.}</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">    <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-dropout is never referenced in the text [sh:figref]">fig-dropout</span>}</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">To test if the dendritic error model <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [fulfills, full fills] (10778) [lt:en:MORFOLOGIK_RULE_EN_US]">fullfills</span> this requirement, in a first step the self-prediction experiment was</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">repeated with neuron dropout. To simulate connection probabilities $p_{conn} \in {0.6, 1.0}$, an appropriate number of</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">synapses was deleted after network setup. To avoid completely separating two neuron populations, this deletion was</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">performed separately for each of the four synaptic populations.</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">As expected, removing synapses caused an increase in all four error metrics. Yet even with only 60\% of synaptic</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">connections present, the network manages to vastly improve from its random initialization. Weight errors are calculated</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">as mean squared errors over the two matrices, which requires matrices to contain data at every cell. Thus, to compute</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">these errors, weights of deleted synapses were set to zero in these matrices. This choice was made under the assumption</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">that a missing connection in an ideal self-predicting network would be matched by a zero-weight - or likewise absent -</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">synapse. These results indicate that the dendritic error rule is capable of compensating for absent synapses by</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">correctly identifying and depressing corresponding feedback connections (and vice versa). Through this mechanism, the</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">network is able to retain its self-predicting properties in spite of physiological constraints.</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">The extent of this capability was confirmed in SNN, by comparing performance on the <span class="highlight" title="An apostrophe may be missing.. Suggestions: [Bars', Bar's] (12134) [lt:en:POSSESSIVE_APOSTROPHE]">Bars</span> dataset of a control network to</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">a <span class="keyword1">\textit</span>{dropout network}. Both networks were initialized with random synaptic weights, and trained with full</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">plasticity ($\eta^{ip}_0 = 0.004, \eta^{pi}_0 = 0.01, \eta^{up}_0 = 0.01, \eta^{up}_1 = 0.003$) to best enable the</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">dropout network to compensate for missing synapses. The dropout network was initialized with $40$ instead of $30$ hidden</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">layer pyramidal neurons to counteract the deletion of $15\%$ of synapses per synaptic population. Both networks</div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline">performed very similarly, with the control network reaching $100\%$ accuracy somewhat faster (Epoch 140 vs. Epoch 200),</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">while the dropout network exhibited slightly lower test loss at the end of training (results not shown).</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">These results prove that the dendritic error model is capable of learning in spite of imperfect connectivity, which must</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline">be expected to occur in the cortex. This sets it apart from the previous implementation of a predictive coding network</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">\citep{Whittington2017}, and further supports its biological plausibility.</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">section</span>{Separation of synaptic polarity}</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">A dogma held in neuroscience for a long time now has been the notion that all neurons are either excitatory or</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">inhibitory, as dictated by their type (also known as Dale's law \citep{Kandel1968}). Several studies have since shown</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">that some neurons violate this law through co-transmission or specific release sites for different neurotransmitters</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">\citep{Svensson2019,Barranca2022}. Despite these findings, pyramidal neurons are still regarded to release exclusively</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">Glutamate, therefore being strictly excitatory \citep{gerfen2018long,spruston2008pyramidal,Eyal2018}.</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">A key limitation of the present network model is the requirement that all synapses must be able to assume both positive</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">and negative polarities. When restricting any synaptic population in the network to just one polarity, the network is</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">unable to reach the self-predicting state \todo{expand?}. Thus, activity in any neuron must be able to have both</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">excitatory and inhibitory postsynaptic effects facilitated by appropriate synaptic weights. This requirement is at odds</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">with biology, which dictates a singular synaptic polarity for all outgoing connections of a neuron, determined by neuron</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">type and its corresponding neurotransmitter <span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (14188) [lt:en:MORFOLOGIK_RULE_EN_US]">citem</span>e</span>.</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline">To investigate to what degree the plasticity rule can deal with this constraint, an experiment was conducted: A</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline">population of pyramidal neurons $A$  was connected to another population $C$ with plastic synapses that were constrained</div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">to positive weights. In order to facilitate the required depression, $A$ was also connected to a population of</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline">inhibitory interneurons $B$ through excitatory synapses with random and non-plastic weights. The interneurons in turn</div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">were connected to $C$ through plastic, inhibitory connections. All incoming synapses at $C$ targeted the same dendritic</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline">compartment. When inducing a dendritic error in that compartment, all plastic synapses in the network collaborated in</div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline">order to minimize that error. When injecting a positive basal error for example, the inhibitory weights (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [BC, PC, JC, QC, WC, AC, C, CC, DC, EC, FC, KC, LC, MC, NC, RC, SC, TC, UC, VC, ZC, °C, GC, HC, IC, OC, YC] (14991) [lt:en:MORFOLOGIK_RULE_EN_US]">$C</span> \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (14995) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>rightarrow</div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline">    B$) decayed, while excitatory synaptic weights ($A \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (15015) [lt:en:MORFOLOGIK_RULE_EN_US]">rightarrow</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [BY, BE, BC, BT, B, BA, BB, BD, BF, BM, BO, BP, BR, BS, BU, BI, BK, BL, BX, BG, BH, BJ, BV] (15026) [lt:en:MORFOLOGIK_RULE_EN_US]">B$</span>) increased. Flipping the sign of that error injection</div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline">had the opposite effect on weights, and likewise cancelled the artificial error. This shows that a separation of</div><div class="clear"></div>
<div class="linenb">252</div><div class="codeline">synaptic polarity does not interfere with the principles of the <span class="highlight-spelling" title="Possible spelling mistake found. (15260) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity when depression is facilitated</div><div class="clear"></div>
<div class="linenb">253</div><div class="codeline">by interneurons.</div><div class="clear"></div>
<div class="linenb">254</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">255</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">256</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">257</div><div class="codeline">    <span class="keyword2">\begin{minipage}</span>{0.2\textwidth}</div><div class="clear"></div>
<div class="linenb">258</div><div class="codeline">        <span class="keyword1">\textbf</span>{a)}\par\medskip</div><div class="clear"></div>
<div class="linenb">259</div><div class="codeline">        \centering</div><div class="clear"></div>
<div class="linenb">260</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_exc_inh_network}</div><div class="clear"></div>
<div class="linenb">261</div><div class="codeline">    <span class="keyword2">\end{minipage}</span>\hfill</div><div class="clear"></div>
<div class="linenb">262</div><div class="codeline">    <span class="keyword2">\begin{minipage}</span>{0.7\textwidth}</div><div class="clear"></div>
<div class="linenb">263</div><div class="codeline">        <span class="keyword1">\textbf</span>{b)}\par\medskip</div><div class="clear"></div>
<div class="linenb">264</div><div class="codeline">        \centering</div><div class="clear"></div>
<div class="linenb">265</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_exc_inh_split}</div><div class="clear"></div>
<div class="linenb">266</div><div class="codeline">    <span class="keyword2">\end{minipage}</span></div><div class="clear"></div>
<div class="linenb">267</div><div class="codeline">    <span class="keyword1">\caption</span>[Error minimization under biological constraints on synaptic polarity and network connectivity]{Error</div><div class="clear"></div>
<div class="linenb">268</div><div class="codeline">        minimization under biological constraints on synaptic polarity and network connectivity. <span class="keyword1">\textbf</span>{a)} Network</div><div class="clear"></div>
<div class="linenb">269</div><div class="codeline">        architecture. An excitatory population $A$ connects to a dendrite of Neuron $C$ both directly and through</div><div class="clear"></div>
<div class="linenb">270</div><div class="codeline">        inhibitory interneuron population $B$. Only synapses $A\rightarrow C$ and $B \rightarrow C$ are plastic through</div><div class="clear"></div>
<div class="linenb">271</div><div class="codeline">        dendritic error rules. Populations $A$ and $B$ are fully connected with random weights. <span class="keyword1">\textbf</span>{b)}</div><div class="clear"></div>
<div class="linenb">272</div><div class="codeline">        <span class="keyword1">\textit</span>{Left:} All plastic synapses arrive at apical dendrites and evolve according to Equation</div><div class="clear"></div>
<div class="linenb">273</div><div class="codeline">        \ref{eq-delta_w_pi}. <span class="keyword1">\textit</span>{Right:} Identical network setup, plasticity for synapses at basal dendrites</div><div class="clear"></div>
<div class="linenb">274</div><div class="codeline">        (Equations \ref{eq-delta_w_up}, \ref{eq-delta_w_ip}). <span class="keyword1">\textit</span>{Top:} Dendritic error of a single target neuron.</div><div class="clear"></div>
<div class="linenb">275</div><div class="codeline">        Errors of opposite signs are induced at $0$ and $500ms$ (vertical dashed line). <span class="keyword1">\textit</span>{Bottom:} Synaptic</div><div class="clear"></div>
<div class="linenb">276</div><div class="codeline">        weights of incoming connections. All initial synaptic weights and input neuron activations were drawn from</div><div class="clear"></div>
<div class="linenb">277</div><div class="codeline">        uniform distributions.}</div><div class="clear"></div>
<div class="linenb">278</div><div class="codeline">    <span class="keyword1">\label</span>{fig-exc-inh-split}</div><div class="clear"></div>
<div class="linenb">279</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">280</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">281</div><div class="codeline">Yet, as <span class="highlight-spelling" title="Possible spelling mistake. 'criticised' is British English.. Suggestions: [criticized] (15344) [lt:en:MORFOLOGIK_RULE_EN_US]">criticised</span> previously \citep{whittington2019theories}, the one-to-one connections between $A$ and $B$ are</div><div class="clear"></div>
<div class="linenb">282</div><div class="codeline">untypical for biological neural networks \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (15460) [lt:en:MORFOLOGIK_RULE_EN_US]">citeme</span>. Hence, a second experiment was performed, in which $A$ and $B$ were</div><div class="clear"></div>
<div class="linenb">283</div><div class="codeline">fully connected through static synapses with random positive weights. This decrease in specificity of the connections</div><div class="clear"></div>
<div class="linenb">284</div><div class="codeline">did not hinder the error-correcting learning, as shown in Fig. \ref{fig-exc-inh-split}.</div><div class="clear"></div>
<div class="linenb">285</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">286</div><div class="codeline">These results are useful, as they enable a biologically plausible way for excitatory long-range pyramidal projections to</div><div class="clear"></div>
<div class="linenb">287</div><div class="codeline">connect to pyramidal neurons in another layer of the network (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>in a different part of the cortex). The steps</div><div class="clear"></div>
<div class="linenb">288</div><div class="codeline">required to facilitate this type of network are rather simple; A pyramidal neuron projection could enter a distant</div><div class="clear"></div>
<div class="linenb">289</div><div class="codeline">cortical area and spread its <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [atonal] (16095) [lt:en:MORFOLOGIK_RULE_EN_US]">axonal</span> tree \phrasing within a layer that contains both pyramidal- and inhibitory</div><div class="clear"></div>
<div class="linenb">290</div><div class="codeline">interneuron dendrites. If these interneurons themselves connect to the local pyramidal population, Dendritic errors with</div><div class="clear"></div>
<div class="linenb">291</div><div class="codeline">arbitrary signs and magnitudes could be minimized.</div><div class="clear"></div>
<div class="linenb">292</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">293</div><div class="codeline">While error minimization is important, it does not necessarily imply that synaptic credit assignment is successful</div><div class="clear"></div>
<div class="linenb">294</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [as well, swell, Aspell, a swell] (16465) [lt:en:MORFOLOGIK_RULE_EN_US]">aswell</span>. Numerous weight configurations are <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conceivable] (16508) [lt:en:MORFOLOGIK_RULE_EN_US]">concievable</span> which could silence dendritic errors, but likely only a small</div><div class="clear"></div>
<div class="linenb">295</div><div class="codeline">subset of them is capable of transmitting useful information. To prove that this nonspecific connectivity is compatible</div><div class="clear"></div>
<div class="linenb">296</div><div class="codeline">with learning of complex tasks, it was introduced into the dendritic error network. The connection between Interneurons</div><div class="clear"></div>
<div class="linenb">297</div><div class="codeline">and Pyramidal neuron apical dendrites was chosen for the first test, as the employed plasticity rule had proven most</div><div class="clear"></div>
<div class="linenb">298</div><div class="codeline">resilient to parameter imperfections previously. A network of rate neurons was set up and parametrized as described in</div><div class="clear"></div>
<div class="linenb">299</div><div class="codeline">Section \ref{sec-le-tpres} ($t_{pres}= 50ms$). The Weights $w^{pi}$ were redrawn and restricted to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [positive, postie] (17118) [lt:en:MORFOLOGIK_RULE_EN_US]">postive</span> values, and a</div><div class="clear"></div>
<div class="linenb">300</div><div class="codeline">secondary inhibitory interneuron population was created and fully connected to both populations as described in Fig.</div><div class="clear"></div>
<div class="linenb">301</div><div class="codeline">\ref{fig-exc-inh-split}. The inhibitory interneuron population was <span class="highlight" title="This word has been used in one of the immediately preceding sentences. Using a synonym could make your text more interesting to read, unless the repetition is intentional.. Suggestions: [decided, selected, picked] (17302) [lt:en:EN_REPEATEDWORDS]">chosen</span> to be 4 times as large as the target pyramidal</div><div class="clear"></div>
<div class="linenb">302</div><div class="codeline">population, and $30\%$ of incoming excitatory connections were randomly deleted. The idea behind this was, to seed</div><div class="clear"></div>
<div class="linenb">303</div><div class="codeline">interneurons which were to serve as inhibitory counterparts for individual excitatory partners. From this seeding, the</div><div class="clear"></div>
<div class="linenb">304</div><div class="codeline">dendritic error rule could then ideally derive useful information about presynaptic activity.</div><div class="clear"></div>
<div class="linenb">305</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">306</div><div class="codeline">The experiment was successful, as the network was able to learn successfully in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [competitive] (17760) [lt:en:MORFOLOGIK_RULE_EN_US]">competetive</span> time (100\% accuracy after</div><div class="clear"></div>
<div class="linenb">307</div><div class="codeline">200 Epochs) albeit to a higher final test loss (results not shown). These results show that the dendritic plasticity</div><div class="clear"></div>
<div class="linenb">308</div><div class="codeline">rule is capable of correctly assigning credit to two separate populations under much less sanitized inputs. Further</div><div class="clear"></div>
<div class="linenb">309</div><div class="codeline">experiments are required to show <span class="keyword1">\textbf</span>{A:} how large such an inhibitory interneuron population needs to be, and what</div><div class="clear"></div>
<div class="linenb">310</div><div class="codeline">role the dropout has to play, <span class="keyword1">\textbf</span>{B:} whether this capability extends to the spiking implementation and <span class="keyword1">\textbf</span>{C:}</div><div class="clear"></div>
<div class="linenb">311</div><div class="codeline">if all neuron populations in the network can be connected in this way to separate excitatory and inhibitory pathways.</div><div class="clear"></div>
<div class="linenb">312</div><div class="codeline">Such experiments would allow for a closer investigation into how well the dendritic error network corresponds to</div><div class="clear"></div>
<div class="linenb">313</div><div class="codeline">cortical connectivity - if at all. Furthermore, the added interneuron populations would themselves have to have some</div><div class="clear"></div>
<div class="linenb">314</div><div class="codeline">cortical equivalent which they are to represent.</div><div class="clear"></div>
<div class="linenb">315</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">316</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">317</div><div class="codeline"><span class="keyword1">\subsection</span>{Interneuron nudging}</div><div class="clear"></div>
<div class="linenb">318</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">319</div><div class="codeline">An easily overlooked connection of this network is the nudging signal from pyramidal neurons to their interneuron</div><div class="clear"></div>
<div class="linenb">320</div><div class="codeline">sisters. These were deliberately not included in the previous dropout studies. If any interneuron was to not receive its</div><div class="clear"></div>
<div class="linenb">321</div><div class="codeline">nudging signal, its incoming synapses would be unable to adapt their weights. As a result, both interneuron- and</div><div class="clear"></div>
<div class="linenb">322</div><div class="codeline">FF error would fail to converge, in turn impeding apical error reduction. These one-to-one connections</div><div class="clear"></div>
<div class="linenb">323</div><div class="codeline">can  therefore be considered the most important communication channels in the network. If there is no redundancy in the</div><div class="clear"></div>
<div class="linenb">324</div><div class="codeline">neurons, the deletion <span class="highlight" title="Consider simply using 'of' instead.. Suggestions: [of] (19257) [lt:en:OF_ANY_OF]">of any of</span> them breaks the network's learning scheme. Sacramento <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>claim that the interneurons</div><div class="clear"></div>
<div class="linenb">325</div><div class="codeline">of the network resemble somatostatin-expressing (<span class="keyword1">\textit</span>{SST}) neurons. This is a reasonable assumption, as SST cells</div><div class="clear"></div>
<div class="linenb">326</div><div class="codeline">are <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [ubiquitous] (19469) [lt:en:MORFOLOGIK_RULE_EN_US]">ubiqutous</span> in the cortex and inhabit the same layers as pyramidal neurons. Furthermore, they share dense and</div><div class="clear"></div>
<div class="linenb">327</div><div class="codeline">recurrent synaptic connections to these pyramidal neurons \citep{urban2016somatostatin}. Finally, they have been shown</div><div class="clear"></div>
<div class="linenb">328</div><div class="codeline">to receive top-down instructive signals, which have been hypothesized to transmit prediction errors</div><div class="clear"></div>
<div class="linenb">329</div><div class="codeline">\citep{Leinweber2017}.</div><div class="clear"></div>
<div class="linenb">330</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">331</div><div class="codeline">Several experiments similar to those on synaptic polarity were conducted in an attempt to replace these one-to-one</div><div class="clear"></div>
<div class="linenb">332</div><div class="codeline">connections with more plausible connectivity schemes. Regrettably, none of them were able to retain the learning</div><div class="clear"></div>
<div class="linenb">333</div><div class="codeline">capability of this network. Thus, these connections remain as perhaps the biologically most implausible aspect of the</div><div class="clear"></div>
<div class="linenb">334</div><div class="codeline">dendritic error network. Further work is required to investigate if and how this constraint can be relaxed.</div><div class="clear"></div>
<div class="linenb">335</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">336</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">337</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">338</div><div class="codeline"><span class="keyword1">\section</span>{Performance of the different implementations}<span class="keyword1">\label</span>{sec-benchmark}</div><div class="clear"></div>
<div class="linenb">339</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">340</div><div class="codeline">As stated in \citep{Haider2021}, simulating large dendritic error networks with the full leaky dynamics quickly becomes</div><div class="clear"></div>
<div class="linenb">341</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [unfeasible] (20384) [lt:en:MORFOLOGIK_RULE_EN_US]">unfeasable</span>. While the NEST simulator can be regarded as rather fast \citep{albada2018performance}, simulations on it by</div><div class="clear"></div>
<div class="linenb">342</div><div class="codeline">design cannot employ batched matrix multiplication, as is typical in machine learning. Thus, by computing neuron updates</div><div class="clear"></div>
<div class="linenb">343</div><div class="codeline">individually even in highly structured networks like this one, NEST was expected to perform worse than previous</div><div class="clear"></div>
<div class="linenb">344</div><div class="codeline">implementations using PyTorch and dedicated GPUs. Yet not only did the NEST implementations compute rather slowly, the</div><div class="clear"></div>
<div class="linenb">345</div><div class="codeline">spiking variant performed <span class="highlight" title="A determiner may be missing.. Suggestions: [the worst] (20856) [lt:en:THE_SUPERLATIVE]">worst</span> across the board. To investigate the extent of this, as well as possible causes, several</div><div class="clear"></div>
<div class="linenb">346</div><div class="codeline">benchmark experiments were performed. These tests were run on an <span class="keyword1">\textit</span>{AMD Ryzen <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Thread ripper] (21026) [lt:en:MORFOLOGIK_RULE_EN_US]">Threadripper</span> 2990WX} using 8 cores at</div><div class="clear"></div>
<div class="linenb">347</div><div class="codeline">up to $3.0GHz$. All reported simulation times $t_{sim}$ are averaged over $5$ independent runs, and only measure the</div><div class="clear"></div>
<div class="linenb">348</div><div class="codeline">time taken simulating without considering network initialization. \newline</div><div class="clear"></div>
<div class="linenb">349</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">350</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">351</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">352</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">353</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.85\textwidth]{fig_benchmark_n_hidden}</div><div class="clear"></div>
<div class="linenb">354</div><div class="codeline">    <span class="keyword1">\caption</span>[Benchmark of the three implementations under different network sizes]{Benchmark of the three</div><div class="clear"></div>
<div class="linenb">355</div><div class="codeline">        implementations under different network sizes. Networks of $[9, n_{hidden}, 3]$ neurons per layer  were</div><div class="clear"></div>
<div class="linenb">356</div><div class="codeline">        instantiated with the same synaptic weights and trained for a single epoch of 10 stimulus presentations of</div><div class="clear"></div>
<div class="linenb">357</div><div class="codeline">        $50ms$ each. $n_{hidden}=30$ was chosen as a baseline, as it is the default throughout all simulations on the</div><div class="clear"></div>
<div class="linenb">358</div><div class="codeline">        Bars dataset.}</div><div class="clear"></div>
<div class="linenb">359</div><div class="codeline">    <span class="keyword1">\label</span>{fig-benchmark-n-hidden}</div><div class="clear"></div>
<div class="linenb">360</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">361</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">362</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">363</div><div class="codeline">To compare how network size affects simulation time, all three implementations created for this project were trained on</div><div class="clear"></div>
<div class="linenb">364</div><div class="codeline">10 examples of the <span class="highlight" title="An apostrophe may be missing.. Suggestions: [bars', bar's] (21386) [lt:en:POSSESSIVE_APOSTROPHE]">bars</span> dataset with different numbers of hidden layer pyramidal neurons. The result of this comparison</div><div class="clear"></div>
<div class="linenb">365</div><div class="codeline">is shown in Fig. \ref{fig-benchmark-n-hidden}.  The NEST implementation using rate neurons performed best in terms of</div><div class="clear"></div>
<div class="linenb">366</div><div class="codeline">speed across the board. This result was slightly surprising, as the demand on the communication interface between</div><div class="clear"></div>
<div class="linenb">367</div><div class="codeline">threads is very high, since all neurons transmit an event to each of their postsynaptic targets at every time step.</div><div class="clear"></div>
<div class="linenb">368</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">369</div><div class="codeline">The NumPy variant is an outlier, and only listed here for completeness. It is the only variant running on a single</div><div class="clear"></div>
<div class="linenb">370</div><div class="codeline">thread due to a limitation of NumPy. This could feasibly be improved greatly by using batched matrix multiplications, as</div><div class="clear"></div>
<div class="linenb">371</div><div class="codeline">are provided for example by \texttt{PyTorch}. The original implementations do this, but for practical reasons the</div><div class="clear"></div>
<div class="linenb">372</div><div class="codeline">Backend was changed here. Notably, this variant exhibits very little slowdown in response to an increase in network</div><div class="clear"></div>
<div class="linenb">373</div><div class="codeline">size. It seems, that the vectorization of updates on a single thread scales better with network size than the</div><div class="clear"></div>
<div class="linenb">374</div><div class="codeline">event-based communication performed by NEST.</div><div class="clear"></div>
<div class="linenb">375</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">376</div><div class="codeline">Not only is the spiking variant of this model slower than the rate version, it also scales worse with network size.</div><div class="clear"></div>
<div class="linenb">377</div><div class="codeline">Simulation time between $100$ and $250$ hidden layer neurons doubled, compared to an increase of $1.6$ for the rate</div><div class="clear"></div>
<div class="linenb">378</div><div class="codeline">network. The Difference between the two was even greater when simulating on an office-grade processor (<span class="keyword1">\textit</span>{Intel</div><div class="clear"></div>
<div class="linenb">379</div><div class="codeline">    Core i5-9300H} @ $2.40GHz$, results not shown). Several insights about the comparatively poor performance can be deduced</div><div class="clear"></div>
<div class="linenb">380</div><div class="codeline">from a first approximation: The most likely causes for increased compute speed are the communication of events and the</div><div class="clear"></div>
<div class="linenb">381</div><div class="codeline">synaptic plasticity rules. Updates to the neuron state are unlikely to be responsible for the worse performance, as both</div><div class="clear"></div>
<div class="linenb">382</div><div class="codeline">neuron models are modelled almost identically. These assumptions were tested experimentally.</div><div class="clear"></div>
<div class="linenb">383</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">384</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">385</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">386</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">387</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">388</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.7\textwidth]{fig_benchmark_plasticity}</div><div class="clear"></div>
<div class="linenb">389</div><div class="codeline">    <span class="keyword1">\caption</span>[Benchmark of both NEST implementations with plastic and non-plastic synapse types]{Benchmark of both NEST</div><div class="clear"></div>
<div class="linenb">390</div><div class="codeline">        implementations with plastic and non-plastic synapse types. Deep networks of $300-200-100-10$ pyramidal neurons</div><div class="clear"></div>
<div class="linenb">391</div><div class="codeline">        per layer were stimulated with 5 samples of random input $\in\{0,1\}$ for $10ms$ each. synaptic weights were</div><div class="clear"></div>
<div class="linenb">392</div><div class="codeline">        initialized between $\{-0.1, 0.1 \}$ to avoid overstimulation of individual neurons. In the plastic paradigm,</div><div class="clear"></div>
<div class="linenb">393</div><div class="codeline">        all synapses except for feedback weights $w^{down}$ were plastic with very low learning rates $\eta =</div><div class="clear"></div>
<div class="linenb">394</div><div class="codeline">            10^{-10}$.}</div><div class="clear"></div>
<div class="linenb">395</div><div class="codeline">    <span class="keyword1">\label</span>{fig-benchmark-plasticity}</div><div class="clear"></div>
<div class="linenb">396</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">397</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">398</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">399</div><div class="codeline">To assess the impact of synaptic updates on computation time, both variants were simulated once with plastic, and once</div><div class="clear"></div>
<div class="linenb">400</div><div class="codeline">with static synapses. The simulation environment is set up to model synaptic populations with zero-valued learning rates</div><div class="clear"></div>
<div class="linenb">401</div><div class="codeline">as non-plastic synapses (\texttt{static\_synapse} and \texttt{rate\_connection\_delayed} respectively). Thus, by setting</div><div class="clear"></div>
<div class="linenb">402</div><div class="codeline">learning rates to zero, it was possible to simulate an entire network without spending any time on synaptic updates.</div><div class="clear"></div>
<div class="linenb">403</div><div class="codeline">Results of this experiment are shown in Fig. \ref{fig-benchmark-plasticity}.</div><div class="clear"></div>
<div class="linenb">404</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">405</div><div class="codeline">As expected, synaptic updates in the spiking network are responsible for a much larger proportion of total simulation</div><div class="clear"></div>
<div class="linenb">406</div><div class="codeline">time than in the rate network. A much less anticipated result was that spiking networks are considerably slower even</div><div class="clear"></div>
<div class="linenb">407</div><div class="codeline">when plasticity is turned off. This is surprising, as neuron models are almost identical <span class="highlight" title="Consider using 'except' or 'except for'. Suggestions: [except, except for] (24050) [lt:en:WITH_THE_EXCEPTION_OF]">with the exception of</span> some</div><div class="clear"></div>
<div class="linenb">408</div><div class="codeline">added complexity in the spike generation process. This added complexity includes drawing from a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Poisson, poison, prison, piston, poisons, caisson, Poussin, frisson, Alisson, Olsson] (24173) [lt:en:MORFOLOGIK_RULE_EN_US]">poisson</span> process, which</div><div class="clear"></div>
<div class="linenb">409</div><div class="codeline">might be time-costly depending on the underlying implementation. Another possible reason might be added complexity</div><div class="clear"></div>
<div class="linenb">410</div><div class="codeline">associated with <span class="highlight-spelling" title="Possible spelling mistake found. (24327) [lt:en:MORFOLOGIK_RULE_EN_US]">SpikeEvents</span> in general, which update some postsynaptic variables not employed for this model. Further</div><div class="clear"></div>
<div class="linenb">411</div><div class="codeline">work is required to more rigorously determine the reasons for this poor performance.\newline</div><div class="clear"></div>
<div class="linenb">412</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">413</div><div class="codeline">\noindent To investigate the degree to which synaptic plasticity and spike transmission in general contribute to</div><div class="clear"></div>
<div class="linenb">414</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Computational] (24524) [lt:en:UPPERCASE_SENTENCE_START]">computational</span> cost, two more experiments were conducted. Training durations under different values for the scaling</div><div class="clear"></div>
<div class="linenb">415</div><div class="codeline">parameter $\psi$, as well as with different numbers of threads were recorded. Results are shown in Fig.</div><div class="clear"></div>
<div class="linenb">416</div><div class="codeline">\ref{fig-benchmark-threads-psi}.</div><div class="clear"></div>
<div class="linenb">417</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">418</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">419</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">420</div><div class="codeline">    <span class="keyword2">\begin{minipage}</span>{0.5\textwidth}</div><div class="clear"></div>
<div class="linenb">421</div><div class="codeline">        <span class="keyword1">\textbf</span>{a)}\par\medskip</div><div class="clear"></div>
<div class="linenb">422</div><div class="codeline">        \centering</div><div class="clear"></div>
<div class="linenb">423</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_benchmark_threads}</div><div class="clear"></div>
<div class="linenb">424</div><div class="codeline">    <span class="keyword2">\end{minipage}</span>\hfill</div><div class="clear"></div>
<div class="linenb">425</div><div class="codeline">    <span class="keyword2">\begin{minipage}</span>{0.5\textwidth}</div><div class="clear"></div>
<div class="linenb">426</div><div class="codeline">        <span class="keyword1">\textbf</span>{b)}\par\medskip</div><div class="clear"></div>
<div class="linenb">427</div><div class="codeline">        \centering</div><div class="clear"></div>
<div class="linenb">428</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_benchmark_psi}</div><div class="clear"></div>
<div class="linenb">429</div><div class="codeline">    <span class="keyword2">\end{minipage}</span></div><div class="clear"></div>
<div class="linenb">430</div><div class="codeline">    <span class="keyword1">\caption</span>[Benchmarks for the spiking implementation]{Benchmarks for the spiking implementation. <span class="keyword1">\textbf</span>{a)}</div><div class="clear"></div>
<div class="linenb">431</div><div class="codeline">        Simulation on the MNIST dataset for a network of $784-300-100-10$ neurons and $\psi=250$ on different numbers of</div><div class="clear"></div>
<div class="linenb">432</div><div class="codeline">        threads. 10 samples were presented for $50ms$ each, and weights were drawn randomly from a uniform distribution</div><div class="clear"></div>
<div class="linenb">433</div><div class="codeline">        $ \{-0.1, 0.1\}$. <span class="keyword1">\textbf</span>{b)} Training of a default network on the Bars dataset using different values of</div><div class="clear"></div>
<div class="linenb">434</div><div class="codeline">        the scaling parameter $\psi$. All simulations use 8 threads.}</div><div class="clear"></div>
<div class="linenb">435</div><div class="codeline">    <span class="keyword1">\label</span>{fig-benchmark-threads-psi}</div><div class="clear"></div>
<div class="linenb">436</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">437</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">438</div><div class="codeline">Simulating a large network on an increasing number of threads highlights the diminishing returns gained by spreading out</div><div class="clear"></div>
<div class="linenb">439</div><div class="codeline">the simulation of NEST neurons. While initial speedup is high, at some point the benefit of parallelizing neuron updates</div><div class="clear"></div>
<div class="linenb">440</div><div class="codeline">is counteracted by the need to communicate more events across threads. It is to be expected that for even higher</div><div class="clear"></div>
<div class="linenb">441</div><div class="codeline">parallelization simulation time will begin to increase for this network size.</div><div class="clear"></div>
<div class="linenb">442</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">443</div><div class="codeline">The second figure shows that reducing $\psi$ much lower will likewise lead to diminishing returns. On the other hand,</div><div class="clear"></div>
<div class="linenb">444</div><div class="codeline">increasing it in the hopes of improving learning performance comes at a stark cost to simulation time. <span class="highlight" title="The demonstrative 'This' may not agree with the plural noun 'results'. Did you mean 'these'?. Suggestions: [These] (25393) [lt:en:THIS_NNS]">This</span> results</div><div class="clear"></div>
<div class="linenb">445</div><div class="codeline">should inform future experiments on increasing efficiency through parametrization.</div><div class="clear"></div>
<div class="linenb">446</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">447</div><div class="codeline"><span class="keyword1">\section</span>{Pre-training}</div><div class="clear"></div>
<div class="linenb">448</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">449</div><div class="codeline">One of the two major criticisms of the network noted in \citep{whittington2019theories} is the requirement for</div><div class="clear"></div>
<div class="linenb">450</div><div class="codeline">pre-training (cf. Supplementary Table \ref{tab-wb-models}). By this, the authors mean the initialization to the self-predicting</div><div class="clear"></div>
<div class="linenb">451</div><div class="codeline">state from which most simulations are started. The original paper implicitly considers three different learning</div><div class="clear"></div>
<div class="linenb">452</div><div class="codeline">configurations: In the first one, the network starts from the self-predicting state and only feedforward weights are</div><div class="clear"></div>
<div class="linenb">453</div><div class="codeline">plastic (cf. Sec. \ref{sec-le-tpres}). In the second one, the network starts from random weights and <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [into, ninth, hints, inn, linen, mint, Finn, hint, mints, pint, tint, Hinton, Linton, pints, tints, Jinan, jinn, lint, pinto, Minn, minty, pinon, linty, INT, INTG, dint, int, lints, INTJ, ITN, Mint, Minton, Sinan, Tintin, vint, vints] (26010) [lt:en:MORFOLOGIK_RULE_EN_US]">$Intn</span> \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (26017) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>rightarrow</div><div class="clear"></div>
<div class="linenb">454</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Proper, Syrupy] (26032) [lt:en:MORFOLOGIK_RULE_EN_US]">Pyr$ synapses are plastic, so they can minimize FB error. The third variant, in which feedback $Pyr</span> \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [right arrow] (26041) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>rightarrow</div><div class="clear"></div>
<div class="linenb">455</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Pyre] (26056) [lt:en:MORFOLOGIK_RULE_EN_US]">Pyr$</span> weights are plastic, is not considered here. An experiment was conducted comparing performance of the first two</div><div class="clear"></div>
<div class="linenb">456</div><div class="codeline">variants while training on the Bars dataset. <span class="highlight" title="Did you mean 'these'?. Suggestions: [These] (26218) [lt:en:THIS_NNS_VB]">This</span> experiments showed that training was marginally slower, but led to</div><div class="clear"></div>
<div class="linenb">457</div><div class="codeline">identical loss (results not shown). While initializing the network to a self-predicting state does give it a slight</div><div class="clear"></div>
<div class="linenb">458</div><div class="codeline">'head-start', this is by no means a condition for learning.</div><div class="clear"></div>
<div class="linenb">459</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">460</div><div class="codeline">Furthermore, training the network towards the self-predicting state does not require any kind of structured input, let</div><div class="clear"></div>
<div class="linenb">461</div><div class="codeline">alone targets for activation. The network is driven towards this state purely by noise injection at the input layer. As</div><div class="clear"></div>
<div class="linenb">462</div><div class="codeline">background noise is trivial to generate (perhaps unavoidable) for any cortical circuit, the self-predicting state might</div><div class="clear"></div>
<div class="linenb">463</div><div class="codeline">be the default rather than the exception. For these reasons, I do not consider pre-training to be an issue that</div><div class="clear"></div>
<div class="linenb">464</div><div class="codeline">interferes with the model's biological plausibility.</div><div class="clear"></div>
<div class="linenb">465</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">466</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">467</div><div class="codeline"><span class="keyword1">\section</span>{Behavioral timescale learning}</div><div class="clear"></div>
<div class="linenb">468</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">469</div><div class="codeline">As a final experiment, the extent to which the network can handle learning on biological timescales was investigated.</div><div class="clear"></div>
<div class="linenb">470</div><div class="codeline">One <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [criticism] (27146) [lt:en:MORFOLOGIK_RULE_EN_US]">critcism</span> occasionally aimed at <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (27177) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> is the requirement for instructive signals to be available immediately</div><div class="clear"></div>
<div class="linenb">471</div><div class="codeline">\citep{Bartunov2018}. The assumption is, that an agent in the real world <span class="highlight" title="Possible typo: you repeated a word. Suggestions: [would] (27313) [lt:en:ENGLISH_WORD_REPEAT_RULE]">would would</span> select an action, and be informed</div><div class="clear"></div>
<div class="linenb">472</div><div class="codeline">about the consequences only after some delay. Learning algorithms should therefore be capable of handling delayed</div><div class="clear"></div>
<div class="linenb">473</div><div class="codeline">instructive signals.</div><div class="clear"></div>
<div class="linenb">474</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">475</div><div class="codeline">Furthermore, all membrane potentials and synaptic weight derivatives of the dendritic error network are reset after each</div><div class="clear"></div>
<div class="linenb">476</div><div class="codeline">stimulus presentation. This procedure ensures that residuals from the previous run do not interfere with learning of a</div><div class="clear"></div>
<div class="linenb">477</div><div class="codeline">subsequent stimulus. It was confirmed experimentally that networks fail to learn when this reset is not performed after</div><div class="clear"></div>
<div class="linenb">478</div><div class="codeline">every training sample (results not shown).</div><div class="clear"></div>
<div class="linenb">479</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">480</div><div class="codeline">Two additions were made to the model to confirm that it is capable of learning without these constraints. First, the</div><div class="clear"></div>
<div class="linenb">481</div><div class="codeline">target activation was delayed to be first injected $5ms$ after the stimulus. This serves to ensure that  that learning</div><div class="clear"></div>
<div class="linenb">482</div><div class="codeline">does not rely on simultaneous presentation of stimulus and target. Secondly, instead of manually resetting membrane</div><div class="clear"></div>
<div class="linenb">483</div><div class="codeline">potentials, the network was allowed to relax after each training sample. During this relaxation period  (termed</div><div class="clear"></div>
<div class="linenb">484</div><div class="codeline"><span class="keyword1">\textit</span>{soft reset}), the network is simulated for $15ms$ without any current injections. A training comparison between</div><div class="clear"></div>
<div class="linenb">485</div><div class="codeline">a vanilla network and these two additions is shown in Fig. \ref{fig-idle-time}.</div><div class="clear"></div>
<div class="linenb">486</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">487</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">488</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h]</div><div class="clear"></div>
<div class="linenb">489</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">490</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_idle_time}</div><div class="clear"></div>
<div class="linenb">491</div><div class="codeline">    <span class="keyword1">\caption</span>[Comparison of learning under minimal external control]{Comparison of learning under minimal external</div><div class="clear"></div>
<div class="linenb">492</div><div class="codeline">        control. <span class="keyword1">\textbf</span>{Blue:} default parametrization for the Bars dataset. <span class="keyword1">\textbf</span>{Orange:} during the first $5ms$ of</div><div class="clear"></div>
<div class="linenb">493</div><div class="codeline">        a training pattern, no target is provided. Afterwards training continues as usual for the remaining $45ms$.</div><div class="clear"></div>
<div class="linenb">494</div><div class="codeline">        <span class="keyword1">\textbf</span>{Green:} Additionally, the network is not manually reset after each training sample, but simulated for</div><div class="clear"></div>
<div class="linenb">495</div><div class="codeline">        another $15ms$ without stimulation. Increased loss of the delayed target paradigms might be explained by the</div><div class="clear"></div>
<div class="linenb">496</div><div class="codeline">        shorter effective training time per stimulus.}</div><div class="clear"></div>
<div class="linenb">497</div><div class="codeline">    <span class="keyword1">\label</span>{fig-idle-time}</div><div class="clear"></div>
<div class="linenb">498</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">499</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">500</div><div class="codeline">All paradigms lead to equally fast learning of the Bars dataset, with delayed target presentation causing a slightly</div><div class="clear"></div>
<div class="linenb">501</div><div class="codeline">higher test loss. These results show that constraints like this have only miniscule impact on learning performance of</div><div class="clear"></div>
<div class="linenb">502</div><div class="codeline">the dendritic error network. Thus, a cortical network of this kind can be expected to be indifferent to idle time in</div><div class="clear"></div>
<div class="linenb">503</div><div class="codeline">which it is only driven by white noise. Likewise, incoming sensory information in the self-predicting state does not</div><div class="clear"></div>
<div class="linenb">504</div><div class="codeline">cause plasticity which would drive weights away from what was previously learned. Only when a target is presented to the</div><div class="clear"></div>
<div class="linenb">505</div><div class="codeline">output layer will weights adapt. This insight shows that the network requires even less external control, which might be</div><div class="clear"></div>
<div class="linenb">506</div><div class="codeline">of use for improving its efficiency \todo{ref outlook}. More importantly, with the need to manually reset membrane</div><div class="clear"></div>
<div class="linenb">507</div><div class="codeline">potentials, another biologically implausible mechanism can be omitted from simulations of this network.</div><div class="clear"></div>
<div class="linenb">508</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">509</div><div class="codeline">It should be noted that this experiment makes the assumption that the brain is either capable of retaining an input</div><div class="clear"></div>
<div class="linenb">510</div><div class="codeline">sequence until feedback is available, or otherwise 'replay' the pattern at a later point. While such mechanisms are much</div><div class="clear"></div>
<div class="linenb">511</div><div class="codeline">less elegant than trace-based solutions for delayed reward signalling \citep{bellec2020solution}, the brain has been</div><div class="clear"></div>
<div class="linenb">512</div><div class="codeline">found capable of such replays \citep{Marblestone2016}.</div><div class="clear"></div>
</div>
<h2 class="filename">05_appendix.tex</h2>

<p>Found 28 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Appendix}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{<span class="highlight-spelling" title="Possible spelling mistake found. (13) [lt:en:MORFOLOGIK_RULE_EN_US]">Somato-dendritic</span> coupling}<span class="keyword1">\label</span>{sec-somato-dendr}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">\citep{urbanczik2014learning} discuss a possible extension to their neuron- and plasticity model, in which the</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (125) [lt:en:MORFOLOGIK_RULE_EN_US]">dendro-somatic</span> coupling transmits voltages in both directions. They show that the plasticity rule requires only minor</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">adaptations for successful learning under this paradigm. Yet, as described by passive cable theory, the flow between</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">neuronal compartments is dictated by their respective membrane <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [capacitance] (423) [lt:en:MORFOLOGIK_RULE_EN_US]">capacitances</span>. These are calculated from their membrane</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">areas, which vastly differ in the case of pyramidal neurons. \todo{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Find] (539) [lt:en:UPPERCASE_SENTENCE_START]">find</span> a nice citation for this}</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">15,006 458</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Will] (584) [lt:en:UPPERCASE_SENTENCE_START]">will</span> not be considered here. The motivation is, that dendritic membrane area is</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline"><span class="keyword1">\section</span>{Default parameters}</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{Integration of the spike-based <span class="highlight-spelling" title="Possible spelling mistake found. (718) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity}</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">Starting with the complete Integral from $t=0$.</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">  \dot{W_{ij}}(t)    &amp; = \eta (\phi(u_i) - \phi(\alpha v^{basal}_i(t))) \phi(u_j)                                                     \\</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \int_t^T dt' \ \eta \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \  \phi(u_j^{t'})                         \\</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \eta \int_t^T dt' \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \ \phi(u_j^{t'})                            \\</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">  V_i^*              &amp; = \phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})                                                                    \\</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">  s_j^*              &amp; = \kappa_s * s_j                                                                                               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">  \Delta W_{ij}(0,t) &amp; =\eta \int_0^t dt' \  \int_0^{t'} dt'' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'')                          \\</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">                     &amp; = \eta \int_0^t dt'' \  \int_{t''}^{t} dt' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'')                      \\</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">                     &amp; = \eta \int_0^t dt'' \  \left[ \tilde{\kappa}(t-t'') - \tilde{\kappa}(0) \right] V_i^\ast (t'') s_j^\ast (t'') \\</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">With $\tilde{\kappa}$ being the antiderivative of $\kappa$:</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">  \kappa(t)         &amp; = \frac{\delta}{\delta t} \tilde{\kappa}(t) \\</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">  \tilde{\kappa}(t) &amp; = - e^{-\frac{t}{t_{\kappa}}}               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">The above can be split up into two separate integrals:</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">Which implies the identities</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">  I_1(t_1, t_2 + \Delta t) &amp; = I_1 (t_1, t_2) + I_1 (t_2, t_2 + \Delta t)                                       \\</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">  I_2(t_1, t_2 + \Delta t) &amp; = e^{- \frac{t_2 - t_1}{\tau_{\kappa}}} I_2 (t_1, t_2) + I_2 (t_2, t_2 + \Delta t)</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">  I_2 (t_1, t_2 + \Delta t) &amp; = -\int_{t_1}^{t_2 + \Delta t} dt' \ \tilde{\kappa} (t_2 + \Delta t - t') V_i^\ast (t') s_j^\ast (t')                                        \\</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">                            &amp; = -\int_{t_1}^{t_2} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">  -\int_{t_2}^{t_2 + \Delta t} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')                                             \\</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">                            &amp; = -e^{- \frac{ \Delta t}{\tau_\kappa}} \int_{t_1}^{t_2} dt' \ \left[ -e^{- \frac{t_2 - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">  -\int_{t_2}^{t_2 + \Delta t} dt' \ \left[ -e^{- \frac{t_2 + \Delta t - t'}{\tau_\kappa}} \right] V_i^\ast (t') s_j^\ast (t')</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">Using this we can rewrite the weight change from $t$ to $T$ as:</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \Delta W_{ij}(0,T) - \Delta W_{ij}(0,t)                                               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">                     &amp; = \eta [-I_2(0,T) + I_1(0,T) + I_2(0,t) - I_1(0,t)]                                     \\</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">                     &amp; = \eta [I_1(t,T) - I_2(t,T) + I_2(0,t)\left( 1 - e^{- \frac{T-t}{\tau_\kappa}} \right)]</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">The simplified \citep{sacramento2018dendritic} case would be:</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">  \frac{dW_{ij}}{dt} &amp; = \eta (\phi(u_i) - \phi(\hat{v_i})) \phi(u_j)                                         \\</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \int_t^T dt' \ \eta \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \  \phi(u_j^{t'}) \\</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">  \Delta W_{ij}(t,T) &amp; = \eta \int_t^T dt' \  (\phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})) \ \phi(u_j^{t'})    \\</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">  V_i^*              &amp; = \phi(u_i^{t'}) - \phi(\widehat{v_i^{t'}})                                            \\</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">  s_j^*              &amp; = \kappa_s * s_j</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">Where $s_i$ is the postsynaptic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (1051) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> and $V_i^*$ is the error between dendritic prediction and somatic rate and</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">$h( u )$. The additional nonlinearity $h( u ) = \frac{d}{du} ln \  \phi(u)$ is <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [limited, committed, omitted, commuted, Olmsted, orbited, vomited, summited] (1167) [lt:en:MORFOLOGIK_RULE_EN_US]">ommited</span> in our model \todo{should it</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">  though?}.</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Antiderivative] (1213) [lt:en:MORFOLOGIK_RULE_EN_US]">Antiderivatives</span>:</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">  \int_{-\infty}^x H(t)dt = tH(t) = max(0,t)</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">  \tau_l &amp; = \frac{C_m}{g_L} = 10 \\</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">  \tau_s &amp; = 3</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">Writing membrane potential to history (happens at every update step of the postsynaptic neuron):</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline"><span class="keyword1">\section</span>{Dendritic leakage conductance}<span class="keyword1">\label</span>{sec-gl-dend}</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">In order to match the dendritic potential of rate neurons  in the spiking neuron model, a suitable leakage conductance</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">for dendritic compartments was required. As described in Equation \ref{eq-spiking-basal-compartment}, a dendritic</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">compartment evolves according to:</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">  C_m^{dend} \dot{v}_j^{dend} &amp; = -g_l^{dend} \  v_j^{dend} + \sum_i W_{ji} \    \langle <span class="keyword1">\textit</span>{n}_i \rangle</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">Under the assumption that the activation of all presynaptic neurons <span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (1668) [lt:en:I_LOWERCASE]">$i$</span> remains static over time, we can replace the</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">spontaneous activation $s_i(t)$ with the expected number of spikes per simulation step $\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [angle, Langley, Angle, tangle, dangle, jangle, mangle, bangle, wangle, l angle] (1797) [lt:en:MORFOLOGIK_RULE_EN_US]">langle</span> <span class="keyword1">\textit</span>{n}_i \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [range, angle, Angle, tangle, dangle, jangle, mangle, bangle, wrangle, rankle, wangle, Randle] (1809) [lt:en:MORFOLOGIK_RULE_EN_US]">rangle</span> =</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">  r_i \ \Delta <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [to, TT, TV, ta, ts, TD, T, TA, TB, TC, TF, TG, TH, TI, TJ, TK, TL, TM, TN, TO, TP, TR, TS, TU, TW, TX, TY, TZ, Ta, Tb, Tc, Te, Th, Ti, Tl, Tm, Tu, Ty, t, tb, ti, tn, tr, TQ] (1833) [lt:en:MORFOLOGIK_RULE_EN_US]">t$</span> (cf. Equation \ref{eq-n-spikes}). Note that these values do not employ matrix notation, but refer to</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">individual neurons. Next, in order to find the convergence point of the ODE, we set the left side of the equation to $0$</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">and to solve it:</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">  0                        &amp; = -g_l^{dend} \  v_j^{dend} + \sum_i W_{ji} \    r_i \ \Delta t \\</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">  g_l^{dend} \  v_j^{dend} &amp; = W_{ji} \    r_i \ \Delta t</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">The instantaneous dendritic potential of rate neurons is given by $v_j^{dend} = \sum_i W_{ji} \ r_i$. Since we are</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">searching for a parametrization which <span class="highlight-spelling" title="Possible spelling mistake. 'fulfils' is British English.. Suggestions: [fulfills] (2179) [lt:en:MORFOLOGIK_RULE_EN_US]">fulfils</span> this equality in the steady state, the terms drop out from the above</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">equation. Thus, the correct parametrization for the dendritic leakage conductance remains:</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">  g_l^{dend} &amp; = \Delta t</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">It was shown experimentally that for high values of $\psi$, this <span class="highlight" title="Do not mix variants of the same word ('parameterization' and 'parametrization') within a single text.. Suggestions: [parametrization] (2408) [lt:en:EN_WORD_COHERENCY]">parameterization</span> leads to an exact match of dendritic</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">potentials between the neuron models. It will therefore be assumed as the default throughout all experiments where</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">spiking neurons are used. \newline</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">In order to keep the two NEST models as similar as possible, rate neurons evolve according to the same dynamics. Like in</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">the original implementation, dendrites of rate neurons ought to be fully defined by their inputs at time $t$. This</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake. 'behaviour' is British English.. Suggestions: [behavior] (2847) [lt:en:MORFOLOGIK_RULE_EN_US]">behaviour</span> is achieved by setting the leakage conductance to $1$ for all dendritic compartments. During network</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">initialization, dendritic leakage <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (2990) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span> are set to either one of these values depending on the type of neuron</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">model employed.</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline"><span class="keyword1">\section</span>{Plasticity in feedback connections}<span class="keyword1">\label</span>{sec-feedback-plast}</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">Within the dendritic error model, Pyramidal-to-pyramidal feedback weights evolve according to:</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">  \dot{w}_{l}^{down} &amp; = \eta_l^{down} \ ( \phi(u_l^{P}) - \phi(w_l^{down} r_{l+1}^P) )\ \phi(u_{l+1}^{P})^T</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">The error term in this case differs slightly from the others, but could arguably still be implemented by biological</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">neurons. An intuitive way to interpret the error term is as the difference between somatic activity and the activity of</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">a distant apical compartment that is innervated only by superficial pyramidal neurons. The separation of pyramidal</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">neuron apical dendrites into a proximal and a distal tree is well documented \citep{Ishizuka1995}. Likewise, the fact</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">that these different apical compartments are innervated by separate presynaptic populations is well established</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">\citep{Larkum2018}. A difference between plasticity mechanisms for synapses arriving at these two integration zones is</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">certainly plausible, as vastly different <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [electrophysiologist, electrophysiologists] (3933) [lt:en:MORFOLOGIK_RULE_EN_US]">electrophysiological</span> properties have been measured between them</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">\citep{Ishizuka1995,Larkum2022}. A more sophisticated model of the apical tree and its plasticity could be a desirable</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline">extension to the model.</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">While the plasticity was successfully implemented in all variants of the model, it did not prove useful for training the</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">networks during initial tests. Making these feedback connections non-plastic led to the best learning performance, and</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">is therefore assumed as the default for all training simulations. This matches the previous implementations of this</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">network, which typically set learning rates of these connections to $0$ <span class="highlight" title="Consider using 'except' or 'except for'. Suggestions: [except, except for] (4539) [lt:en:WITH_THE_EXCEPTION_OF]">with the exception of</span> a few experiments</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">employing steady-state approximations. Note that under these conditions the network effectively implements a type of</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">Feedback alignment \citep{Lillicrap2014}. Further work is required to identify why plasticity in these synapses seemed</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">to break learning in the present model.</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline"><span class="highlight-sh" title="If you are writing a research paper, do not force page breaks. [sh:nonp]"></span>\newpage</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline"><span class="keyword1">\section</span>{Supplementary Figures}</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">\renewcommand{\thefigure}{S\arabic{figure}}</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h!]</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_3_numpy}</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">  <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>[Replication of Fig. \ref{fig-bars-le-snest</span>} using the NumPy network]{Replication of Fig.</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">    \ref{fig-bars-le-snest} using the NumPy network. This varaint is a slightly modified version of the python code from</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">    \citep{Haider2021}. Resulting performance matches the original results closely, showing that this version can serve</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">    as a baseline for comparing performance of the NEST implementation to the original results. Note, how in this</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">    implementation, presentation time has hardly any effect on the LE network because all updates are instantaneous. At</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">    the lower end presentation time is only limited by simulation timestep $\Delta t$.}</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">  <span class="keyword1">\label</span>{fig-bars-le-numpy}</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h!]</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_3_rnest}</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">  <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>[Replication of Fig. \ref{fig-bars-le-snest</span>} using networks of rate neurons in the NEST simulator]{Replication</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">    of Fig. \ref{fig-bars-le-snest} using networks of rate neurons in the NEST simulator. A notable difference to the</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">    python implementation in Fig. \ref{fig-bars-le-numpy} is, that this version does not handle very low presentation</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">    times as well. This can likely be traced back to the synaptic delay enforced by NEST, which imposes an upper bound</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">    on network relaxation time. Besides that, performance of the two variants is very similar.}</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">  <span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure fig-bars-le-rnest is never referenced in the text [sh:figref]">fig-bars-le-rnest</span>}</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [clear page] (4870) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>clearpage</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline"><span class="keyword1">\section</span>{Supplementary Tables}</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">\renewcommand{\thetable}{S\arabic{table}}</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">\arrayrulecolor{white} <span class="comment">% &lt;--- {\renewcommand{\arraystretch}{1.45} </span></div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline"><span class="keyword2">\begin{table}</span>[h]</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">  \resizebox{\textwidth}{!}{</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">    <span class="keyword2">\begin{tabular}</span>{|ll|ll|ll|} \hline \rowcolor[HTML]{B3B3B3} \multicolumn{2}{|l|}{\cellcolor[HTML]{B3B3B3}}      &amp;</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">               \multicolumn{2}{l|}{\cellcolor[HTML]{B3B3B3}Temporal-error</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline">               model}                                                                                          &amp;</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">               \multicolumn{2}{l|}{\cellcolor[HTML]{B3B3B3}Explicit-error</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">               model}                                                                                                           \\</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">               \cline{3-6}</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline">               \rowcolor[HTML]{B3B3B3}</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">               \multicolumn{2}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{B3B3B3}}}</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{B3B3B3}Contrastive</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">               learning}                                                                                       &amp;</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">               Continuous</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">               update</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{B3B3B3}Predictive</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">               coding}                                                                                         &amp;</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">               Dendritic error</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">               \\ \hline</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">               \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Control signal}            &amp;</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{FE0000}</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">               Required}}                                                                                      &amp;</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">               {\color[HTML]{FE0000}</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">                   Required}</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">               Not required}}                                                                                  &amp;</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline">               {\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline">                   Not required}</div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">               \\ \hline</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline">               \rowcolor[HTML]{D9D9D9}</div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">               \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Connectivity}</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline">                   Unconstrained}}</div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">252</div><div class="codeline">               {\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">253</div><div class="codeline">               Unconstrained}                                                                                  &amp;</div><div class="clear"></div>
<div class="linenb">254</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{FE0000}</div><div class="clear"></div>
<div class="linenb">255</div><div class="codeline">                   Constrained}}</div><div class="clear"></div>
<div class="linenb">256</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">257</div><div class="codeline">               {\color[HTML]{FE0000}</div><div class="clear"></div>
<div class="linenb">258</div><div class="codeline">               Constrained}                                                                                                     \\</div><div class="clear"></div>
<div class="linenb">259</div><div class="codeline">               \hline</div><div class="clear"></div>
<div class="linenb">260</div><div class="codeline">               \rowcolor[HTML]{D9D9D9}</div><div class="clear"></div>
<div class="linenb">261</div><div class="codeline">               \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Propagation</div><div class="clear"></div>
<div class="linenb">262</div><div class="codeline">               time}                                                                                           &amp;</div><div class="clear"></div>
<div class="linenb">263</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">264</div><div class="codeline">               L-1}}                                                                                           &amp;</div><div class="clear"></div>
<div class="linenb">265</div><div class="codeline">               {\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">266</div><div class="codeline">                   L-1}</div><div class="clear"></div>
<div class="linenb">267</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">268</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{FE0000}</div><div class="clear"></div>
<div class="linenb">269</div><div class="codeline">               2L-1}}                                                                                          &amp;</div><div class="clear"></div>
<div class="linenb">270</div><div class="codeline">               {\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">271</div><div class="codeline">                   L-1}</div><div class="clear"></div>
<div class="linenb">272</div><div class="codeline">               \\</div><div class="clear"></div>
<div class="linenb">273</div><div class="codeline">               \hline \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Pre-training}       &amp;</div><div class="clear"></div>
<div class="linenb">274</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">275</div><div class="codeline">               Not required}}                                                                                  &amp;</div><div class="clear"></div>
<div class="linenb">276</div><div class="codeline">               {\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">277</div><div class="codeline">               Not required}                                                                                   &amp;</div><div class="clear"></div>
<div class="linenb">278</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}{\color[HTML]{32CB00}</div><div class="clear"></div>
<div class="linenb">279</div><div class="codeline">                   Not required}}</div><div class="clear"></div>
<div class="linenb">280</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">281</div><div class="codeline">               {\color[HTML]{FE0000}</div><div class="clear"></div>
<div class="linenb">282</div><div class="codeline">               Required}                                                                                                        \\</div><div class="clear"></div>
<div class="linenb">283</div><div class="codeline">               \hline</div><div class="clear"></div>
<div class="linenb">284</div><div class="codeline">               \rowcolor[HTML]{D9D9D9}</div><div class="clear"></div>
<div class="linenb">285</div><div class="codeline">               \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Error</div><div class="clear"></div>
<div class="linenb">286</div><div class="codeline">               encoded in}                                                                                     &amp;</div><div class="clear"></div>
<div class="linenb">287</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Difference</div><div class="clear"></div>
<div class="linenb">288</div><div class="codeline">                                                                   in activity \\</div><div class="clear"></div>
<div class="linenb">289</div><div class="codeline">                                                                   between</div><div class="clear"></div>
<div class="linenb">290</div><div class="codeline">                                                                   separate</div><div class="clear"></div>
<div class="linenb">291</div><div class="codeline">                                                                   \\</div><div class="clear"></div>
<div class="linenb">292</div><div class="codeline">                                                                   phases<span class="keyword2">\end{tabular}</span>}</div><div class="clear"></div>
<div class="linenb">293</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">294</div><div class="codeline">               <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Rate</div><div class="clear"></div>
<div class="linenb">295</div><div class="codeline">                     of</div><div class="clear"></div>
<div class="linenb">296</div><div class="codeline">                     change</div><div class="clear"></div>
<div class="linenb">297</div><div class="codeline">                     of</div><div class="clear"></div>
<div class="linenb">298</div><div class="codeline">                     \\</div><div class="clear"></div>
<div class="linenb">299</div><div class="codeline">                     activity<span class="keyword2">\end{tabular}</span></div><div class="clear"></div>
<div class="linenb">300</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">301</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Activity</div><div class="clear"></div>
<div class="linenb">302</div><div class="codeline">                                                                   of</div><div class="clear"></div>
<div class="linenb">303</div><div class="codeline">                                                                   specialised</div><div class="clear"></div>
<div class="linenb">304</div><div class="codeline">                                                                   \\</div><div class="clear"></div>
<div class="linenb">305</div><div class="codeline">                                                                   neurons<span class="keyword2">\end{tabular}</span>}                 &amp;</div><div class="clear"></div>
<div class="linenb">306</div><div class="codeline">               <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Apical dendrites of \\ pyramidal neurons<span class="keyword2">\end{tabular}</span>                  \\</div><div class="clear"></div>
<div class="linenb">307</div><div class="codeline">               \hline \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}Data accounted for} &amp;</div><div class="clear"></div>
<div class="linenb">308</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Neural</div><div class="clear"></div>
<div class="linenb">309</div><div class="codeline">                                                                   responses \\ and</div><div class="clear"></div>
<div class="linenb">310</div><div class="codeline">                                                                   behaviour in a</div><div class="clear"></div>
<div class="linenb">311</div><div class="codeline">                                                                   \\</div><div class="clear"></div>
<div class="linenb">312</div><div class="codeline">                                                                   variety</div><div class="clear"></div>
<div class="linenb">313</div><div class="codeline">                                                                   of</div><div class="clear"></div>
<div class="linenb">314</div><div class="codeline">                                                                   tasks<span class="keyword2">\end{tabular}</span>}</div><div class="clear"></div>
<div class="linenb">315</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">316</div><div class="codeline">               <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Typical spike-time- \\ dependent plasticity<span class="keyword2">\end{tabular}</span>             &amp;</div><div class="clear"></div>
<div class="linenb">317</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}<span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Increased</div><div class="clear"></div>
<div class="linenb">318</div><div class="codeline">                                                                   neural \\</div><div class="clear"></div>
<div class="linenb">319</div><div class="codeline">                                                                   activity to</div><div class="clear"></div>
<div class="linenb">320</div><div class="codeline">                                                                   \\</div><div class="clear"></div>
<div class="linenb">321</div><div class="codeline">                                                                   unpredicted</div><div class="clear"></div>
<div class="linenb">322</div><div class="codeline">                                                                   stimuli<span class="keyword2">\end{tabular}</span>}</div><div class="clear"></div>
<div class="linenb">323</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">324</div><div class="codeline">               <span class="keyword2">\begin{tabular}</span>[c]{@{}l@{}}Properties of \\</div><div class="clear"></div>
<div class="linenb">325</div><div class="codeline">                     pyramidal neurons<span class="keyword2">\end{tabular}</span>                                                          \\</div><div class="clear"></div>
<div class="linenb">326</div><div class="codeline">               \hline \rowcolor[HTML]{D9D9D9} \multicolumn{2}{|l|}{\cellcolor[HTML]{D9D9D9}MNIST performance}  &amp;</div><div class="clear"></div>
<div class="linenb">327</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}$\sim$2-3}</div><div class="clear"></div>
<div class="linenb">328</div><div class="codeline">                                                                                                               &amp; -</div><div class="clear"></div>
<div class="linenb">329</div><div class="codeline">                                                                                                               &amp;</div><div class="clear"></div>
<div class="linenb">330</div><div class="codeline">               \multicolumn{1}{l|}{\cellcolor[HTML]{D9D9D9}$\sim$1.7}</div><div class="clear"></div>
<div class="linenb">331</div><div class="codeline">                                                                                                               &amp; $\sim$1.96     \\</div><div class="clear"></div>
<div class="linenb">332</div><div class="codeline">               \hline</div><div class="clear"></div>
<div class="linenb">333</div><div class="codeline">    <span class="keyword2">\end{tabular}</span></div><div class="clear"></div>
<div class="linenb">334</div><div class="codeline">  }<span class="keyword1">\caption</span>[Comparison between biologically plausible approximations of Backprop]{Comparison between biologically</div><div class="clear"></div>
<div class="linenb">335</div><div class="codeline">    plausible approximations of Backprop, adapted from \citep{whittington2019theories}. From left to right: Contrastive</div><div class="clear"></div>
<div class="linenb">336</div><div class="codeline">    hebbian learning \citep{OReilly1996}, Contrastive learing with continuous update \citep{Bengio2017}, Predictive</div><div class="clear"></div>
<div class="linenb">337</div><div class="codeline">    coding network \citep{Whittington2017}, Dendritic error network \citep{sacramento2018dendritic}. All algorithms were</div><div class="clear"></div>
<div class="linenb">338</div><div class="codeline">    selected because they reflect some properties of biological brains, some of which are highlighted in the row "Data</div><div class="clear"></div>
<div class="linenb">339</div><div class="codeline">    accounted for". All of the algorithms need to make concessions for this. In the first four rows, desirable</div><div class="clear"></div>
<div class="linenb">340</div><div class="codeline">    properties are highlighted in green, while undesirable traits are highlighted in red.}<span class="keyword1">\label</span>{tab-wb-models}</div><div class="clear"></div>
<div class="linenb">341</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">342</div><div class="codeline"><span class="keyword2">\end{table}</span></div><div class="clear"></div>
<div class="linenb">343</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">344</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">345</div><div class="codeline"><span class="keyword2">\begin{table}</span></div><div class="clear"></div>
<div class="linenb">346</div><div class="codeline">  \fontsize{12pt}{12pt}\selectfont</div><div class="clear"></div>
<div class="linenb">347</div><div class="codeline">  <span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">348</div><div class="codeline">    <span class="keyword2">\begin{tabular}</span>{p{0.25\textwidth}p{0.6\textwidth}p{0.15\textwidth}}    \hline</div><div class="clear"></div>
<div class="linenb">349</div><div class="codeline">      <span class="keyword1">\textbf</span>{Name}                &amp; <span class="keyword1">\textbf</span>{Description}                                                        &amp;</div><div class="clear"></div>
<div class="linenb">350</div><div class="codeline">      <span class="keyword1">\textbf</span>{Default}</div><div class="clear"></div>
<div class="linenb">351</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">352</div><div class="codeline">      \hline</div><div class="clear"></div>
<div class="linenb">353</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">354</div><div class="codeline">      \\<span class="keyword1">\textbf</span>{Simulation} \\\hline</div><div class="clear"></div>
<div class="linenb">355</div><div class="codeline">      \texttt{n\_epochs}           &amp; Number of training iterations                                               &amp;</div><div class="clear"></div>
<div class="linenb">356</div><div class="codeline">      $1000$                                                                                                             \\</div><div class="clear"></div>
<div class="linenb">357</div><div class="codeline">      \texttt{delta\_t}            &amp; Euler integration step in [ms]                                              &amp; $0.1$</div><div class="clear"></div>
<div class="linenb">358</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">359</div><div class="codeline">      \texttt{t\_pres}             &amp; Stimulus presentation time during training [ms]                             &amp; $50$</div><div class="clear"></div>
<div class="linenb">360</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">361</div><div class="codeline">      \texttt{out\_lag}            &amp; Delay before recording output during testing [ms]                           &amp; $35$</div><div class="clear"></div>
<div class="linenb">362</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">363</div><div class="codeline">      \texttt{dims}                &amp; Network dimensions, <span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>pyramidal neurons per layer                        &amp; [9,</div><div class="clear"></div>
<div class="linenb">364</div><div class="codeline">      30, 3]                                                                                                             \\</div><div class="clear"></div>
<div class="linenb">365</div><div class="codeline">      \texttt{threads}             &amp; Number of threads for parallel processing                                   &amp; $8$</div><div class="clear"></div>
<div class="linenb">366</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">367</div><div class="codeline">      \texttt{test\_interval}      &amp; Test the network every N epochs                                             &amp; $10$</div><div class="clear"></div>
<div class="linenb">368</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">369</div><div class="codeline">      \texttt{record\_interval}    &amp; Interval for storing membrane potentials [ms]                               &amp; $1$</div><div class="clear"></div>
<div class="linenb">370</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">371</div><div class="codeline">      \texttt{init\_self\_pred}    &amp; Flag to initialize weights to self-predicting state                         &amp;</div><div class="clear"></div>
<div class="linenb">372</div><div class="codeline">      \texttt{True}</div><div class="clear"></div>
<div class="linenb">373</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">374</div><div class="codeline">      \texttt{noise}               &amp; Flag to apply noise to all somatic membrane potentials                      &amp;</div><div class="clear"></div>
<div class="linenb">375</div><div class="codeline">      \texttt{False}                                                                                                     \\</div><div class="clear"></div>
<div class="linenb">376</div><div class="codeline">      \texttt{sigma}               &amp; Standard deviation for membrane potential noise                             &amp; 0.3</div><div class="clear"></div>
<div class="linenb">377</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">378</div><div class="codeline">      \texttt{mode}                &amp; Which dataset to train on. Choice between (bars, mnist, self-pred, teacher) &amp; bars</div><div class="clear"></div>
<div class="linenb">379</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">380</div><div class="codeline">      \texttt{store\_errors}       &amp; Flag to compute and store apical and interneuron errors during training     &amp;</div><div class="clear"></div>
<div class="linenb">381</div><div class="codeline">      \texttt{False}</div><div class="clear"></div>
<div class="linenb">382</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">383</div><div class="codeline">      \texttt{network\_type}       &amp; Choice between (numpy, snest, rnest)                                        &amp; snest</div><div class="clear"></div>
<div class="linenb">384</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">385</div><div class="codeline">      \texttt{tau\_x}              &amp; Network input filtering time constant [ms]                                  &amp; 0.1</div><div class="clear"></div>
<div class="linenb">386</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">387</div><div class="codeline">      \texttt{reset}               &amp; Reset method between simulations (0=no reset, 1=soft reset, 2=hard reset)   &amp; 2</div><div class="clear"></div>
<div class="linenb">388</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">389</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">390</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">391</div><div class="codeline">      <span class="keyword1">\textbf</span>{Neurons}</div><div class="clear"></div>
<div class="linenb">392</div><div class="codeline">      \\\hline</div><div class="clear"></div>
<div class="linenb">393</div><div class="codeline">      \texttt{latent\_equilibrium} &amp; Flag for whether to use prospective transfer functions                      &amp;</div><div class="clear"></div>
<div class="linenb">394</div><div class="codeline">      \texttt{True}</div><div class="clear"></div>
<div class="linenb">395</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">396</div><div class="codeline">      \texttt{g\_l}                &amp; Somatic leakage conductance [nS]                                            &amp; 0.03</div><div class="clear"></div>
<div class="linenb">397</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">398</div><div class="codeline">      \texttt{g\_a}                &amp; Apical compartment coupling conductance [nS]                                &amp; 0.06</div><div class="clear"></div>
<div class="linenb">399</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">400</div><div class="codeline">      \texttt{g\_d}                &amp; Basal compartment coupling conductance [nS]                                 &amp; 0.1</div><div class="clear"></div>
<div class="linenb">401</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">402</div><div class="codeline">      \texttt{g\_som}              &amp; Output neuron nudging conductance [nS]                                      &amp; 0.06</div><div class="clear"></div>
<div class="linenb">403</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">404</div><div class="codeline">      \texttt{g\_l\_eff}           &amp; Effective leakage conductance [nS]                                          &amp;</div><div class="clear"></div>
<div class="linenb">405</div><div class="codeline">      \texttt{g\_l+g\_d+g\_a}</div><div class="clear"></div>
<div class="linenb">406</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">407</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">408</div><div class="codeline">      \texttt{g\_lk\_dnd}          &amp; Dendritic leakage [nS]                                                      &amp;</div><div class="clear"></div>
<div class="linenb">409</div><div class="codeline">      \texttt{delta\_t}</div><div class="clear"></div>
<div class="linenb">410</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">411</div><div class="codeline">      \texttt{t\_ref}              &amp; Refractory period [ms]                                                      &amp; 0.0</div><div class="clear"></div>
<div class="linenb">412</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">413</div><div class="codeline">      \texttt{C\_m\_som}           &amp; Somatic compartment membrane capacitance [pF]                               &amp; 1.0</div><div class="clear"></div>
<div class="linenb">414</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">415</div><div class="codeline">      \texttt{C\_m\_bas}           &amp; Basal compartment membrane capacitance [pF]                                 &amp; 1.0</div><div class="clear"></div>
<div class="linenb">416</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">417</div><div class="codeline">      \texttt{C\_m\_api}           &amp; Apical compartment membrane capacitance [pF]                                &amp; 1.0</div><div class="clear"></div>
<div class="linenb">418</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">419</div><div class="codeline">      \texttt{gamma}               &amp; Linearly scales  activation function $\phi$                                 &amp; 1.0</div><div class="clear"></div>
<div class="linenb">420</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">421</div><div class="codeline">      \texttt{beta}                &amp; Exponentially scales  activation function $\phi$                            &amp; 1.0</div><div class="clear"></div>
<div class="linenb">422</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">423</div><div class="codeline">      \texttt{theta}               &amp; Shifts  activation function $\phi$                                          &amp; 0.0</div><div class="clear"></div>
<div class="linenb">424</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">425</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">426</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">427</div><div class="codeline">      <span class="keyword1">\textbf</span>{Synapses}</div><div class="clear"></div>
<div class="linenb">428</div><div class="codeline">      \\\hline</div><div class="clear"></div>
<div class="linenb">429</div><div class="codeline">      \texttt{wmin\_init}          &amp; Min. initial synaptic weight                                                &amp; -1.0</div><div class="clear"></div>
<div class="linenb">430</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">431</div><div class="codeline">      \texttt{wmax\_init}          &amp; Max. initial synaptic weight                                                &amp; 1.0</div><div class="clear"></div>
<div class="linenb">432</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">433</div><div class="codeline">      \texttt{Wmin}                &amp; Min. allowed synaptic weight                                                &amp; -4.0</div><div class="clear"></div>
<div class="linenb">434</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">435</div><div class="codeline">      \texttt{Wmax}                &amp; Max. allowed synaptic weight                                                &amp; 4.0</div><div class="clear"></div>
<div class="linenb">436</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">437</div><div class="codeline">      \texttt{tau\_delta}          &amp; Weight change filter time constant (NEST only) [ms]                         &amp; 1.0</div><div class="clear"></div>
<div class="linenb">438</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">439</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">440</div><div class="codeline">      \texttt{p\_conn}             &amp; Connection probability between populations                                  &amp; 1.0</div><div class="clear"></div>
<div class="linenb">441</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">442</div><div class="codeline">      \texttt{eta\_ip}             &amp; Learning rate for $pyr\rightarrow intn$ synapses                            &amp; 0.004</div><div class="clear"></div>
<div class="linenb">443</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">444</div><div class="codeline">      \texttt{eta\_pi}             &amp; Learning rate for $intn\rightarrow pyr$ synapses                            &amp; 0.01</div><div class="clear"></div>
<div class="linenb">445</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">446</div><div class="codeline">      \texttt{eta\_up}             &amp; Learning rates for feedforwarsd $pyr\rightarrow pyr$ synapses               &amp;</div><div class="clear"></div>
<div class="linenb">447</div><div class="codeline">      [0.01, 0.003]                                                                                                      \\</div><div class="clear"></div>
<div class="linenb">448</div><div class="codeline">      \texttt{eta\_down}           &amp; Learning rate for feedback $pyr\rightarrow pyr$ synapses                    &amp; 0.0</div><div class="clear"></div>
<div class="linenb">449</div><div class="codeline">      \\</div><div class="clear"></div>
<div class="linenb">450</div><div class="codeline">    <span class="keyword2">\end{tabular}</span><span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{Default parameters for the dendritic error model. }<span class="keyword1">\label</span>{tab-params</span>}</div><div class="clear"></div>
<div class="linenb">451</div><div class="codeline">  <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">452</div><div class="codeline"><span class="keyword2">\end{table}</span></div><div class="clear"></div>
</div>
<h2 class="filename">01_introduction.tex</h2>

<p>Found 77 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Introduction}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{Motivation}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">The outstanding learning capabilities of the human brain have been found to be elusive and as yet impossible to fully</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">explain or replicate in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [silicic] (171) [lt:en:MORFOLOGIK_RULE_EN_US]">silicio</span>. While in recent years the power of classical machine learning solutions  has improved</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">even beyond human capabilities for some tasks, their underlying algorithms cannot serve as a model of human cognition.</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">Some reasons why brains and machines appear irreconcilable relate to questions about network structure and neuron</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">models. Yet more pressingly, almost all the most powerful artificial neural networks are trained with the</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (605) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors algorithm, which has long been considered to be impossible for neurons to implement. Hence,</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">Neuroscience has dismissed this algorithm in an almost dogmatic way for many years after its development, stating that</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">the brain must employ a different mechanism to learn.</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">Yet in recent years, there has been a resurgence of research by neuroscientists towards reconciling biological and</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">artificial neural networks in spite of these concerns. This led to a number of experimental results indicating that</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">brains might be capable of performing something very similar to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (1192) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> after all. Furthermore, despite rigorous</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">efforts, no unifying alternative to this learning principle was found which performs well enough to account for the</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">brain's unmatched capabilities.</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">Hence, there now exists a vibrant community developing alternative ways to implement this algorithm - or some</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">approximation of it. These novel approaches are capable of replicating an increasing number of properties of biological</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">brains. Nevertheless, many issues remain unsolved, and a lot of neuronal features remain unaccounted for in brain models</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">that are capable of any kind of learning. It is this open problem, to which I want to dedicate my efforts in this</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">thesis. After reviewing the existing literature, I have selected a promising model of learning in cortical circuits.</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">This model uses multi-compartment neuron models and local plasticity rules to implement a variant of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (2081) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span>. In</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">this project, I will investigate and attempt to further improve its concordance with data on the human neocortex. I will</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">use the approach of computationally modelling the model while progressively adding biological features, attempting to</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">retain learning performance in the process.</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline"><span class="keyword1">\section</span>{The <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (2390) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors algorithm}</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">The <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (2431) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> of errors algorithm (<span class="keyword1">\textit</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (2468) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop}</span>) \citep{Schmidhuber2014} is the workhorse of modern machine</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">learning and is able to outperform humans on a growing number of tasks \citep{LeCun2015}. Particularly for training deep</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">neural networks it has remained popular and largely unchanged since its initial development. Its learning potential</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">stems from its unique capability to attribute errors in the output of a network to activations of specific neurons and</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">connections within its hidden layers. This property also forms the basis of the algorithm's name; After an initial</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">forward pass to form a prediction about the nature of a given input, a separate backward pass propagates the arising</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">error through all layers in reverse order. During this second network traversal, local error gradients dictate to what</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">extent a given weight needs to be altered so that the next presentation of the same sample would elicit a lower error in</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">the output layer. It has been argued that through this mechanism, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (3397) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> solves the <span class="keyword1">\textit</span>{credit assignment problem}</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">- i.e.\ the question to what degree a parameter contributes to an error signal - optimally \citep{Lillicrap2020}. With</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">this critical information in hand, computing parameter changes that decrease error becomes almost trivial. As biological</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">neural networks are likewise subject to the credit assignment problem, finding a general solution to it promises to be</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">invaluable to neuroscience. For a long time <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (3828) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> was believed to be unsuitable for networks of biological neurons</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">for several reasons.</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"><span class="keyword1">\section</span>{Concerns over biological plausibility}</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">While <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (3970) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> continues to prove exceptionally useful in conventional machine learning systems, it is viewed critically</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">by many neuroscientists. For one, it relies on a slow adaptation of synaptic weights, and therefore requires a large</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">amount of examples to learn rather simple input-output mappings. In this particular way, its performance is far inferior</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">to the powerful one-shot learning exhibited by humans \citep{Brea2016}. Yet more importantly, no plausible mechanisms</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">have yet been found by which biological neural networks could implement the algorithm. In fact, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (4524) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> as a way by</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">which brains may learn has been dismissed entirely by much of the neuroscience community for decades</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">\citep{Grossberg1987,Crick1989,Mazzoni1991,OReilly1996}. This dismissal is often focussed on three mechanisms that are</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">instrumental for the algorithm \citep{whittington2019theories,Bengio2015,Liao2016}:</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline"><span class="keyword1">\subsection</span>{Local error representation}</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">Neuron-specific errors in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (4806) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> are computed and propagated by a mechanism that is completely detached from the</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">network itself, which requires access to the entirety of the network state. In order to compute the weight changes for a</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">given layer, the algorithm takes as an input the activation and synaptic weights of all downstream neurons. In contrast,</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">plasticity in biological neurons is largely considered to be primarily dependent on factors that are local to the</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">synapse \citep{Abbott2000,magee2020synaptic,urbanczik2014learning}. While <span class="highlight-spelling" title="Possible spelling mistake found. (5270) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromodulators</span> are known to influence</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">synaptic plasticity, their dispersion is too wide to communicate neuron-specific errors. Thus, biologically plausible</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (5427) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> would require a method for encoding errors locally, i.e.\ close to the neurons to which they relate. This has</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">been perhaps the strongest criticism of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (5586) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> in the brain, as many questions regarding mechanisms for both computing</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">and storing these errors remain unanswered as yet.</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline"><span class="keyword1">\subsection</span>{The weight transport problem}</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">During the weight update stage of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (5783) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>, errors are transmitted between layers with the same weights that are used in</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">the forward pass. In other words, the magnitude of a neuron-specific error that is back-propagated through a given</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">connection should be proportional to its impact on output loss during the forward pass. To replicate this, a neuronal</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">network implementing <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (6124) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> would require feedback connections that mirror both the precise connectivity and synaptic</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">weights of the forward connections. Bidirectional connections that could theoretically back-propagate errors are common</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">in the cortex, yet it is unclear by which mechanism pairs of synapses would be able to align. This issue becomes</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">particularly apparent when considering long-range pyramidal projections. In these, the feedforward and feedback synapses</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">which need to be aligned would potentially be separated by a considerable distance.</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline"><span class="keyword1">\subsection</span>{Neuron models}</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">Finally, the types of artificial neurons typically used in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (6736) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> transmit a continuous scalar activation at all</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">times, instead of discrete spikes. In theory, these activations correspond to the firing rate of a spiking neuron,</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">giving this class of models the title <span class="keyword1">\textit</span>{rate neurons}. Yet handling spike based communication requires more</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">sophisticated neuron models than are typically employed in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7071) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> networks. Additionally, plasticity rules for rate</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">neurons do not necessarily have an easily derived counterpart for spiking neurons. A notable example for this issue is</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7249) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> itself; The local error gradient of a neuron is not trivial to compute for spiking neural networks (SNN), as a</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (7369) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> has no natural derivative. Furthermore, a given neuron's activation in classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7461) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> is computed from a</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">simple weighted sum of all inputs. This fails to capture the complex <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [collinearities] (7558) [lt:en:MORFOLOGIK_RULE_EN_US]">nonlinearities</span> of dendritic integration that are</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">fundamental to cortical neurons (cf. Section \ref{sec-dendrites}). Finally, these abstract neurons - at least in</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7712) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> - have no persistence through time. Thus, their activation is dictated strictly by instantaneous</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">presynaptic activity, in contrast to the leaky membrane dynamics exhibited by biological neurons.</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">section</span>{Overcoming biological implausibility}</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (7956) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> has remained the gold standard against which most attempts at modelling learning in the brain eventually are</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">compared. Also, despite its apparent biological implausibility, it does share some notable parallels to learning in the</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">brain. Artificial neural networks (ANN) trained with <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (8247) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> have been shown to develop similar representations to</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">those found in brain areas responsible for comparable tasks</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">\citep{Yamins2016,Whittington2018,KhalighRazavi2014,Kubilius2016}. Thus, numerous attempts have been made to define more</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">biologically plausible learning rules which approximate <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (8485) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> to some degree. A full review of the available</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">literature would be out of scope for this thesis, so only a few examples will be discussed in this section. \newline</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">\noindent One approach to solve the issues around local error representations is, to drive synaptic plasticity through a</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Global] (8661) [lt:en:UPPERCASE_SENTENCE_START]">global</span> error signal \citep{potjans2011imperfect,mozafari2018combining,sutton2018reinforcement}. The appeal of this</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">solution is that such signalling could be plausibly performed by <span class="highlight-spelling" title="Possible spelling mistake found. (8770) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromodulators</span> like dopamine</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">\citep{Mazzoni1991,Seung2003,izhikevich2007solving}. These types solutions to not approximate <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (8846) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>, but instead lead</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">to a kind of reinforcement learning. While some consider this the most plausible way for brains to learn</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">\citep{sutton2018reinforcement}, performance of global error/reward signalling stays far behind that of the credit</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">assignment performed by <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (9089) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. Additionally, this class of algorithms requires even more examples of a training</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">dataset, and was shown to scale poorly with network size \citep{Werfel2003}. </div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">Two prominent classes of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (9269) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> approximations have been developed, which are capable of locally representing errors.</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">These algorithms encode errors in either activation changes over time or local membrane potentials. They will be</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">discussed further in Section \ref{sec-model-selection}.\newline </div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">\noindent The weight transport problem was successfully addressed by a mechanism called <span class="keyword1">\textit</span>{Feedback Alignment} (FA)</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">\citep{Lillicrap2014}. This seminal paper shows that <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (9556) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> can still learn successfully when feedback weights are</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">random. In addition to learning to represent an input-output mapping in forward weights, the network is trained to</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">extract useful information from randomly weighted instructive pathways. The authors call this process <span class="keyword1">\textit</span>{learning</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">to learn}, and show that performance is even superior to classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (9912) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> for some tasks. This mechanism was further</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">expanded to show that the principles of FA perform very well when biologically plausible plasticity rules are employed</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">\citep{Liao2016,Zenke2018}. Another popular line of thought is - instead of computing local errors - to compute optimal</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">activations for hidden layer neurons using <span class="highlight-spelling" title="Possible spelling mistake found. (10223) [lt:en:MORFOLOGIK_RULE_EN_US]">autoencoders</span> \citep{Bengio2014,Lee2015,Ahmad2020}. Approaches derived from</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">this do not suffer from the weight transport problem, and by design does not require local error representations. While</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">these solutions (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [summited] (10402) [lt:en:MORFOLOGIK_RULE_EN_US]">summized</span> as <span class="keyword1">\textit</span>{Target propagation} algorithms) solve the weight transport problem, they fall far</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">behind traditional <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (10514) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> on more complex benchmark datasets like</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline"><span class="keyword1">\textit</span>{\href{https://www.cs.toronto.edu/~kriz/cifar.html}{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [CIGAR, CIFAD, CIF AR] (10563) [lt:en:MORFOLOGIK_RULE_EN_US]">CIFAR}}</span> and</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline"><span class="keyword1">\textit</span>{\href{https://www.image-net.org/index.php}{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Imagine, Imagined, Magnet, Imagines, Imogene] (10573) [lt:en:MORFOLOGIK_RULE_EN_US]">ImageNet}}</span> \citep{Bartunov2018}.\newline</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">\noindent Numerous approaches for implementing Backprop with more plausible neuron models exist, most of which employ variants of</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [The] (10597) [lt:en:UPPERCASE_SENTENCE_START]">the</span> <span class="keyword1">\textit</span>{Leaky Integrate-and-fire} (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [IF, LIFE, LIE, LIFT, LID, LIP, LIN, LIZ, AIF, LEIF, LIB, LIFO, RIF, LIT, LIQ, BIF, CIF, DIF, FIF, GIF, HIF, IIF, LBF, LCF, LDIF, LEF, LFI, LGF, LI, LIA, LIH, LIM, LIR, LIX, LOF, LPF, LRF, LSF, LTF, LUF, MIF, NIF, PIF, WIF, ZIF, LIEF, LII, L IF, LI F, L&amp;F, LDF, LF, LIC, LIU, LIV, TIF] (10627) [lt:en:MORFOLOGIK_RULE_EN_US]">LIF</span>) neuron \citep{Sporea2013,Lee2016,Bengio2017,Lee2020}. The aforementioned</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">issue of computing the derivative over <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike trains] (10702) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrains</span> has been solved in several ways, with the most prominent variant</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">perhaps being <span class="keyword1">\textit</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SuperS pike] (10793) [lt:en:MORFOLOGIK_RULE_EN_US]">SuperSpike}</span> \citep{Zenke2018}. One might therefore view this as the weakest criticism aimed at</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (10873) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. Yet none of the employed neuron models come close to portraying the intricacies of biological neurons, and</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">thus fail to provide explanations for their complexity. One aspect of this will be discussed in the upcoming</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">section.\newline</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">\noindent All of these studies successfully solve one or more concerns of biological plausibility, while still</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Approximating] (11118) [lt:en:UPPERCASE_SENTENCE_START]">approximating</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (11132) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> to some degree. Yet none of them are able to solve all three simultaneously, and some of them</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">introduce novel mechanisms that are themselves biologically questionable. It further appears that in all but a few</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">cases, an increase in biological plausibility leads to a decrease in performance. Thus, whether <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (11446) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> could be</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">implemented or approximate by biological neurons remains an open question.</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline"><span class="keyword1">\subsection</span>{Dendrites as computational elements}<span class="keyword1">\label</span>{sec-dendrites}</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">The issue of oversimplified neuron models is by far the most frequent to be omitted from explanations of the biological</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">implausibility of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (11715) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> (See for example \citep{Meulemans2020,Lillicrap2014}). This disregard might stem from the</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">fact that rate-based point neurons are employed in many of the most powerful artificial neural networks. This fact might</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">be taken as an argument that the simple summation of synaptic inputs is sufficient for powerful and generalized</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">learning. Modelling neurons more closely to biology would <span class="highlight" title="Did you maybe mean 'buy' or 'be'?. Suggestions: [buy, be] (12073) [lt:en:BY_BUY]">by</span> this view only increase mathematical complexity and</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">computational cost without practical benefit. Another hypothesis states that the dominance of point neurons stems from a</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline"><span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span><span class="highlight-spelling" title="Possible spelling mistake found. (12250) [lt:en:MORFOLOGIK_RULE_EN_US]">somato-centric</span> perspective<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> within neuroscience \citep{Larkum2018}, which stems from the technical challenges inherent</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">to studying dendrites in vivo. The vastly different amount of available data regarding these two neuronal components</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">might have induced a bias in how neurons are modelled computationally. Some researchers have even questioned whether</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">dendrites should be seen as more of a 'bug' than a 'feature' \citep{Haeusser2003}, i.e.\ a biological necessity which</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">needs to be overcome and compensated for.</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">Yet in recent years, with novel mechanisms of dendritic computation being discovered, interest in researching and</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">explicitly modelling dendrites has increased. Particularly the vast dendritic branches of pyramidal neurons found in the</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">cerebral cortex, hippocampus and amygdala, were shown to perform complex integrations of their synaptic inputs</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">\citep{spruston2008pyramidal}. These dendritic trees are capable of performing coincidence- \citep{Larkum1999} and</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">sequence detection \citep{Branco2010}. The size of dendritic trees is also known to discriminate regular spiking from</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">burst firing pyramidal neurons \citep{Elburg2010}. Furthermore, pyramidal neuron dendrites are capable of performing</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">computations, which were previously assumed to require multi-layer neural networks \citep{Schiess2016,Gidon2020}. See</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">\citep{Larkum2022} and \citep{Poirazi2020} for extensive reviews. </div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline">These neuroscientific insights have  sparked hope that modelling dendritic compartments explicitly might aid machine</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">learning in terms in both learning performance and energy efficiency</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">\citep{Chavlis2021,guerguiev2017towards,Richards2019,Eyal2018}. It appears that, if not for computational gains,</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">dendrites should be considered essential for any model attempting to explain the power of human learning. While the</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">network discussed in this thesis includes very simple multi-compartment models, the choice of model was</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">strongly influenced by the fact that segregated dendrites were considered at all.</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline"><span class="keyword1">\section</span>{Cortical microcircuits}</div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">Another feature of the brain which is often not considered in (biologically plausible) machine learning models is its</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">intricate connectivity. This is quite understandable, as there is still some uncertainty about which brain areas would</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">be involved in <span class="highlight-spelling" title="Possible spelling mistake found. (14308) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop-like</span> learning. It is also unclear, to what level of detail these areas would need to be modeled.</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">It has been shown that the connectivity patterns of cortical circuits are superior to amorphous networks in some cases</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">\citep{haeusler2007statistical}, so there might be a computational gain from modeling network structure closer to</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">biology. The question over network structure goes hand in hand with the choice of neuron models, as synaptic connections</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">arrive at specific points of pyramidal neuron dendrites, depending on the origin of the connection</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">\citep{felleman1991distributed,Ishizuka1995,Larkum2018}.</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">Several theories of cortical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [function] (14874) [lt:en:MORFOLOGIK_RULE_EN_US]">funcition</span> focus more on reinforcement \citep{Legenstein2008} or unsupervised learning</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">\citep{George2009,hausler2017inhibitory}. Without dismissing these theories, this thesis will adopt the viewpoint that</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">human brains require a form of gradient descent to successfully adapt to their ever-changing environments. Furthermore,</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">we share the hypothesis that this kind of learning occurs predominantly in the neocortex \citep{Marblestone2016}.</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">The literature on the subject of learning historically appears to be somewhat split (although <span class="highlight" title="This word has been used in one of the immediately preceding sentences. Using a synonym could make your text more interesting to read, unless the repetition is intentional.. Suggestions: [many, numerous, various, countless] (15332) [lt:en:EN_REPEATEDWORDS]">several</span> important</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">exceptions have been published recently). On the one hand, the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>machine-learning<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> point of view largely considers the</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">utility of network changes first, with considerations of biology appearing as an afterthough<span class="highlight-sh" title="Add a space before citation or reference. [sh:c:001]">t<span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cite me] (15562) [lt:en:MORFOLOGIK_RULE_EN_US]">cite</span>m</span>e</span>. On the other hand,</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">intricate models of cortical circuits exist, which can so far not be trained to perform tasks</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">\citep{potjans2014cell,schmidt2018multi,van2022bringing}. Within this thesis, I hope to contribute to the body of</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">literature between those extremes. For this, my approach will be to select a learning model that is already highly</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">biologically plausible, and to attempt to improve its plausibility - without breaking the learning rule.</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline"><span class="keyword1">\section</span>{Model selection}<span class="keyword1">\label</span>{sec-model-selection}</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline">The model selection progress was strongly influenced by a review article on biologically plausible approximations of</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (16099) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> \citep{whittington2019theories}. The authors narrow the wide range of proposed solutions down to four</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">algorithms that are both highly performant and largely biologically plausible. Due to impact of the paper on this</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">thesis, their model comparison is depicted in Supplementary Table \ref{tab-wb-models}. The algorithms were in part</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline">selected for requiring minimal external control during training, as well as by the fact that they can all be described</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">within a common framework of energy minimization \citep{Scellier2017}. The first two models are Contrastive learning</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">\citep{OReilly1996}, and its extension to time-continuous updates \citep{Bengio2017}. Both of these encode</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">neuron-specific errors in the change of neural activity over time. One of their appeals is the fact that they rely on</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Serbian, Lesbian, Debian, Hessian] (16806) [lt:en:MORFOLOGIK_RULE_EN_US]">Hebbian</span> (and <span class="highlight-spelling" title="Possible spelling mistake found. (16819) [lt:en:MORFOLOGIK_RULE_EN_US]">Anti-Hebbian</span>) plasticity, which are highly regarded in the neuroscience literature</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">\citep{magee2020synaptic,Brea2016}. Yet in the plasticity rule also lies their greatest weakness, as synapses need to</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">switch between the two opposing mechanisms once the target for a given stimulus is provided. This switch requires a</div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">global signal that communicates the change in state to all neurons in the network simultaneously.</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">The second class of models was more appealing to me, as both variants are based on the predictive coding account in</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">Neuroscience \citep{rao1999predictive}, which deserves its own introduction.</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline"><span class="keyword1">\subsection</span>{Predictive coding}</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">In this seminal model of processing in the visual cortex, each level of the visual hierarchy represents the outside</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">world at some level of abstraction. Recurrent connections then serve communicate prediction errors and predictions up</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">and down the hierarchy respectively, which the network attempts to reconcile. The authors show that through rather</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">simple computations, these prediction errors can be minimized to obtain useful representations at each level of the</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">hierarchy. They further show that a predictive coding network trained on natural images exhibits end-stopping properties</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">previously found in mammalian visual cortex neurons. This work was instrumental to shaping the modern neuroscientific</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">perspective of perception being largely driven by <span class="highlight-spelling" title="Possible spelling mistake found. (18149) [lt:en:MORFOLOGIK_RULE_EN_US]">cortico-cortical</span> feedback connections in addition to the feedforward</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">processes. The extension of predictive coding principles from visual processing to the entire living system is promising</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">to revolutionize neuroscience under the name of <span class="keyword1">\textit</span>{Active inference} \citep{Friston2008,Friston2009,Adams2015}. By</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">this view, the entire brain aims to minimize prediction errors with respect to an internal (generative) model of the</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline">world. A noteworthy property of this hypothesis is that it implies an agents action in the world as 'just another' way</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline">in which it can decrease discrepancies between its beliefs and sensory information. In a seminal paper, a model of the</div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">cortical microcircuit \citep{haeusler2007statistical} was shown to have a plausible way for performing the computations</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline">required by predictive coding \citep{bastos2012canonical}.</div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline">While predictive coding was originally described as a mechanism for unsupervised learning, through a slight modification</div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline">it is also capable of performing <span class="highlight-spelling" title="Possible spelling mistake found. (19049) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop-like</span> supervised learning \citep{Whittington2017}. This is the third model</div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline">considered in the review paper, in which values (i.e.\ predictions) and errors of a layer are encoded in separate,</div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline">recurrently connected neuron populations. By employing only local <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Serbian, Lesbian, Debian, Hessian] (19293) [lt:en:MORFOLOGIK_RULE_EN_US]">Hebbian</span> plasticity, this network is capable of</div><div class="clear"></div>
<div class="linenb">252</div><div class="codeline">approximating <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (19354) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> in multilayer perceptrons while conforming to the principles of predictive coding. The constraint</div><div class="clear"></div>
<div class="linenb">253</div><div class="codeline">on network topology was later relaxed by showing that the model is capable of approximating <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (19553) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> for arbitrary</div><div class="clear"></div>
<div class="linenb">254</div><div class="codeline">computation graphs \citep{Millidge2022}. The neuron-based predictive coding network was therefore an important</div><div class="clear"></div>
<div class="linenb">255</div><div class="codeline">contribution towards unifying the fields of Active inference and machine learning research. As noted in a recent review</div><div class="clear"></div>
<div class="linenb">256</div><div class="codeline">article:</div><div class="clear"></div>
<div class="linenb">257</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">258</div><div class="codeline"><span class="keyword2">\begin{quotation}</span></div><div class="clear"></div>
<div class="linenb">259</div><div class="codeline">  \noindent``Since predictive coding is largely biologically plausible, and has many potentially plausible process</div><div class="clear"></div>
<div class="linenb">260</div><div class="codeline">  <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Theories] (19815) [lt:en:UPPERCASE_SENTENCE_START]">theories</span>, this close link between the theories provides a potential route to the development of a biologically</div><div class="clear"></div>
<div class="linenb">261</div><div class="codeline">  plausible alternative to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [backdrop, back prop] (19953) [lt:en:MORFOLOGIK_RULE_EN_US]">backprop</span>, which may be implemented in the brain. Additionally, since predictive coding can be</div><div class="clear"></div>
<div class="linenb">262</div><div class="codeline">  derived as a variational inference algorithm, it also provides a close and fascinating link between <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [back propagation] (20149) [lt:en:MORFOLOGIK_RULE_EN_US]">backpropagation</span> of</div><div class="clear"></div>
<div class="linenb">263</div><div class="codeline">  error and variational inference.`` \citep{millidge2021predictive} <span class="keyword2">\end{quotation}</span></div><div class="clear"></div>
<div class="linenb">264</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">265</div><div class="codeline">\noindent With this perspective in mind, we turn to the final model discussed in the review paper.</div><div class="clear"></div>
<div class="linenb">266</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">267</div><div class="codeline"><span class="keyword1">\subsection</span>{The Dendritic error model}</div><div class="clear"></div>
<div class="linenb">268</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">269</div><div class="codeline">The predictive coding network stores local prediction errors in nodes (i.e.\ neurons) close to the nodes to which these</div><div class="clear"></div>
<div class="linenb">270</div><div class="codeline">errors relate. That errors may be represented within the activation of individual neurons is a promising hypothesis with</div><div class="clear"></div>
<div class="linenb">271</div><div class="codeline">some advantages, as well as results backing it up \citep{Hertaeg2022}. Yet there is a competing view, by which errors</div><div class="clear"></div>
<div class="linenb">272</div><div class="codeline">elicited by individual neurons may be represented by membrane potentials of their dendritic compartments</div><div class="clear"></div>
<div class="linenb">273</div><div class="codeline">\citep{guerguiev2017towards}. The <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>Dendritic error model<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> \citep{sacramento2018dendritic} - as the name implies -</div><div class="clear"></div>
<div class="linenb">274</div><div class="codeline">follows this line of thought. It contains a highly recurrent network of both pyramidal- and interneurons, in which</div><div class="clear"></div>
<div class="linenb">275</div><div class="codeline">pyramidal neuron apical dendrites encode prediction errors. This view is supported by behavioral rodent experiments</div><div class="clear"></div>
<div class="linenb">276</div><div class="codeline">which show that stimulation to pyramidal neuron apical tufts in cortical layer 1 controls learning \citep{Doron2020}.</div><div class="clear"></div>
<div class="linenb">277</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">278</div><div class="codeline">For the errors to be encoded successfully, the model requires a symmetry between feedforward and feedback sets of</div><div class="clear"></div>
<div class="linenb">279</div><div class="codeline">weights, which it has to learn prior to training. After that, apical compartments behave like the error nodes in a</div><div class="clear"></div>
<div class="linenb">280</div><div class="codeline">predictive coding network. They are silent during a feedforward network pass, and encode local prediction errors in</div><div class="clear"></div>
<div class="linenb">281</div><div class="codeline">their membrane potential when a target is applied to the output layer. Since they are a part of the pyramidal neuron,</div><div class="clear"></div>
<div class="linenb">282</div><div class="codeline">only local information is required to minimize these prediction errors through a plasticity rule for multi-compartment</div><div class="clear"></div>
<div class="linenb">283</div><div class="codeline">neurons \citep{urbanczik2014learning}. A critical observation made in \citep{whittington2019theories} is that the</div><div class="clear"></div>
<div class="linenb">284</div><div class="codeline">dendritic error model is mathematically equivalent to their predictive coding network \todo{expand if I have time,</div><div class="clear"></div>
<div class="linenb">285</div><div class="codeline">otherwise this will be a ref<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (21873) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> All of these factors combined make the dendritic error model a promising model to help</div><div class="clear"></div>
<div class="linenb">286</div><div class="codeline">us further understand both predictive coding and deep learning in cortical circuits. While both the employed neuron and</div><div class="clear"></div>
<div class="linenb">287</div><div class="codeline">connectivity model are far behind some of the more rigorous cortical simulations, it is regarded in the</div><div class="clear"></div>
<div class="linenb">288</div><div class="codeline">literature as an important step towards integrating deep learning and neuroscience.</div><div class="clear"></div>
<div class="linenb">289</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">290</div><div class="codeline">Nevertheless, the model still suffers from some constraints with regard to its biological plausibility; Both the</div><div class="clear"></div>
<div class="linenb">291</div><div class="codeline">predictive coding network and the dendritic error network require strongly constrained connectivity schemes, without</div><div class="clear"></div>
<div class="linenb">292</div><div class="codeline">which they cannot learn. This kind of specificity (in particular one-to-one relationships between pairs of neurons) are</div><div class="clear"></div>
<div class="linenb">293</div><div class="codeline">highly untypical for cortical connections \citep{Thomson2003}. Hence, their exact network architectures are unlikely to</div><div class="clear"></div>
<div class="linenb">294</div><div class="codeline">be present in the cortex. The Dendritic error model additionally requires Pre-training to be capable of approximating</div><div class="clear"></div>
<div class="linenb">295</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (22844) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. Both of these issues will be discussed in this thesis. Yet the most salient improvement to the network's</div><div class="clear"></div>
<div class="linenb">296</div><div class="codeline">biological plausibility is likely, to change neuron models from rate-based to spiking neurons. It has been shown that</div><div class="clear"></div>
<div class="linenb">297</div><div class="codeline">the Plasticity rule employed by the network is capable of performing simple learning tasks when adapted to spiking</div><div class="clear"></div>
<div class="linenb">298</div><div class="codeline">neurons \citep{Stapmanns2021}. Yet, (to the best of my knowledge) there are no studies investigating if this variant is</div><div class="clear"></div>
<div class="linenb">299</div><div class="codeline">capable of learning more complex tasks on a network-level. A spiking implementation of the dendritic error network will</div><div class="clear"></div>
<div class="linenb">300</div><div class="codeline">therefore be the starting point for this thesis, upon which further analysis shall build.</div><div class="clear"></div>
<div class="linenb">301</div><div class="codeline">&nbsp;</div><div class="clear"></div>
</div>
<h2 class="filename">04_discussion.tex</h2>

<p>Found 70 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\chapter{Discussion}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{Contribution}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">In this project the capabilities and limitations of the dendritic error network and its underlying plasticity rule were</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">further tested. While sensitive to certain parameter changes, the network was shown to be exceptionally capable of</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">handling various constraints that are imposed on biological neural networks. Furthermore, the performed experiments can</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">be interpreted as dispelling some criticisms aimed at the model's biological plausibility. Here we summarize many of the</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">major questions about the biological plausibility of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (557) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> from the relevant literature.</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline"><span class="keyword1">\subsection</span>*{Evaluation of biological plausibility}</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">The original dendritic error network by design solves several biologically implausible mechanisms of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (738) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. It</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline"><span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{locally computes prediction errors}} and encodes them within membrane potentials of pyramidal neuron</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">apical dendrites. Furthermore, it provides two separate solutions to the <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{weight transport problem}}:</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">First, it is capable of learning through feedback alignment, as was used in all present <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [simulations] (1037) [lt:en:MORFOLOGIK_RULE_EN_US]">simualtions</span>. Secondly,</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">experiments employing steady-state-approximations have been successful in training feedback weights through variants of</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">the dendritic error rules. Finally, the network relies strictly on <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{Local plasticity}}</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">\citep{Whittington2017}, and models a (somewhat limited) variability in cell types \citep{Bartunov2018}.\newline</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">The present work further improves on the <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{neuron model}} by showing that spike-based communication does</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">not interfere with the dendritic plasticity rule, or the intricate balance of excitation and inhibition demanded by the</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">network. We also show that the network can be trained with absolutely <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{minimal external control}}</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">\citep{Whittington2017}. The network requires no external interference such as manual resets or phased plasticity, and</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">can handle background noise in between sample presentations. Exploratory experiments were conducted in support of the</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">hypothesis that the plasticity rule is capable of credit assignment when the network conforms strictly to</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline"><span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{Dale's law}} \citep{Bartunov2018}. In related experiments, the network proved very capable of</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (2055) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop-like</span> learning when constrained by a more <span class="keyword1">\textit</span>{<span class="keyword1">\textbf</span>{plausible architecture}} \citep{Whittington2017} in</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">which populations were connected imperfectly. Finally, the criticism that the network requires pre-training</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">\citep{whittington2019theories} was argued to be largely immaterial. The sum of these observations arguably makes the</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">dendritic error model one of the most biologically plausible approximations of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (2412) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> yet. In spite of these advances,</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">several critical limitations remain.</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline"><span class="keyword1">\section</span>{Limitations}</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline"><span class="highlight" title="This word has been used in one of the immediately preceding sentences. Using a synonym could make your text more interesting to read, unless the repetition is intentional.. Suggestions: [Many, Numerous, Various, Countless] (2506) [lt:en:EN_REPEATEDWORDS]">Several</span> general concerns regarding <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (2541) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> were not <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [addressed, dressed, a dressed] (2559) [lt:en:MORFOLOGIK_RULE_EN_US]">adressed</span> here. For example, it remains unclear how an agent would</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">come by the labels with which it might perform this type of learning \citep{Bengio2015}. Furthermore, whether cortical neurons engage in</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">any kind of supervised learning is still in question by some \citep{magee2020synaptic}. More importantly, the</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">requirement for one-to-one connections between pyramidal- and interneurons could not be overcome in this thesis. Thus,</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">one of the major criticisms from \citep{whittington2019theories} remains. Also, these nudging connections transmit</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">somatic activation without any kind of nonlinearity or delay, further making them biologically questionable. A plausible</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">way for cortical circuits to implement the learning mechanism of this model therefore remains to be found.</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Response to unpredicted stimuli}</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">One of the predictions about cortical activity made by predictive coding is an increased network activity in response to</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">sensory input that violates expectations. A diverse set of studies has since reported <span class="highlight-spelling" title="Possible spelling mistake. 'behaviour' is British English.. Suggestions: [behavior] (3509) [lt:en:MORFOLOGIK_RULE_EN_US]">behaviour</span> consistent with this in</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">various primate cortical neurons (see <span class="highlight-sh" title="Do not refer to tables using hard-coded numbers. Use \ref instead. [sh:hctab]">Table 1</span> in \citep{bastos2012canonical} for a review). As the dendritic error</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">network encodes errors in dendritic potentials instead of neuron activations, experiments proved that it does not</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">exhibit this property in any of its populations. In fact, overall network activity in response to a stimulus seems to</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">increase after training. In this way, the model conflicts with empirical data on cortical activity.</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Spike frequencies}</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">As discussed in Section \ref{sec-c-m-api}, the network in its current state is unable to learn efficiently for low</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">values of $\psi$. As a result, the implementation demands physiologically impossible spike frequencies from both</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">pyramidal- and interneurons. While increasing membrane <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [capacitance] (4248) [lt:en:MORFOLOGIK_RULE_EN_US]">capacitances</span> did relax this constraint somewhat, it in turn</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">requires longer presentation times per stimulus. Further work is required to determine if the network is capable of</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">learning when spike frequencies are as low as reported for cortical neurons.</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Benchmark datasets}</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">Training the network on a benchmark dataset would have been very desirable for comparing the spiking implementation to</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">previous iterations. Yet as noted previously, the full network dynamics are prohibitively expensive for simulations of</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">large networks. Extrapolating the results from Fig. \ref{fig-benchmark-threads-psi} shows that training this</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">implementation on the full <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MOIST, MIST, MONIST, MN IST, NIST] (4866) [lt:en:MORFOLOGIK_RULE_EN_US]">MNIST</span> dataset is unfeasible. A full training with <span class="highlight" title="In English-speaking countries (except for South Africa), the correct thousands separator is a comma.. Suggestions: [5,000,000] (4916) [lt:en:COMMA_PERIOD_CONFUSION]">$5.000.000$</span> sample presentations (cf.</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline"><span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\cite{</span>Haider2021}) would require over 1 Year on 32 threads (excluding testing/validation). Several iterations of the</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">neuron model were required to reach this performance. The implementation furthermore contains mechanisms for <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [down scaling] (5164) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>downscaling</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">the dataset to be compatible with smaller network sizes. Yet training for these still took on the order of <span class="highlight" title="This word has been used in one of the immediately preceding sentences. Using a synonym could make your text more interesting to read, unless the repetition is intentional.. Suggestions: [many, numerous, various, countless] (5283) [lt:en:EN_REPEATEDWORDS]">several</span> days,</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">making parameter imperfections very costly. While I am optimistic about the network's capability in general, no</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">parametrization was found in time under which the network was capable of learning <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MOIST, MIST, MONIST, MN IST, NIST] (5491) [lt:en:MORFOLOGIK_RULE_EN_US]">MNIST</span>. Thus, computational efficiency</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">remains a major drawback of this model, and has been a limiting factor in this thesis.</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline"><span class="keyword1">\section</span>{Future directions}</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{Computational efficiency}</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">The high computational demand of the network was already reported in the original implementation. It was largely</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">alleviated through steady-state approximations and the addition of Latent equilibrium. The SNN implementation in the</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">present work reintroduces this issue, and regrettably exacerbates it considerably. To a degree, decreased speed is an</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [inadvisable, inadvertence, avertable, insertable, inseverable, invertible] (6011) [lt:en:MORFOLOGIK_RULE_EN_US]">inadvertable</span> price to be paid for a more exact modelling of neuronal processes. There is a high computational cost</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">attached to more complex neuron dynamics, plasticity rules, network structures and so forth. In short, any attempt at</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">modelling the intricacies of biological neurons must be expected to be computational costly. Given the (still very high)</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">level of abstraction of the developed model paired with its poor speed, this perspective is slightly concerning.</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">Therefore, the models require rigorous optimization.</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">Some initial directions for this are provided by the benchmarks performed in this study. The spike-based plasticity rule</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">for example is highly costly. One possible optimization was already provided by \citep{Stapmanns2021}. In the paper, a</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">third update variant for the dendritic plasticity rule is discussed. Instead of a strictly event-based or time-based</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">update rule, a hybrid variant called <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>Event-based update with compression<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> is discussed. This variant accepts an</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">increased number of synapse updates by removing of redundant computations. In initial tests, it proved particularly</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">advantageous for networks in which neurons had a large in-degree. Therefore, this alternative integration scheme can be</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">expected to perform well for training on more complex datasets. Regrettably, it was not available in time for this</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">thesis, so potential gains remain speculative for now.</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">Another possible improvement is approximating the plasticity rule with the instantaneous error at the time of a spike.</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">This would eliminate the requirement for both frequent updates, <span class="highlight" title="Probable usage error. Use 'and' after 'both'.. Suggestions: [and] (7574) [lt:en:BOTH_AS_WELL_AS]">as well as</span> for storing and reading a history of</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">dendritic error. Thus, a network employing this simplified plasticity rule would be much less computationally costly. As</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">shown in Fig. \ref{fig-error-comp-le}, error terms in LE networks relax after only a few simulation steps. Therefore,</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">under the condition that input remains static throughout, this crude approximation is expected to perform fairly well.</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">\noindent The neuron model should likewise be investigated for potential improvements in terms of efficiency. Modeling</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Interneurons] (7969) [lt:en:UPPERCASE_SENTENCE_START]">interneurons</span> without an apical compartment might yield some improvements (although initials experiments have dampened</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">expectations for this). It is also possible, that the network does not require integration <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [time steps] (8178) [lt:en:MORFOLOGIK_RULE_EN_US]">timesteps</span> as low as $0.1ms$,</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">which has not been investigated yet. \newline</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline"><span class="keyword1">\subsubsection</span>*{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Meromorphic, Neurotrophic] (8252) [lt:en:MORFOLOGIK_RULE_EN_US]">Neuromorphic</span> hardware}</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">A prospect which likely would vastly improve simulation speed is a full re-implementation of the model on <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (8381) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>neuromorphic</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">hardware. This network fits the self-described niche of such systems almost perfectly; It employs strictly local</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">plasticity rules, its nodes use leaky membrane dynamics and communicate through binary spikes. By a rough estimation,</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">even the first generation of Intel's <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (8662) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> chips \citep{davies2018loihi} should be capable of simulating this neuron</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">model. The chip is capable of modelling multiple dendritic trees per neuron, and the learning engine appears capable of</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">communicating the dendritic error to all <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [synapses It] (8883) [lt:en:MORFOLOGIK_RULE_EN_US]">synapses\footnote{It</span> is possible that the learning rule would need to be</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">approximated somewhat for <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (8972) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> 1. The publicly available information about the follow-up chip <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (9041) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> 2</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">\citep{Davies2021} is still somewhat sparse, but it claims to support a much more diverse set of learning rules<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (9145) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Of</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">course, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Lois, Luigi, Loire, Loki, Lori, Loci, Lodi, Loin, Loins, Loewi, LHI, LIH, LOI, OHI, Lii, Lvii, Lxii, Joshi, LOIs, Loïc, Sochi, Lo-fi] (9159) [lt:en:MORFOLOGIK_RULE_EN_US]">Loihi</span> is only one of many <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (9185) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphic</span> systems. Another very promising system is</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline"><span class="keyword1">\textit</span>{BrainScaleS-2}, which appears to be spearheading the field with regard to simulating segregated dendrites</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">\citep{Kaiser2022}. Regardless of the system used, the novel field of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (9400) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphic</span> hardware might be capable of</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">reducing the high computational cost which currently obstructs further research into the model.</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline"><span class="keyword1">\subsection</span>*{Network}</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">Oversized networks have a higher tendency of failing on some tasks (results not shown). This issue is not consistent,</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">but plagued some of my experiments with no satisfactory explanation yet found.</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">When injecting large currents (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [fun, fund, funk, UNC, FNC, FLNC] (9787) [lt:en:MORFOLOGIK_RULE_EN_US]">func</span> approx experiments), the network kinda <span class="highlight" title="This word is considered offensive. (9831) [lt:en:PROFANITY]">shits</span> itself. Somewhat</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">expected, somewhat annoying</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 2 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span>{Future directions}</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">\<span class="highlight-sh" title="This subsection is very short (about 137 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">subsection</span></span>{Additional <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (9916) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> concerns}</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">from \citep{Marblestone2016} on one-shot learning</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">Additionally, the nervous system may have a way of quickly</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">storing and replaying sequences of events. This would allow</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">the brain to move an item from episodic memory into a long-</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">term memory stored in the weights of a cortical network (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Hi, I, Jim, Jr, Jo, GI, JC, JD, JP, Pi, JM, Jib, Jig, AI, AJI, BJI, Bi, CI, Ci, DI, DJI, Di, EI, FI, FJI, HI, J, JA, JAI, JE, JIC, JIM, JIT, JNI, JO, JRI, JT, JU, JV, LI, Li, MI, NI, Ni, QI, RI, SI, Si, TI, Ti, UI, VI, WI, Xi, ZI, Ii, Jg, Mi, Oi, Vi, J i, JB, JCI, JJ, JK, JS, Jin, Jio, Yi, Ai] (10203) [lt:en:MORFOLOGIK_RULE_EN_US]">Ji</span> and</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline">Wilson, 2007), by replaying the memory over and over. This</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">solution effectively uses many iterations of weight updating to</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">fully learn a single item, even if one has only been exposed to</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">it once. Alternatively, the brain could rapidly store an episodic</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">memory and then retrieve it later without the need to perform</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">slow gradient updates, which has proven to be useful for</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">fast reinforcement learning in scenarios with limited available</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">data (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Blunder, Blunders, Bluebell, Blondel, Lindell] (10652) [lt:en:MORFOLOGIK_RULE_EN_US]">Blundell</span> et al., 2016)</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">Where do targets come from? \cite{Bengio2015}</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 92 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This section is very short (about 93 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span>{Correspondence of the final model to cortical circuitry}</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">Cortical neurons depend on <span class="highlight-spelling" title="Possible spelling mistake found. (10795) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromodulation</span> too \cite{Roelfsema2018}</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">Completely unclear whether cortical circuits perform supervised learning \citep{magee2020synaptic}</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">This would probs be super efficient on <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (10938) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphics</span>!</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">I am not going to try plasticity with spike-spike or spike-rate dendritic errors</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">Training on <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Imagine, Imagined, Magnet, Imagines, Imogene] (11048) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>ImageNet</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline">Making this <span class="highlight" title="This word is considered offensive. (11070) [lt:en:PROFANITY]">shit</span> faster</div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PTT, BPT, BTT, BITT, BUTT, B PTT, BP TT, BPT T] (11083) [lt:en:MORFOLOGIK_RULE_EN_US]">BPTT</span>/time-continuous inputs</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline">The dendritic error rule at the core of <span class="highlight-spelling" title="Possible spelling mistake found. (11153) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> looks absurdly similar to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [super spike, supers pike] (11194) [lt:en:MORFOLOGIK_RULE_EN_US]">superspike</span> \citep{Zenke2018}.</div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">Someone should look into that!</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">Different configurations for which synapses are plastic should be elaborated on.</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">subsection</span>{Neuron model}</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">The neuron models employed in the network can be improved in several ways. Note, that this perspective is not made with</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">learning performance in mind, but rather with increased correspondence to the cortical circuitry.</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">Two properties that are part of most spiking neuron models, but have not been investigated here, are membrane reset and</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">refractory periods. The neuron model contains a mechanism for <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [refractories] (11743) [lt:en:MORFOLOGIK_RULE_EN_US]">refractoriness</span> after a spike, yet no experiments regarding</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">the impact of this feature on learning have yet been performed. As a natural extension of this, resetting the membrane</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">potential after eliciting a spike could likewise improve biological plausibility with simple changes to the model. These</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">two changes together would change the spike generation process to that of a stochastic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [IF, LIFE, LIE, LIFT, LID, LIP, LIN, LIZ, AIF, LEIF, LIB, LIFO, RIF, LIT, LIQ, BIF, CIF, DIF, FIF, GIF, HIF, IIF, LBF, LCF, LDIF, LEF, LFI, LGF, LI, LIA, LIH, LIM, LIR, LIX, LOF, LPF, LRF, LSF, LTF, LUF, MIF, NIF, PIF, WIF, ZIF, LIEF, LII, L IF, LI F, L&amp;F, LDF, LF, LIC, LIU, LIV, TIF] (12129) [lt:en:MORFOLOGIK_RULE_EN_US]">LIF</span> neuron. These neurons have</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">previously performed well for modelling sensory representations in the cortex \citep{Pillow2008}. Another neuron</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">property considered in that study is <span class="keyword1">\textit</span>{spike-frequency-adaptation}. Neurons with this mechanism increase their</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">threshold potential in response to previous activity. Such adaptability has been observed in $\tilde 20 \% $ of neurons</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">in the mouse visual cortex \citep{allen2018}, and has been shown to lead to significant performance increases in</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">recurrent SNN \citep{bellec2018long,bellec2020solution}. A neuron model capable of this has already been implemented in</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">NEST, so extending the present model in this regard should be feasible. These three changes together would vastly</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">improve correspondence of the neuron model to physiological insights. If successful, learning with such a model would</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">lend further support to the model's claim of biological plausibility.</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Pyr] (12961) [lt:en:UPPERCASE_SENTENCE_START]">pyr</span> and <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [into, inn, INT, INTG, int, in tn, INTJ, ITN] (12969) [lt:en:MORFOLOGIK_RULE_EN_US]">intn</span> are <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [way, ways, wary, wavy, waxy, AYY] (12978) [lt:en:MORFOLOGIK_RULE_EN_US]">wayy</span> to similar</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Reward-modulated] (12995) [lt:en:UPPERCASE_SENTENCE_START]">reward-modulated</span> <span class="highlight-spelling" title="Possible spelling mistake found. (13012) [lt:en:MORFOLOGIK_RULE_EN_US]">urbanczik-senn</span> plasticity?</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Wtf] (13040) [lt:en:UPPERCASE_SENTENCE_START]">wtf</span> is shunting inhibition?</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">Consider neurogenesis deeper. Any network that is plausible must be able to develop in a plausible fashion. Investigating</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">how the cortex develops might hold insights into plausible connectivity schemes. This does not necessarily compete or</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline">conflict with looking at connectivity of developed brains</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline"><span class="highlight-sh" title="A section title should start with a capital letter. [sh:001]"></span>\<span class="highlight-sh" title="This subsubsection is very short (about 50 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsubsection</span>{improve prospective activity with regard to spike creation}</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Le] (13429) [lt:en:UPPERCASE_SENTENCE_START]">le</span> does a lot, particularly at hidden layers. Yet response time under spiking paradigms is still</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">lackluster. In particular, prospective activation does next to nothing for these very low input time</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">constants. SNN performs best when $\tau_x$ is greater than $\Delta_t$ by roughly x10.</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">\<span class="highlight-sh" title="This subsection is very short (about 4 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This subsection is very short (about 4 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsection</span></span>{Improving efficiency}</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 0 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This section is very short (about 0 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span></span></span>{Conclusion}</div><div class="clear"></div>
</div>
<h2 class="filename">thesis.tex</h2>

<p>Found 91 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline"><span class="keyword1">\documentclass</span>[11pt,a4paper,titlepage]{report}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline"><span class="keyword1">\usepackage</span>[english]{babel}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline"><span class="keyword1">\usepackage</span>[utf8]{inputenc}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline"><span class="keyword1">\usepackage</span>[table,xcdraw]{xcolor}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline"><span class="keyword1">\usepackage</span>{graphicx}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline"><span class="keyword1">\usepackage</span>{subcaption}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">\graphicspath{ {./images/} }</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline"><span class="keyword1">\usepackage</span>[font=small,labelfont=bf]{caption}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline"><span class="keyword1">\usepackage</span>{listings}</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline"><span class="keyword1">\usepackage</span>{amsmath}</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline"><span class="keyword1">\usepackage</span>{amssymb}</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline"><span class="keyword1">\usepackage</span>{multirow}</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline"><span class="keyword1">\usepackage</span>[round]{natbib}</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">\bibliographystyle{apalike}</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline"><span class="keyword1">\usepackage</span>[onehalfspacing]{setspace}</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline"><span class="keyword1">\usepackage</span>{etoolbox}</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">\AtBeginEnvironment{quote}{\par\singlespacing\small}</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline"><span class="keyword1">\usepackage</span>[top=100pt,bottom=100pt,left=75pt,right=75pt]{geometry}</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline"><span class="keyword1">\usepackage</span>{enumitem}</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline"><span class="keyword1">\usepackage</span>{hyperref}</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline"><span class="keyword1">\usepackage</span>[noabbrev]{cleveref}</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline"><span class="keyword1">\usepackage</span>{tabularx}</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline"><span class="keyword1">\usepackage</span>[ngerman]{datetime}</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">\DeclareSymbolFont{letters}{OML}{ztmcm}{m}{it}</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">\DeclareSymbolFontAlphabet{\mathnormal}{letters}</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">\pagestyle{headings}</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">\addto<span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>senglish{\renewcommand{\figurename}{Fig.}</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">\hypersetup{</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">    colorlinks=true,</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">    linkcolor=black,</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">    citecolor=black,</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">    urlcolor=blue,</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">}</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">\newcommand*\ttvar[1]{\texttt{\expandafter\dottvar\detokenize{#1}\relax}}</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">\newcommand*\dottvar[1]{\ifx\relax#1\else</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">  \expandafter\ifx\string_#1\string_\allowbreak\else#1\fi</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">  \expandafter\dottvar\fi}</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">\definecolor{codegreen}{rgb}{0,0.6,0}</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">\definecolor{codegray}{rgb}{0.5,0.5,0.5}</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">\definecolor{codepurple}{rgb}{0.58,0,0.82}</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">\definecolor{backcolour}{rgb}{0.95,0.95,0.92}</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">\lstdefinestyle{mystyle}{</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">    backgroundcolor=\color{backcolour},   </div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">    commentstyle=\color{codegreen},</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">    keywordstyle=\color{magenta},</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">    numberstyle=\tiny\color{codegray},</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">    stringstyle=\color{codepurple},</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">    basicstyle=\ttfamily\footnotesize,</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">    breakatwhitespace=false,         </div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">    breaklines=true,                 </div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">    captionpos=b,                    </div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">    keepspaces=true,                 </div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">    numbers=left,                    </div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">    numbersep=5pt,                  </div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">    showspaces=false,                </div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">    showstringspaces=false,</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">    showtabs=false,                  </div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">    tabsize=2</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">}</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">\lstset{style=mystyle}</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">\newcommand{\what}[1] {\textcolor{red}{<span class="keyword1">\textbf</span>{#1} \addcontentsline{toc}{subsection}{\textcolor{orange}{#1}}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">\newcommand{\todo}[1] {\textcolor{orange}{<span class="keyword1">\textbf</span>{TODO: #1} \addcontentsline{toc}{subsection}{\textcolor{orange}{#1}}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">\newcommand{\citeme}{\textcolor{orange}{<span class="keyword1">\textbf</span>{TODO: cite}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">\newcommand{\phrasing}{\textcolor{green}{<span class="keyword1">\textbf</span>{phrasing}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">\newcommand{\intro}{\textcolor{blue}{<span class="keyword1">\textbf</span>{introduce term?}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">\newdateformat{myformat}{\THEDAY{. }\monthnamengerman[\THEMONTH] \THEYEAR}</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline"><span class="keyword2">\begin{document}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"><span class="keyword1">\title</span>{<span class="keyword1">\textbf</span>{<span class="highlight-spelling" title="Possible spelling mistake found. (1) [lt:en:MORFOLOGIK_RULE_EN_US]">Philipps-Universität</span> Marburg}}</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">\date{\parbox{\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [line width] (32) [lt:en:MORFOLOGIK_RULE_EN_US]">linewidth}{</span>\centering<span class="comment">%</span></div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (52) [lt:en:MORFOLOGIK_RULE_EN_US]">Fachbereich</span> 17\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (67) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">    AG <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Allegiance] (82) [lt:en:MORFOLOGIK_RULE_EN_US]">Allgemeine</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [UND, and, end, fund, UNC, undo, CND, DND, HND, UNB, Ind, 2nd, AND, BND, END, GND, LND, ND, Nd, OND, PND, SND, TND, UAD, UCD, UED, UFD, UMD, UN, UNF, UNH, UNI, UNL, UNM, UNR, UNT, UNV, UPD, URD, USD, VND, ind, uni, DnD, IND, UHD, UID, UNDP, UNO, in, on, an, any, had, one, under, used, band, did, found, no, up, us, use, UK, land, led, old, round, run, June, find, hand, mid, red, runs, sound, unit, CD, DVD, God, add, aid, bad, bound, ends, funds, gun, guns, kind, mind, send, tend, wind, ad, bed, bid, bond, fed, fun, god, odd, punk, sand, sun, sung, tune, Andy, CNN, HD, Land, Ltd, PhD, RNA, aunt, hung, hunt, ink, lung, mud, pond, pound, rod, sued, sunk, ups, wound, Bond, Hunt, ID, LCD, LED, SD, TNA, TNT, Ted, UHF, USA, USB, UTC, UV, bend, bind, dad, fond, funk, hid, hind, id, inn, kid, lend, lid, mad, mod, mound, nun, nuns, nut, pad, pod, pun, punt, rid, sad, tuned, unto, Ana, BNP, BSD, Bud, CCD, CNS, DNS, INS, JD, Jun, Jung, Juno, LSD, Luna, NAD, NP, PD, Rand, SNL, SNP, Sand, UA, UDP, URL, USDA, Unix, Urdu, ant, cod, dune, fend, junk, nod, nu, quad, quid, tuna, urn, wed, ASD, CNC, CNG, CNR, CNT, FD, GNP, HUD, Hung, Indy, Judd, LD, LNG, PNC, PNG, PNP, PPD, PSD, Pound, RCD, Sung, Suns, UAW, UCL, UDF, UE, UPC, UPI, UPS, USF, USO, USP, WMD, bud, bun, bunk, dunk, fad, hound, lad, med, mend, puns, rune, rung, sod, um, undue, urns, wand, AD, ADD, APD, BCD, BMD, BPD, CFD, CKD, CNE, CSD, DNR, ESD, Enid, FWD, GED, HDD, HNL, Hurd, IED, Jed, Lind, MCD, MHD, MNA, MVD, NPD, NY, PNA, PNL, PNS, QED, RFD, SBD, SNA, SNC, SVD, TLD, TNG, Tod, UCA, UDC, UDR, UNIDO, UUP, WD, buns, bunt, curd, dud, jun, rind, suns, uh, ulna, unwed, ACD, BNC, CNH, CNI, CVD, DDD, DMD, DPD, DSD, FNC, Fundy, GMD, IUD, Ina, Kurd, MNP, MNS, NDB, PGD, PNM, SNF, TDD, UAR, UNEF, USG, Uzi, VN, WNW, Zuni, cued, hued, hunk, inf, mung, rad, rand, undid, DCD, MLD, RNG, SGD, UAM, URB, USNO, Wed, Zn, bung, cad, cud, duns, hod, nub, puny, rend, runt, tad, TBD, bod, gnu, gunk, nus, rued, ted, tuns, ugh, unfed, upend, vend, ETD, YTD, Ann, DNA, GNU, MD, Ned, TD, UFO, USC, USS, pend, UNDRO, cpd, 1D, 2D, 3D, 3rd, 4D, 8D, AAD, ABD, AED, AFD, AGD, AHD, AID, AKD, ALD, AMD, AN, ANB, ANC, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANQ, ANR, ANS, ANT, ANV, ANW, ANX, ANY, ANZ, AOD, ARD, ATD, AUD, AUN, AWD, AXD, AYD, AZD, BBD, BD, BDD, BED, BFD, BGD, BHD, BID, BKD, BLD, BNA, BNB, BNE, BNF, BNG, BNI, BNJ, BNK, BNL, BNM, BNN, BNO, BNQ, BNR, BNS, BNT, BNU, BNV, BNW, BNY, BNZ, BOD, BRD, BUD, BVD, BWD, BXD, BZD, Bend, CAD, CBD, CDD, CED, CHD, CID, CLD, CMD, CN, CNA, CNB, CNDP, CNDS, CNED, CNF, CNJ, CNK, CNL, CNM, CNO, CNP, CNQ, CNU, CNV, CNW, CNX, CNY, CNZ, COD, CPD, CRD, CUD, Cd, Cid, Cod, D, D2D, DBD, DD, DGD, DID, DN, DNB, DNF, DNK, DNL, DNT, DOD, DUD, DZD, Dunn, EAD, EDD, EED, EGD, EID, ELD, EMD, EN, ENC, ENE, ENF, ENG, ENH, ENI, ENL, ENM, ENP, ENS, ERD, EUN, Ed, Eng, FCD, FED, FFD, FID, FJD, FN, FNA, FNB, FNE, FNG, FNO, FNQ, FNR, FNS, FNT, FOD, FPD, FRD, FSD, FUD, Fed, GAD, GDD, GN, GNC, GNF, GNT, GNV, GRD, GYD, Gd, HKD, HMD, HNE, HNF, HNR, HNS, HPD, Hun, Huns, IAD, ICD, IFD, IMD, IN, INA, INB, INC, INE, INED, INF, ING, INH, INM, INP, INR, INT, IPD, IRD, ISD, IVD, In, Inc, JED, JLD, JMD, JNA, JNE, JNI, JNT, JWD, KED, KN, KNA, KNB, KNK, KNM, KNO, KNU, KNX, KOD, KPD, KTD, KWD, LBD, LDD, LLD, LMD, LN, LN2, LNA, LNB, LNE, LNH, LNI, LNO, LNR, LNT, LPD, LRD, LTD, LVD, LYD, Ln, MAD, MDD, MED, MEND, MGD, MID, MKD, MN, MN4, MNB, MNE, MNG, MNK, MNL, MNT, MNW, MOND, MPD, MSD, MTD, Md, Mn, N, NB, NC, NDF, NDH, NDK, NDR, NDS, NDT, NE, NED, NF, NG, NH, NI, NID, NJ, NK, NL, NLD, NM, NNI, NNN, NO, NR, NS, NT, NTD, NU, NV, NW, NZ, NZD, Na, Nb, Ne, Ni, No, Np, OAD, OCD, OD, ODD, OED, OGD, ON, ONB, ONE, ONF, ONG, ONM, ONP, ONS, ONT, ONU, ORD, OSD, Ono, Ont, PCD, PDD, PED, PFD, PHD, PID, PKD, PLD, PNB, PNE, PNF, PNH, PNI, PNK, PNN, PNR, PNT, PNUD, PNV, PNW, PRD, PUN, PUPD, PVD, Pd, QDD, QNE, QNH, QNJ, RAD, RBD, RD, RED, RHD, RJD, RKD, RLD, RN, RN6, RNB, RNE, RNF, RNN, RNO, RNR, RNS, RNT, RNU, RPD, RTD, RUD, RUN, RWD, Rd, Rn, Rod, SAD, SCD, SDD, SHD, SID, SN, SNB, SNE, SNG, SNI, SNR, SNS, SNU, SNV, SOD, SRD, STD, SUD, SYD, Sid, Sn, Sun, TAD, TCD, TED, TGD, THD, TID, TKD, TMD, TN, TNB, TNC, TNE, TNF, TNK, TNL, TNM, TNN, TNO, TNP, TNR, TNS, TSD, TTD, TUN, TWD, Tad, U, U10, U11, U12, U13, U14, U15, U16, U17, U18, U19, U20, U21, U22, U23, U30, U31, U32, U37, U38, U39, U40, U41, U42, U43, U44, U45, U46, U47, U49, U50, U51, U54, U55, U56, U57, U58, U59, U60, U61, U62, U63, U64, U65, U66, U67, U68, U69, U70, U71, U72, U73, U74, U75, U76, U77, U78, U79, U80, U83, U87, U96, U99, UAA, UAB, UAC, UAE, UAG, UAH, UAI, UAL, UAP, UAS, UAU, UB4, UBB, UBC, UBS, UBV, UBX, UC, UCAD, UCB, UCC, UCF, UCM, UCU, UDB, UDDM, UDL, UDM, UDS, UEC, UEM, UER, UES, UET, UFA, UFC, UFE, UFF, UFG, UFI, UFP, UFR, UFS, UFW, UG, UGA, UGB, UGC, UGE, UGG, UGI, UGM, UGS, UGT, UGU, UGX, UHCD, UI, UIA, UIC, UIG, UIL, UIM, UINL, UIO, UIP, UIR, UIT, UIZ, UJA, UJM, UKR, UL, ULC, ULG, ULK, ULM, ULR, ULT, ULX, UMA, UMB, UMF, UMH, UMI, UML, UMM, UMP, UMR, UMS, UMT, UNAC, UNAF, UNIL, UNIS, UNIT, UNIX, UNODC, UNPO, UNSA, UO, UO2, UOB, UP, UP1, UPA, UPB, UPE, UPF, UPK, UPN, UPP, UPR, UPU, UPV, UQC, UQO, UQR, URA, URE, URG, URI, URN, URS, URT, URV, URY, US, USE, USI, USJ, USN, USR, UT, UT1, UTA, UTF, UTG, UTL, UTP, UTV, UUA, UUB, UUC, UUG, UUID, UUO, UUS, UUU, UVA, UVB, UVF, UVM, UVT, UWA, UWC, UWP, UWW, UY, UYF, UZ, UZB, Ufa, Ut, Ute, VD, VDD, VGD, VHD, VNC, VNE, VNM, VNU, VRD, VSD, WAD, WNS, WNT, WSD, X2D, XCD, XNA, YHD, ZAD, ZED, ZNP, ang, ans, d, dd, dun, dung, ed, en, enc, ens, fwd, gad, inc, ins, int, kn, ltd, mun, n, nude, pd, pud, rd, std, tn, tun, turd, u, uhf, ult, ump, unbid, unis, univ, usu, wad, wend, yd, yid, zed, 'n', A&amp;D, BYD, Bundy, D&amp;D, DNC, DNI, DNP, DTD, DoD, ECD, ENA, ENT, EOD, FHD, FMD, FNL, FTD, GID, GNI, GTD, GUID, HRD, HWD, IBD, INI, JVD, KD, L&amp;D, LOD, Lundy, MNC, NAND, NCD, NDA, NDC, NDP, NHD, NWD, ONR, PMD, PN, Pune, QLD, Quid, R&amp;D, RNC, RNZ, Rudd, SPD, SSD, Syd, TNW, UAN, UAT, UAV, UBA, UCI, UCP, UCR, UCS, UCSD, UDA, UHC, UIs, ULB, UMC, UNAM, UNSC, UNSW, UOM, UOS, UPT, UPnP, USDT, UTI, UTM, UVC, UW, UX, Udo, Ulm, Urs, VVD, VoD, XD, XSD, ZD, Zuñi, bundy, cmd, env, gtd, kN, nm, né, uhh, uhm, umm, undos, vid] (93) [lt:en:MORFOLOGIK_RULE_EN_US]">und</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Biological, Biologist, Biologists, Biologic, Biologics] (97) [lt:en:MORFOLOGIK_RULE_EN_US]">Biologische</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Psychologies, Psychologic, Psychologize] (109) [lt:en:MORFOLOGIK_RULE_EN_US]">Psychologie</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (121) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">    AE <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Theoretical, Theoretic] (136) [lt:en:MORFOLOGIK_RULE_EN_US]">Theoretische</span> <span class="highlight-spelling" title="Possible spelling mistake found. (149) [lt:en:MORFOLOGIK_RULE_EN_US]">Kognitionswissenschaft</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (172) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (185) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (198) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">    Learning in cortical microcircuits with multi-compartment pyramidal neurons\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (286) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (299) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (312) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (325) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">    <span class="keyword1">\textbf</span>{<span class="highlight-spelling" title="Possible spelling mistake found. (337) [lt:en:MORFOLOGIK_RULE_EN_US]">Masterarbeit}</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (350) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [our, fur, bur, cur, Eur, AUR, BUR, CUR, EUR, MUR, PUR, RUR, SUR, ZAR, ZCR, ZRR, ZUP, Zr, Zug] (362) [lt:en:MORFOLOGIK_RULE_EN_US]">zur</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Erlangen] (366) [lt:en:MORFOLOGIK_RULE_EN_US]">Erlangung</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [DES, yes, does, dies, desk, Del, den, DEC, DNS, Dec, Dee, dues, dyes, eds, DMS, DSS, DTS, dens, dew, DDS, deg, odes, Debs, DPs, DVS, ides, debs, Xes, dds, AES, BES, CES, DCS, DE, DEA, DEB, DECs, DEG, DEI, DEL, DEM, DEP, DEQ, DESE, DET, DEU, DFS, DGS, DHS, DOS, DPS, DRS, DS, DUS, Dem, Dis, ES, Es, FDES, FES, GES, HES, IDES, IES, KES, LES, Les, MES, PES, RES, SES, TES, UES, VES, deb, def, dis, dos, hes, mes, res, d es, DBS, DEX, DJs, DLS, DMs, Dems, Desi, DoS, IDEs, dep, deps, dev, devs, dms, is, was, as, his, has, he, we, be, get, her, me, new, day, did, due, its, set, us, use, best, days, died, do, few, led, less, men, see, west, West, deal, gas, idea, let, lies, news, red, rest, sea, ten, uses, yet, Best, DVD, Dr, Los, bus, dates, dead, deep, die, disc, dog, dry, ex, eyes, gets, goes, hey, ideas, key, leg, ones, sees, sets, sides, test, web, EU, Jews, Lee, ages, bed, codes, deals, debt, deck, diet, disk, dogs, dress, drew, duo, fed, fee, fees, jet, legs, met, net, tea, ties, vs, wet, Ben, CDs, Dan, Dean, Mrs, adds, ads, beds, debts, deer, demo, dense, deny, dish, dose, duel, duet, dust, edges, keys, lens, modes, nest, nodes, odds, pen, per, pet, rides, seas, tens, ups, CSS, DVDs, Drew, GPS, LED, NHS, PCs, Rev, SAS, Ted, Web, Wei, axes, bee, bees, bet, cues, dad, dam, dams, dash, deaf, dean, decks, deed, deeds, demos, dig, doses, dot, dots, dub, dug, dunes, dye, hides, jets, lbs, lets, mesh, mess, nets, owes, pets, ref, toes, ACS, Andes, BBS, CNS, DA, DDR, DDT, DL, DM, DRM, DSM, DSO, DSP, DT, Denis, Devi, ESA, ESP, Eden, FRS, Feb, Gus, IEC, INS, IRS, Jew, Mel, Mesa, NEC, OEM, PET, RMS, RSS, RTS, Rex, SBS, SMS, SOS, Zen, Zeus, aces, aides, apes, beg, begs, bets, dB, dear, diets, dim, din, dip, dives, domes, duets, dukes, dusk, dyed, fades, foes, gel, gem, gems, hers, lest, ores, pas, peas, pens, peso, pest, pies, sec, tides, vest, wed, yen, AEC, AEG, APS, Aden, Ames, Ares, Bess, CCS, CEA, CFS, CVS, DAC, DAT, DCC, DCI, DG, DLR, DMC, DMZ, DRC, DSC, DSL, DST, DTM, DUI, DUP, Danes, Dell, Deng, Doe, Dow, Dyer, EEA, EEC, ESC, ESL, FBS, FCS, Geo, Hess, IAS, IDs, IPS, Ives, Jess, KBS, Lea, Len, Lew, MTS, Meg, NEA, NPS, OAS, OCS, Oder, PDS, PRS, RCS, RDS, REM, Rep, SNES, TFS, TLS, TVs, TeX, Tess, UE, UPS, VMS, Yves, deem, deems, defy, desks, digs, dikes, dips, dons, doves, dries, dubs, duels, eel, eels, fen, fess, fest, gen, hen, hens, hex, hues, med, ms, nee, ode, ops, pea, peg, pegs, pep, pews, reds, rep, seq, teas, tee, vet, webs, wee, woes, AEF, AGS, AHS, BDS, BMS, CEP, CGS, CLS, DBA, DBMS, DCE, DCL, DCM, DKK, DLA, DLL, DME, DMG, DMK, DNR, DOA, DPI, DRG, DRI, DSA, DSB, DSE, DTC, DTI, DVR, DWM, Dawes, Dias, Diem, Dir, Dons, ESB, ESD, ESO, ESR, ESS, Esq, GED, GLS, HFS, HHS, HSS, Hades, IEA, IED, IEP, IHS, IVs, JCS, Jed, LCS, MBS, MDS, MMS, NBS, NEP, NFS, NLS, OE, PMS, PNS, PSS, QED, SCS, SLS, SRS, Sec, TSS, ales, byes, dais, dares, deans, deft, deism, deli, dell, dent, doc, doe, dud, duos, eh, esp, ewes, fens, gels, hem, jest, keg, kegs, mesa, noes, pew, pus, rec, rem, reps, resp, sew, tees, ts, vets, zest, BRS, CNES, DCB, DCF, DDA, DDB, DDC, DDD, DDE, DLM, DMD, DMP, DMV, DND, DOB, DPD, DPT, DSD, DSR, DTA, DTP, DVRs, DWI, DWT, Degas, ERS, ESE, Eu, Fez, GDS, GEF, GRS, Hus, JMS, KSS, LFS, LRS, MNS, Mex, PVS, Pres, RVs, VEB, abs, beys, dads, dales, dames, deist, dents, dept, dimes, dist, doer, doers, dupes, dz, fem, fps, gee, ices, lees, reg, rel, revs, secs, sues, tel, weds, wets, zen, DBC, DCD, DLI, DPN, DRV, DVP, DWG, Dena, ESN, Edens, HDS, Ines, LGS, NeWS, Nev, Odets, TEF, Wed, adzes, bps, dab, deeps, delis, dibs, dines, docs, doges, dost, dotes, dregs, dudes, duds, duh, duns, dyer, dyers, feds, gens, hems, hep, hew, idem, ifs, leis, lodes, mew, mews, mos, pres, refs, sews, sis, vies, wades, yews, BKS, Duse, EEO, Feds, Leos, PMs, TESL, Zest, ayes, bides, bodes, cedes, dabs, dells, dewy, dims, doles, dozes, ekes, eves, exes, hews, idles, jades, neg, nus, obs, rues, ted, vex, yeas, ODs, Tues, daces, degas, dices, dopes, doss, eek, ems, rems, roes, yep, CBS, CEO, DJ, DNA, FEMS, HMS, Leo, Ned, OKs, PS, SOs, USS, awes, dbl, dears, dob, doz, dpt, gees, leas, megs, mks, née, ohs, oms, rps, wens, Hts, Mmes, Odis, REMs, Zens, dazes, dins, lades, oles, teds, yens, AAS, ABES, ABS, ADBS, ADS, AE, AEA, AEB, AED, AEE, AEI, AEJ, AEK, AEL, AEM, AEN, AEO, AEP, AEQ, AER, AESA, AESV, AET, AEU, AEV, AEW, AEX, AEZ, AFS, AIS, AIs, AJS, AKS, ALS, AMS, ANS, AOS, AQS, ASS, ATS, AUS, AVS, AWS, AXS, AYS, AZS, Adas, As, Ats, BAES, BAS, BCS, BDE, BEA, BEB, BECS, BED, BEE, BEF, BEH, BEI, BEK, BEM, BEO, BEP, BEQ, BER, BEST, BET, BEU, BEV, BEW, BEX, BFS, BGS, BHS, BIS, BJS, BLS, BNS, BOS, BPS, BS, BSS, BTS, BVS, BWS, BXS, BYS, BZS, Be, CAES, CDE, CDS, CE, CEB, CEC, CED, CEE, CEF, CEG, CEI, CEJ, CEL, CEM, CEN, CEPS, CEQ, CER, CESD, CESE, CESI, CESR, CESS, CEST, CET, CEU, CEV, CHS, CJS, CKS, CMS, CPS, CQS, CRS, CS, CZS, Ce, Cs, D, D10, D11, D12, D13, D14, D15, D16, D17, D18, D19, D20, D21, D22, D23, D24, D25, D26, D27, D28, D29, D2D, D30, D31, D32, D33, D34, D35, D36, D37, D38, D39, D40, D41, D42, D45, D46, D48, D50, D51, D52, D55, D56, D57, D58, D59, D60, D61, D62, D63, D64, D65, D66, D67, D68, D70, D71, D72, D73, D74, D76, D78, D79, D8, D80, D82, D83, D86, D89, D90, D91, D96, D97, D99, DAA, DAB, DAE, DAEU, DAF, DAI, DAJ, DAK, DAL, DAR, DARES, DAU, DAV, DAX, DB7, DB9, DBB, DBCS, DBD, DBF, DBL, DBO, DBP, DBQ, DBR, DBU, DBZ, DC, DCA, DCK, DCN, DCO, DCP, DCR, DCT, DCU, DD, DDEA, DDH, DDI, DDL, DDM, DDO, DDSP, DDTs, DDV, DECT, DEEE, DEET, DELE, DELF, DEUG, DEXA, DFA, DFB, DFE, DFF, DFG, DFI, DFL, DFM, DFN, DFO, DFP, DFT, DFW, DGA, DGD, DGE, DGF, DGH, DGI, DGSE, DGSI, DGSN, DGT, DH, DHA, DHC, DHEA, DHEPS, DHH, DHI, DHM, DHP, DHR, DHT, DI, DIC, DID, DIF, DIL, DIN, DIP, DIR, DISA, DIV, DIVS, DIY, DJF, DJG, DJI, DJO, DJT, DJU, DK, DKB, DKP, DKR, DKZ, DLC, DLEM, DLF, DLG, DLH, DLP, DLT, DMF, DMI, DMM, DMO, DMR, DN, DNB, DNF, DNK, DNL, DNT, DO, DOC, DOD, DOE, DOF, DOI, DOJ, DOM, DON, DOP, DOT, DP, DPA, DPB, DPC, DPE, DPH, DPO, DPSD, DPU, DPW, DR, DR1, DRESG, DRH, DRP, DRT, DSF, DSG, DSI, DSK, DSN, DSQ, DSU, DSV, DTU, DTV, DTW, DUD, DVA, DVB, DVI, DWH, DWV, DXB, DXF, DXM, DXO, DZA, DZD, DZI, DZO, Day, Deon, Depp, Di, Dix, Don, Dot, Du, Dy, E, EAS, EBS, EC, ECS, EDS, EE, EED, EEE, EEG, EEK, EEM, EEP, EER, EFS, EG, EHS, EI, EIS, EK, ELS, EM, EN, ENS, EO, EPS, ER, ESF, ESH, ESI, ESM, EST, ET, ETS, EUS, EVS, EW, EWS, EX, EXS, EZ, EZS, Ed, Er, Esc, FADES, FDEVS, FDS, FEB, FEC, FED, FEF, FEI, FEM, FEN, FENS, FEP, FER, FET, FEVS, FFS, FLS, FMS, FMs, FNS, FPS, FS, FSS, Fe, Fed, GAS, GCS, GDE, GE, GEA, GEH, GEI, GEM, GEQ, GET, GEV, GFS, GGS, GHS, GIS, GMES, GMS, GTS, Ge, Gen, Ger, HBS, HCS, HDDS, HDE, HDPS, HEB, HEC, HEF, HEL, HER, HEV, HNS, HPS, HS, HTS, He, Heb, ID3S, IDE, IDMS, IDS, IE, IEF, IEG, IEM, IERS, IESN, IFS, IGS, IIS, IMS, INES, IOS, IRDES, IS, ISS, ITES, ITS, JBS, JDE, JE, JED, JEF, JEV, JHS, JLS, JWS, KDS, KE, KEA, KED, KEF, KEM, KEN, KER, KET, KHS, KKS, KMS, KOS, KS, KTS, Ken, Key, Ks, LDE, LE, LEA, LEB, LEC, LEF, LEJ, LEM, LEP, LER, LESZ, LHS, LMS, LS, LVS, Las, Le, Lesa, MAS, MCS, MDE, ME, MEA, MEB, MEC, MED, MEF, MEG, MEL, MEN, MEP, MER, MET, MFS, MGS, MKS, MOS, MPS, MRS, MS, MSS, MUS, Me, Ms, Mses, NAS, NCS, NDS, NE, NED, NEE, NEH, NEM, NEN, NER, NGS, NOS, NS, NSS, NUS, Ne, Neb, Neo, Nos, OBS, ODAS, ODEM, ODS, OEA, OEB, OEC, OED, OEP, OFS, OMS, ONS, ORS, OS, OSS, OSes, OVS, OWS, Odesa, Os, PAES, PAS, PBS, PCS, PE, PEA, PEB, PEC, PED, PEE, PEF, PEI, PEK, PEL, PEM, PEN, PENS, PEP, PEPS, PER, PESC, PEV, PFS, PGS, PHS, PIS, PKS, PLS, POS, PPS, PUS, Peg, Pei, Pen, QE2, QEV, RAS, RBS, RE, REA, REC, RED, REG, REI, REL, REP, RER, RERS, RESF, REV, REX, RFS, RGS, RHS, RIS, RLS, RNS, ROS, RPS, RRS, RS, Re, S, SDEI, SDFS, SDIS, SDPS, SE, SEA, SEAS, SEB, SEC, SEF, SEG, SEJ, SEK, SEL, SEM, SEO, SEP, SER, SESA, SET, SEU, SEV, SEX, SEY, SEZ, SFS, SGS, SIS, SNS, SPS, SS, SSS, STS, SUS, Se, Sen, Sep, Set, TBS, TDE, TDPS, TDS, TEB, TEC, TED, TEE, TEI, TEM, TEP, TEQ, TER, TESS, TEV, TGS, THS, TIS, TJS, TKS, TMS, TNS, TOS, TPS, TQS, TRS, TS, TTS, TVS, TWS, TZS, Te, Tet, Tex, UAS, UBS, UDPS, UDS, UEC, UED, UEM, UER, UESL, UET, UFS, UGS, UMS, URS, US, UUS, Utes, VBS, VE, VEI, VESA, VGS, VHS, VPS, VRS, VSS, VVS, WCS, WDS, WEC, WEG, WET, WGS, WHS, WMS, WNS, WPS, WSS, Wis, XEU, XMS, XPS, XS, Xe, YBS, YEC, YEM, YHS, ZED, ZEE, ZEF, ZEP, ZESN, Zs, ans, ares, ass, bey, bis, bxs, cis, cos, cps, cs, d, dag, dags, db, dd, dded, derv, div, dobs, doesn, don, dosh, dpi, drys, dun, dykes, e, eV, ea, ed, em, en, ens, er, est, fer, fey, fez, gs, hies, hoes, hos, hrs, iOS, ids, ins, ken, kens, ks, lea, lee, lei, ls, mas, mdse, meg, meh, mus, mys, nos, nudes, pecs, pee, pees, peps, pis, qts, ques, re, rev, rs, s, sades, sen, sens, sex, shes, tbs, ve, veg, wees, xis, ye, yea, yeps, yer, yest, yew, yrs, zed, zeds, zens, ADLs, AEST, AGs, APs, AVs, Adel, BEC, BJs, CAS, CCs, CDEC, CDNs, CDOs, CEMS, CEOs, CESC, CIS, CKs, CTS, CTs, CVs, D&amp;D, DAP, DAUs, DAW, DB, DDoS, DECA, DF, DFC, DFV, DGP, DHL, DJed, DKA, DLK, DMA, DMGs, DMT, DMX, DMZs, DMed, DNAs, DNC, DNI, DNP, DOH, DOL, DQ, DRA, DRMs, DTD, DTT, DUIs, DV, DVC, DVM, DVMs, DVT, DW, DWC, DX, DeVos, Demi, DnD, DoD, DoJ, EB, EF, EGS, EMS, EOS, EP, EPs, EQ, EQS, EQs, ES5, ES6, ESG, ESV, ESY, EV, EVs, EY, El, FAS, FCs, FHS, FOS, FPs, FTEs, GBS, GCs, GDPs, GIs, H&amp;S, HCs, HDDs, HEO, HLS, HWs, I&amp;S, IBS, ICEs, ICS, IDPs, IDed, IEPs, ILS, IMs, IPs, IQs, Inès, Inés, JDKs, JS, Jeb, Jem, Jen, Jens, KDE, KWS, LDCs, LDS, LEDs, LEQ, LEV, LMs, LPS, LPs, LTS, Léa, Léo, MCs, MDs, MEPs, MERS, MIS, MLS, MPs, MWS, MeV, Mets, NDAs, NDIS, NRS, NTS, NYS, ODMs, OEMs, PAs, PDFs, PDMS, PEO, PPEs, PRs, QBs, QMS, QS, QTS, RET, RTs, Rey, SDE, SDGs, SDKs, SDS, SEPs, SHS, SMEs, SOEs, SQS, T&amp;S, TAS, TCS, TECS, TEU, TEUs, TTs, TWEs, ToS, UCS, UIs, UOS, Urs, V6s, V8s, VCS, VCs, VDE, VFS, VMs, VPs, VWs, WBS, WS, XLS, XSS, ZDs, ZFS, ais, cts, dBm, dm, dmed, dox, doxes, drey, dreys, mDNS, meds, né, pHs, pcs, pgs, pls, pms, pts, req, rodes, ryes] (376) [lt:en:MORFOLOGIK_RULE_EN_US]">des</span> Master of Science<span class="highlight-sh" title="There should be a space after a period. If you are writing a URL or a filename, use the \url{} or \verb markup. [sh:d:002]"> (M.Sc</span>.)\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (406) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [I'm, IM, in, is, I, it, him, if, km, FM, am, cm, mm, IBM, Jim, PM, pm, rim, ID, IQ, id, DM, IMF, dim, IMG, IMO, VM, um, ICM, IH, IMA, IMC, IMT, IMU, IOM, JM, imp, BIM, aim, gm, vim, GM, IP, Kim, Tim, AIM, AM, Am, BM, CIM, CM, Cm, EM, FIM, Fm, HM, IA, IDM, IE, IEM, IF, IFM, IGM, IIM, IJ, IJM, IL, ILM, IMB, IMD, IME, IMH, IMK, IMN, IMP, IMS, IMV, IN, INM, IPM, IR, IRM, IS, ISM, IT, IV, IZ, Ia, In, Io, Ir, It, JIM, KM, LIM, M, MIM, MM, NM, OIM, OM, PIM, Pm, QM, RIM, RM, SIM, SM, Sm, TM, Tm, UIM, WIM, Wm, YM, ZIM, em, i, ii, ism, iv, ix, m, mi, om, rm, sim, 3M, IAM, IB, IC, IG, IMs, IU, LM, dm, hm, nm, vm, µm] (418) [lt:en:MORFOLOGIK_RULE_EN_US]">im</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [interdisciplinary] (421) [lt:en:MORFOLOGIK_RULE_EN_US]">interdisziplinären</span> <span class="highlight-spelling" title="Possible spelling mistake found. (440) [lt:en:MORFOLOGIK_RULE_EN_US]">Master-Studiengang</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (459) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">    <span class="keyword1">\textbf</span>{„<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Cognitive] (472) [lt:en:MORFOLOGIK_RULE_EN_US]">Kognitive</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [UND, and, end, fund, UNC, undo, CND, DND, HND, UNB, Ind, 2nd, AND, BND, END, GND, LND, ND, Nd, OND, PND, SND, TND, UAD, UCD, UED, UFD, UMD, UN, UNF, UNH, UNI, UNL, UNM, UNR, UNT, UNV, UPD, URD, USD, VND, ind, uni, DnD, IND, UHD, UID, UNDP, UNO, in, on, an, any, had, one, under, used, band, did, found, no, up, us, use, UK, land, led, old, round, run, June, find, hand, mid, red, runs, sound, unit, CD, DVD, God, add, aid, bad, bound, ends, funds, gun, guns, kind, mind, send, tend, wind, ad, bed, bid, bond, fed, fun, god, odd, punk, sand, sun, sung, tune, Andy, CNN, HD, Land, Ltd, PhD, RNA, aunt, hung, hunt, ink, lung, mud, pond, pound, rod, sued, sunk, ups, wound, Bond, Hunt, ID, LCD, LED, SD, TNA, TNT, Ted, UHF, USA, USB, UTC, UV, bend, bind, dad, fond, funk, hid, hind, id, inn, kid, lend, lid, mad, mod, mound, nun, nuns, nut, pad, pod, pun, punt, rid, sad, tuned, unto, Ana, BNP, BSD, Bud, CCD, CNS, DNS, INS, JD, Jun, Jung, Juno, LSD, Luna, NAD, NP, PD, Rand, SNL, SNP, Sand, UA, UDP, URL, USDA, Unix, Urdu, ant, cod, dune, fend, junk, nod, nu, quad, quid, tuna, urn, wed, ASD, CNC, CNG, CNR, CNT, FD, GNP, HUD, Hung, Indy, Judd, LD, LNG, PNC, PNG, PNP, PPD, PSD, Pound, RCD, Sung, Suns, UAW, UCL, UDF, UE, UPC, UPI, UPS, USF, USO, USP, WMD, bud, bun, bunk, dunk, fad, hound, lad, med, mend, puns, rune, rung, sod, um, undue, urns, wand, AD, ADD, APD, BCD, BMD, BPD, CFD, CKD, CNE, CSD, DNR, ESD, Enid, FWD, GED, HDD, HNL, Hurd, IED, Jed, Lind, MCD, MHD, MNA, MVD, NPD, NY, PNA, PNL, PNS, QED, RFD, SBD, SNA, SNC, SVD, TLD, TNG, Tod, UCA, UDC, UDR, UNIDO, UUP, WD, buns, bunt, curd, dud, jun, rind, suns, uh, ulna, unwed, ACD, BNC, CNH, CNI, CVD, DDD, DMD, DPD, DSD, FNC, Fundy, GMD, IUD, Ina, Kurd, MNP, MNS, NDB, PGD, PNM, SNF, TDD, UAR, UNEF, USG, Uzi, VN, WNW, Zuni, cued, hued, hunk, inf, mung, rad, rand, undid, DCD, MLD, RNG, SGD, UAM, URB, USNO, Wed, Zn, bung, cad, cud, duns, hod, nub, puny, rend, runt, tad, TBD, bod, gnu, gunk, nus, rued, ted, tuns, ugh, unfed, upend, vend, ETD, YTD, Ann, DNA, GNU, MD, Ned, TD, UFO, USC, USS, pend, UNDRO, cpd, 1D, 2D, 3D, 3rd, 4D, 8D, AAD, ABD, AED, AFD, AGD, AHD, AID, AKD, ALD, AMD, AN, ANB, ANC, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANQ, ANR, ANS, ANT, ANV, ANW, ANX, ANY, ANZ, AOD, ARD, ATD, AUD, AUN, AWD, AXD, AYD, AZD, BBD, BD, BDD, BED, BFD, BGD, BHD, BID, BKD, BLD, BNA, BNB, BNE, BNF, BNG, BNI, BNJ, BNK, BNL, BNM, BNN, BNO, BNQ, BNR, BNS, BNT, BNU, BNV, BNW, BNY, BNZ, BOD, BRD, BUD, BVD, BWD, BXD, BZD, Bend, CAD, CBD, CDD, CED, CHD, CID, CLD, CMD, CN, CNA, CNB, CNDP, CNDS, CNED, CNF, CNJ, CNK, CNL, CNM, CNO, CNP, CNQ, CNU, CNV, CNW, CNX, CNY, CNZ, COD, CPD, CRD, CUD, Cd, Cid, Cod, D, D2D, DBD, DD, DGD, DID, DN, DNB, DNF, DNK, DNL, DNT, DOD, DUD, DZD, Dunn, EAD, EDD, EED, EGD, EID, ELD, EMD, EN, ENC, ENE, ENF, ENG, ENH, ENI, ENL, ENM, ENP, ENS, ERD, EUN, Ed, Eng, FCD, FED, FFD, FID, FJD, FN, FNA, FNB, FNE, FNG, FNO, FNQ, FNR, FNS, FNT, FOD, FPD, FRD, FSD, FUD, Fed, GAD, GDD, GN, GNC, GNF, GNT, GNV, GRD, GYD, Gd, HKD, HMD, HNE, HNF, HNR, HNS, HPD, Hun, Huns, IAD, ICD, IFD, IMD, IN, INA, INB, INC, INE, INED, INF, ING, INH, INM, INP, INR, INT, IPD, IRD, ISD, IVD, In, Inc, JED, JLD, JMD, JNA, JNE, JNI, JNT, JWD, KED, KN, KNA, KNB, KNK, KNM, KNO, KNU, KNX, KOD, KPD, KTD, KWD, LBD, LDD, LLD, LMD, LN, LN2, LNA, LNB, LNE, LNH, LNI, LNO, LNR, LNT, LPD, LRD, LTD, LVD, LYD, Ln, MAD, MDD, MED, MEND, MGD, MID, MKD, MN, MN4, MNB, MNE, MNG, MNK, MNL, MNT, MNW, MOND, MPD, MSD, MTD, Md, Mn, N, NB, NC, NDF, NDH, NDK, NDR, NDS, NDT, NE, NED, NF, NG, NH, NI, NID, NJ, NK, NL, NLD, NM, NNI, NNN, NO, NR, NS, NT, NTD, NU, NV, NW, NZ, NZD, Na, Nb, Ne, Ni, No, Np, OAD, OCD, OD, ODD, OED, OGD, ON, ONB, ONE, ONF, ONG, ONM, ONP, ONS, ONT, ONU, ORD, OSD, Ono, Ont, PCD, PDD, PED, PFD, PHD, PID, PKD, PLD, PNB, PNE, PNF, PNH, PNI, PNK, PNN, PNR, PNT, PNUD, PNV, PNW, PRD, PUN, PUPD, PVD, Pd, QDD, QNE, QNH, QNJ, RAD, RBD, RD, RED, RHD, RJD, RKD, RLD, RN, RN6, RNB, RNE, RNF, RNN, RNO, RNR, RNS, RNT, RNU, RPD, RTD, RUD, RUN, RWD, Rd, Rn, Rod, SAD, SCD, SDD, SHD, SID, SN, SNB, SNE, SNG, SNI, SNR, SNS, SNU, SNV, SOD, SRD, STD, SUD, SYD, Sid, Sn, Sun, TAD, TCD, TED, TGD, THD, TID, TKD, TMD, TN, TNB, TNC, TNE, TNF, TNK, TNL, TNM, TNN, TNO, TNP, TNR, TNS, TSD, TTD, TUN, TWD, Tad, U, U10, U11, U12, U13, U14, U15, U16, U17, U18, U19, U20, U21, U22, U23, U30, U31, U32, U37, U38, U39, U40, U41, U42, U43, U44, U45, U46, U47, U49, U50, U51, U54, U55, U56, U57, U58, U59, U60, U61, U62, U63, U64, U65, U66, U67, U68, U69, U70, U71, U72, U73, U74, U75, U76, U77, U78, U79, U80, U83, U87, U96, U99, UAA, UAB, UAC, UAE, UAG, UAH, UAI, UAL, UAP, UAS, UAU, UB4, UBB, UBC, UBS, UBV, UBX, UC, UCAD, UCB, UCC, UCF, UCM, UCU, UDB, UDDM, UDL, UDM, UDS, UEC, UEM, UER, UES, UET, UFA, UFC, UFE, UFF, UFG, UFI, UFP, UFR, UFS, UFW, UG, UGA, UGB, UGC, UGE, UGG, UGI, UGM, UGS, UGT, UGU, UGX, UHCD, UI, UIA, UIC, UIG, UIL, UIM, UINL, UIO, UIP, UIR, UIT, UIZ, UJA, UJM, UKR, UL, ULC, ULG, ULK, ULM, ULR, ULT, ULX, UMA, UMB, UMF, UMH, UMI, UML, UMM, UMP, UMR, UMS, UMT, UNAC, UNAF, UNIL, UNIS, UNIT, UNIX, UNODC, UNPO, UNSA, UO, UO2, UOB, UP, UP1, UPA, UPB, UPE, UPF, UPK, UPN, UPP, UPR, UPU, UPV, UQC, UQO, UQR, URA, URE, URG, URI, URN, URS, URT, URV, URY, US, USE, USI, USJ, USN, USR, UT, UT1, UTA, UTF, UTG, UTL, UTP, UTV, UUA, UUB, UUC, UUG, UUID, UUO, UUS, UUU, UVA, UVB, UVF, UVM, UVT, UWA, UWC, UWP, UWW, UY, UYF, UZ, UZB, Ufa, Ut, Ute, VD, VDD, VGD, VHD, VNC, VNE, VNM, VNU, VRD, VSD, WAD, WNS, WNT, WSD, X2D, XCD, XNA, YHD, ZAD, ZED, ZNP, ang, ans, d, dd, dun, dung, ed, en, enc, ens, fwd, gad, inc, ins, int, kn, ltd, mun, n, nude, pd, pud, rd, std, tn, tun, turd, u, uhf, ult, ump, unbid, unis, univ, usu, wad, wend, yd, yid, zed, 'n', A&amp;D, BYD, Bundy, D&amp;D, DNC, DNI, DNP, DTD, DoD, ECD, ENA, ENT, EOD, FHD, FMD, FNL, FTD, GID, GNI, GTD, GUID, HRD, HWD, IBD, INI, JVD, KD, L&amp;D, LOD, Lundy, MNC, NAND, NCD, NDA, NDC, NDP, NHD, NWD, ONR, PMD, PN, Pune, QLD, Quid, R&amp;D, RNC, RNZ, Rudd, SPD, SSD, Syd, TNW, UAN, UAT, UAV, UBA, UCI, UCP, UCR, UCS, UCSD, UDA, UHC, UIs, ULB, UMC, UNAM, UNSC, UNSW, UOM, UOS, UPT, UPnP, USDT, UTI, UTM, UVC, UW, UX, Udo, Ulm, Urs, VVD, VoD, XD, XSD, ZD, Zuñi, bundy, cmd, env, gtd, kN, nm, né, uhh, uhm, umm, undos, vid] (482) [lt:en:MORFOLOGIK_RULE_EN_US]">und</span> Integrative <span class="highlight-spelling" title="Possible spelling mistake found. (498) [lt:en:MORFOLOGIK_RULE_EN_US]">Systemneurowissenschaften</span>“}\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (525) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (538) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (551) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">    <span class="keyword1">\textbf</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Retreat, Beirut, Beret, Bereft] (563) [lt:en:MORFOLOGIK_RULE_EN_US]">Betreut</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Dutch, Burch, dutch, lurch] (571) [lt:en:MORFOLOGIK_RULE_EN_US]">durch</span>:}\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (578) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">    Prof. Dr. Dominik <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Endures, Andres, Endues, End res, Andrés] (608) [lt:en:MORFOLOGIK_RULE_EN_US]">Endres</span>, <span class="highlight-spelling" title="Possible spelling mistake found. (616) [lt:en:MORFOLOGIK_RULE_EN_US]">Philipps-Universität</span> Marburg\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (645) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">    Prof. Dr. Johan <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Without] (673) [lt:en:MORFOLOGIK_RULE_EN_US]">Kwisthout</span>, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Radioed, Redbud] (684) [lt:en:MORFOLOGIK_RULE_EN_US]">Radboud</span> University Nijmegen\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (712) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (725) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">    \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (738) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Forelegs, Foreleg] (750) [lt:en:MORFOLOGIK_RULE_EN_US]">Vorgelegt</span> von Johannes <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Grille, Gill, Gills, Lille, Gillie, Gilles, Gilly, Guille] (773) [lt:en:MORFOLOGIK_RULE_EN_US]">Gille</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [engram] (779) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>endgraf</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [I'm, IM, in, is, I, it, him, if, km, FM, am, cm, mm, IBM, Jim, PM, pm, rim, ID, IQ, id, DM, IMF, dim, IMG, IMO, VM, um, ICM, IH, IMA, IMC, IMT, IMU, IOM, JM, imp, BIM, aim, gm, vim, GM, IP, Kim, Tim, AIM, AM, Am, BM, CIM, CM, Cm, EM, FIM, Fm, HM, IA, IDM, IE, IEM, IF, IFM, IGM, IIM, IJ, IJM, IL, ILM, IMB, IMD, IME, IMH, IMK, IMN, IMP, IMS, IMV, IN, INM, IPM, IR, IRM, IS, ISM, IT, IV, IZ, Ia, In, Io, Ir, It, JIM, KM, LIM, M, MIM, MM, NM, OIM, OM, PIM, Pm, QM, RIM, RM, SIM, SM, Sm, TM, Tm, UIM, WIM, Wm, YM, ZIM, em, i, ii, ism, iv, ix, m, mi, om, rm, sim, 3M, IAM, IB, IC, IG, IMs, IU, LM, dm, hm, nm, vm, µm] (791) [lt:en:MORFOLOGIK_RULE_EN_US]">im</span> Mai 2023</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">    }}</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">    <span class="keyword1">\maketitle</span></div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">    </div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">\noindent <span class="keyword1">\textbf</span>{Eidesstattliche Erklärung}</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">\noindent Ich versichere hiermit, dass ich die vorliegende Arbeit mit dem Titel\newline</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (826) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">\noindent„Learning in cortical microcircuits with multi-compartment pyramidal neurons“\newline</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (836) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">\noindent selbständig verfasst und keine anderen als die im Text angegebenen Hilfsmittel verwendet habe. \smallskip</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">\noindent Sämtliche Textstellen, die im Wortlaut oder dem Sinn nach anderen Werken entnommen wurden, sind mit einer</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (848) [lt:en:MORFOLOGIK_RULE_EN_US]">Quellenangabe</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Kennewick, Kennith, kenneling, gemütlich] (862) [lt:en:MORFOLOGIK_RULE_EN_US]">kenntlich</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [each, reach, beach, Beach, exact, teach, react, rematch, yacht, enact, Leach, Mach, Wehrmacht, detach, peach, Camacho, GMAC, leach, macho, Emacs, GMAT, gouache, mach, XEmacs, peachy, redact, Almach, CEMAC, EMAC, GEAAC, geocache, React, démarche] (872) [lt:en:MORFOLOGIK_RULE_EN_US]">gemacht</span>. Die <span class="highlight-spelling" title="Possible spelling mistake found. (885) [lt:en:MORFOLOGIK_RULE_EN_US]">Masterarbeit</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [were, sure, wide, word, words, murder, curve, guide, pure, wire, wore, Ward, burden, crude, cure, nurse, ward, wards, worse, Burke, cured, curse, lure, purse, surge, wired, Purdue, Urdu, Verde, Wade, Wilde, lured, purge, rude, urge, warden, ware, Jude, Ware, horde, hurdle, worded, Curie, Hurd, curd, dude, wade, Kurd, curds, suede, warded, wordy, puree, warder, curdle, curie, furze, gourde, prude, wurst, IRDE, URD, URE, nude, turd, turds, Kurds, Purdy, WPRE, Word, nurdle, purée] (898) [lt:en:MORFOLOGIK_RULE_EN_US]">wurde</span> in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [her, Dr, deer, per, Del, den, DDR, DEC, Dec, Dee, dear, DLR, Dyer, Oder, dew, DNR, DVR, Dir, deg, DSR, doer, dyer, AER, BER, CER, DAR, DBR, DCR, DE, DEA, DEB, DEG, DEI, DEL, DEM, DEP, DEQ, DES, DET, DEU, DHR, DIR, DKR, DMR, DR, Dem, EER, ER, Er, FER, Ger, HER, KER, LER, MER, NER, PER, RER, SER, TER, UER, deb, def, derv, er, fer, yer, DEX, dep, dev] (907) [lt:en:MORFOLOGIK_RULE_EN_US]">der</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [antigen, jetties, jetliner, jettison, Metzger, netizen] (911) [lt:en:MORFOLOGIK_RULE_EN_US]">jetzigen</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Oder, over, older, order, odor, ode, odes, coder, doer, odder, ODEM, ODR, o'er, one, or, her, other, under, does, modern, open, power, border, code, cover, ever, idea, lower, model, offer, our, Dr, door, mode, ones, opera, orders, owner, tower, user, beer, codes, elder, odd, outer, wider, Roger, boxer, deer, holder, lover, modes, node, nodes, odds, peer, per, powder, rider, rode, tier, voter, Del, Owen, Wonder, coded, den, obey, oven, owed, owes, pier, poker, wonder, Adler, Boer, DDR, DEC, Dec, Dee, Dover, Eden, GDR, Homer, OEM, Omar, Rover, Ryder, codec, codex, colder, dear, fodder, folder, homer, loader, loser, modem, mover, ores, otter, overt, owe, oxen, rodeo, rover, rower, sober, ADR, Aden, Boyer, CDR, DLR, Dyer, FDR, MDR, OPEC, Odin, Ogden, Opel, alder, cider, dew, foyer, hover, loner, louder, oar, omen, seer, solder, DNR, DVR, Dir, HDR, Holder, Nader, OCR, OE, SDR, UDR, Vader, adder, bolder, deg, doe, dour, dower, idler, joker, lodger, mower, odors, ogre, olden, ponder, toner, veer, DDE, DSR, Odell, Odom, Rodger, Seder, bier, bode, borer, bower, coders, comer, doers, goer, lode, ole, udder, wader, yonder, Adar, Odets, Orem, Oreo, cower, dodger, dyer, eider, gofer, hider, idem, jeer, lodes, oleo, roper, yodel, Olen, boded, bodes, dodder, edger, fonder, ides, leer, molder, ocher, osier, poser, roger, ruder, sower, ODs, codger, corer, goner, ocker, wooer, ceder, doper, ore, Odis, moper, oles, sorer, ADMR, AER, AFER, ATER, Amer, BDE, BDR, BER, CDE, CER, CFER, DAR, DBR, DCR, DDEA, DE, DEA, DEB, DEG, DEI, DEL, DEM, DEP, DEQ, DES, DET, DEU, DHR, DIR, DKR, DMR, DR, Dem, EER, ER, Er, FDES, FEDER, FER, FSER, GDE, GDPR, Ger, Godel, HDE, HER, IDE, IDES, IDR, IPER, JDE, KER, LDE, LER, MDE, MER, MODEM, NDLR, NDR, NER, OAE, OAR, OBE, OCDE, OD, ODAS, ODB, ODBC, ODC, ODD, ODF, ODG, ODI, ODM, ODMRP, ODN, ODP, ODS, OEA, OEB, OEC, OED, OEP, OGE, OGR, OKed, OLED, OMR, ONDAR, ONE, ONEM, ONERA, ONERC, OPEP, OR, OSEF, OSR, OSes, Odesa, Ore, Oreg, Orr, PER, QDR, RDAR, REER, RER, SDEI, SER, TDE, TDR, TER, UDSR, UER, aver, boner, coyer, dded, deb, def, derv, doter, er, ewer, fer, hoer, honer, nuder, ogler, overs, weer, yer, Acer, Adel, CDEC, Cher, DEX, EDPR, EDR, Godber, Gödel, Hofer, IDEs, IDed, Iker, KDE, ODHA, ODMs, ODT, OKR, ONR, OPEX, Oleg, Opera, Otero, Owler, PDR, SDE, UDHR, Uber, VDE, Zoner, aider, dep, dev, e'er, fader, oiler, opex, polder, rodes, suer] (920) [lt:en:MORFOLOGIK_RULE_EN_US]">oder</span> in <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [enricher] (928) [lt:en:MORFOLOGIK_RULE_EN_US]">ähnlicher</span> Form <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Noah, notch, Enoch, Foch, nosh, Koch, och, n och, no ch, Boch, NOC] (943) [lt:en:MORFOLOGIK_RULE_EN_US]">noch</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [BEI, Ba, be, bad, bar, ban, bay, bed, Ben, bag, bat, Wei, bee, bet, beg, ma, pa, ta, Pa, BRI, BWI, bah, baa, AEI, BBI, BCI, BDI, BEA, BEB, BED, BEE, BEF, BEH, BEK, BEM, BEO, BEP, BEQ, BER, BES, BET, BEU, BEV, BEW, BEX, BFI, BGI, BHI, BII, BJI, BKI, BLI, BMI, BNEI, BNI, BOI, BPI, BSI, BTI, BUI, BVI, BYI, Be, Bi, CEI, Ca, DEI, EI, FEI, GEI, Ga, Ha, Ia, La, Mai, Na, PEI, Pei, REI, Ra, TEI, Ta, VEI, Va, bap, bey, bi, ca, ea, fa, ha, la, lei, ya, be i, BEC, Kai, Tai, bam, tai, was, I, by, but, had, has, he, we, back, been, can, get, her, me, new, band, being, day, may, set, yes, best, few, law, led, main, man, men, said, see, wait, way, BBC, San, base, bye, car, far, gas, let, pay, ran, red, saw, sea, ten, war, yet, BC, Best, ball, bank, bass, beat, begin, big, bit, box, boy, bus, die, ex, fan, gain, hair, hey, key, laid, lay, leg, map, paid, pair, rail, say, semi, tail, tax, van, web, Bob, EU, FBI, Lee, baby, bars, beam, bear, beer, belt, bid, buy, cap, cat, eat, fair, fed, fee, gap, hat, heir, jail, jet, lap, lie, mail, met, net, pain, raid, rain, raw, ray, sat, tag, tea, tie, wet, BBQ, BMW, Bell, Dan, Jay, Mac, Max, Nazi, Ray, Sam, bare, bark, bats, beds, beef, bell, beta, bin, bow, bred, ear, fail, fat, jaw, lab, oak, pan, pen, per, pet, rap, sail, ski, taxi, vein, BMG, BTW, Ball, Bass, Del, Han, LED, Mar, Neil, PCI, Pat, Reid, Rev, Ted, Web, bags, bail, barn, bath, bays, beak, bees, bend, bent, bio, blew, bug, cab, dad, dam, den, gag, hail, hay, mad, maid, nail, obey, pad, par, pie, ram, rat, ref, sad, tan, tap, vain, wax, Abel, BBS, BMT, BNP, BSA, BSD, BT, BYU, Bach, Bart, Benz, Berg, Bern, Bert, Boer, Bud, CGI, CPI, CSI, Cain, DEC, Dec, Dee, Devi, Feb, GI, GUI, Ham, Hay, IEC, Jain, Jew, Kay, Lao, Levi, MRI, Mae, Mali, Mao, Mel, NEC, OEM, PET, PRI, Rex, Wii, Zen, bait, bald, bang, bans, bead, bean, begs, bets, bog, boil, bra, brig, cam, fax, gait, gel, gem, ham, hi, jar, lag, lair, mat, max, pas, pi, psi, sac, sap, sax, sci, sec, tab, tar, var, veil, vie, wed, yen, AEC, AEG, Abe, BFA, BMC, BMP, BPA, Baku, Bali, Barr, Bean, Beau, Bede, Benin, Bess, Bret, CBI, CCI, CEA, DCI, DUI, EEA, EEC, Fay, Gaia, Geo, Hui, Jedi, KBE, Kali, Lea, Len, Lew, Mani, Mari, Maui, Meg, Meir, NEA, Nair, Nam, Nat, OSI, PKI, PPI, Pam, REM, Rep, SGI, TDI, Tao, TeX, UE, UPI, WWI, baht, bake, bard, beet, beige, bop, brew, bud, bun, cal, dew, eel, fad, fen, gal, gen, hen, hex, lad, lax, maxi, med, moi, nee, oar, pal, paw, pea, peg, pep, rag, rein, rep, seq, tau, tee, vat, vet, wee, yam, yaw, AEF, BBB, BCD, BDS, BLT, BMA, BMD, BMO, BMS, BPD, BPM, BRM, BWF, Baal, Baez, Batu, Baum, Beck, Begin, Bela, Brit, CAI, CEP, CLI, CTI, Cali, DPI, DRI, DTI, FCI, FDI, FSI, GDI, GED, IEA, IED, IEP, Jan, Jed, Kari, LSI, Lat, Leif, MDI, MSI, Magi, NEP, OE, PMI, QED, RMI, RPI, RSI, SBI, SDI, Sat, Sec, TBI, Teri, bade, bale, balm, bane, barb, bash, baud, beau, beck, beep, berm, bib, bled, boa, boo, bot, brim, bum, byes, dais, deg, deli, eh, hag, hem, jab, keg, lac, lat, mag, mar, nap, nay, oat, pat, pew, phi, rec, rem, sag, sari, sew, tam, tat, wadi, wan, yak, BIM, BME, BMR, BNC, BRIC, BRS, BWR, Beria, Brie, CFI, CNI, DWI, Dali, EIC, Eben, Eu, Fez, GEF, HMI, IPI, Jami, Keri, Mex, RCI, REIT, SMI, Saki, Uzi, VEB, babe, balk, bask, bast, bbl, bevy, beys, bier, blip, bur, fem, gar, gee, ibex, jag, jay, kepi, magi, maim, nag, obi, pail, pap, rad, rah, reg, rel, tel, val, wag, wail, yeti, zen, DLI, FYI, Fri, GMI, HRI, Jeri, Nev, TEF, Tami, VRI, Wed, abet, bani, befit, belie, berg, bps, brie, bro, cad, dab, fay, gab, hep, hew, leis, maw, mew, nab, nae, nevi, oaf, pah, tad, waif, zap, BKS, BRG, EEO, Gail, MLI, bate, bod, caw, fain, fie, hap, mam, naif, neg, ted, vac, vex, abed, baas, barf, bawl, bub, eek, tali, yap, yep, Ali, CEO, Leo, Ned, RBI, jam, née, weir, Xes, brr, gay, AAI, ABES, ACI, AE, AEA, AEB, AED, AEE, AEJ, AEK, AEL, AEM, AEN, AEO, AEP, AEQ, AER, AES, AET, AEU, AEV, AEW, AEX, AEZ, AFI, AGI, AHI, AI, AII, AJI, ANI, AOI, API, AQI, ARI, ATI, AUI, AVI, AYI, AZI, B, B00, B10, B11, B12, B13, B14, B15, B16, B17, B18, B19, B20, B21, B22, B23, B24, B25, B26, B27, B28, B29, B2G, B2M, B30, B31, B33, B34, B35, B36, B37, B38, B39, B40, B41, B42, B43, B44, B45, B46, B47, B48, B49, B50, B51, B52, B53, B54, B55, B56, B57, B58, B59, B60, B61, B64, B65, B66, B67, B68, B69, B70, B71, B72, B74, B75, B77, B79, B80, B81, B82, B83, B85, B86, B87, B88, B89, B90, B91, B92, B93, B94, B95, B96, B97, B99, BA, BAA, BAE, BAEL, BAES, BAF, BAH, BAII, BAM, BAP, BAS, BAU, BAV, BAW, BAX, BAZ, BB, BBA, BBD, BBE, BBF, BBG, BBH, BBJ, BBK, BBL, BBM, BBN, BBO, BBP, BBR, BBT, BBU, BBV, BBW, BBX, BBY, BBZ, BCA, BCB, BCC, BCE, BCF, BCG, BCH, BCJ, BCK, BCL, BCM, BCN, BCO, BCP, BCR, BCS, BCT, BCU, BCV, BCX, BCY, BCZ, BD, BDA, BDB, BDC, BDD, BDE, BDF, BDG, BDH, BDJ, BDK, BDL, BDM, BDN, BDO, BDP, BDQ, BDR, BDT, BDU, BDV, BDW, BDX, BDY, BDZ, BEAC, BECS, BERP, BEST, BF, BFB, BFC, BFD, BFE, BFF, BFG, BFK, BFL, BFM, BFN, BFO, BFP, BFR, BFS, BFT, BFU, BFV, BFW, BFX, BGA, BGB, BGC, BGD, BGE, BGF, BGG, BGH, BGJ, BGK, BGL, BGM, BGN, BGO, BGP, BGQ, BGR, BGS, BGT, BGU, BGV, BGW, BGX, BGY, BHA, BHB, BHC, BHD, BHE, BHF, BHG, BHL, BHM, BHO, BHP, BHQ, BHR, BHS, BHT, BHU, BHV, BHX, BHY, BIA, BIB, BID, BIF, BIH, BIK, BIL, BIN, BIO, BIR, BIS, BIU, BIV, BIW, BIX, BIZ, BJC, BJH, BJJ, BJK, BJM, BJO, BJP, BJR, BJS, BJV, BJW, BJX, BKA, BKC, BKD, BKE, BKF, BKH, BKK, BKL, BKM, BKN, BKP, BKQ, BKR, BKT, BKV, BKW, BKX, BKY, BLA, BLB, BLC, BLD, BLE, BLF, BLG, BLL, BLM, BLP, BLR, BLS, BLU, BLV, BLW, BLX, BLY, BLZ, BM, BMB, BMF, BMJ, BMK, BML, BMM, BMN, BMQ, BMU, BMV, BMX, BMZ, BNA, BNB, BND, BNE, BNF, BNG, BNIX, BNJ, BNK, BNL, BNM, BNN, BNO, BNQ, BNR, BNS, BNT, BNU, BNV, BNW, BNY, BNZ, BO, BOC, BOD, BOE, BOF, BOG, BOH, BOK, BOM, BOP, BOS, BOU, BOV, BOW, BOX, BOZ, BP, BPB, BPC, BPEL, BPF, BPG, BPH, BPK, BPL, BPN, BPO, BPP, BPR, BPS, BPT, BPU, BPY, BQE, BQK, BQP, BQT, BQV, BQW, BR, BRA, BRB, BRC, BRD, BRE, BRF, BRH, BRJ, BRL, BRN, BRO, BRP, BRR, BRT, BRV, BRX, BRZ, BS, BSB, BSC, BSE, BSF, BSG, BSH, BSK, BSL, BSM, BSN, BSO, BSP, BSQ, BSR, BSS, BST, BSU, BSV, BSW, BSX, BTA, BTB, BTC, BTF, BTG, BTH, BTK, BTL, BTM, BTN, BTO, BTP, BTQ, BTR, BTS, BTT, BTU, BTV, BTX, BU, BUA, BUB, BUC, BUD, BUE, BUF, BUH, BUJ, BUK, BUL, BUM, BUO, BUP, BUQ, BUR, BUT, BUU, BUV, BUW, BUX, BUY, BVA, BVB, BVC, BVD, BVE, BVF, BVG, BVH, BVM, BVN, BVO, BVP, BVR, BVS, BVT, BVU, BVV, BVW, BVX, BVY, BVZ, BWA, BWB, BWC, BWD, BWE, BWG, BWH, BWK, BWL, BWM, BWN, BWP, BWQ, BWS, BWT, BWU, BWV, BWY, BXA, BXB, BXC, BXD, BXG, BXH, BXK, BXL, BXM, BXO, BXS, BXU, BXV, BYA, BYB, BYC, BYG, BYH, BYL, BYM, BYN, BYO, BYQ, BYR, BYS, BYV, BYW, BYX, BZA, BZC, BZD, BZG, BZH, BZL, BZN, BZO, BZP, BZR, BZS, BZT, BZU, BZZ, Baha, Belg, Bend, Beth, Bib, Bic, Bk, Blu, Br, Btu, CBCI, CBE, CDI, CE, CEB, CEC, CED, CEE, CEF, CEG, CEJ, CEL, CEM, CEN, CEQ, CER, CES, CESI, CET, CEU, CEV, CHI, CI, CII, CMI, COI, CQI, CVI, Cal, Can, Ce, Chi, Ci, DAI, DDI, DE, DEA, DEB, DEG, DEL, DEM, DEP, DEQ, DES, DET, DEU, DFI, DGI, DHI, DI, DJI, DMI, DOI, DSI, DVI, DZI, Day, Dem, Di, E, EAI, EBE, EBEN, EBI, EC, EDI, EE, EED, EEE, EEG, EEK, EEM, EEP, EER, EFI, EG, EIA, EIB, EID, EIE, EIG, EIN, EIP, EIS, EIT, EK, ELI, EM, EMI, EN, ENI, EO, EPI, ER, ES, ESI, ET, EW, EX, EZ, Ed, Eli, Er, Es, FAI, FBE, FEB, FEC, FED, FEF, FEM, FEN, FEP, FER, FES, FET, FFI, FGI, FI, FII, FJI, FMI, FTI, FWI, Fe, Fed, GCI, GE, GEA, GEH, GEM, GEQ, GES, GET, GEV, GFI, GHI, GOI, GRI, GSI, GTI, Gap, Gay, Ge, Gen, Ger, HAI, HCI, HDI, HEB, HEC, HEF, HEL, HER, HES, HEV, HI, HKI, HOI, Hal, Haw, He, Heb, Hebei, Hubei, IAI, IBE, ICI, IE, IEF, IEG, IEM, IES, IFI, IGI, IMEI, INEI, IOI, IRI, ISI, IVI, Ian, JAI, JE, JED, JEF, JEV, JNI, JRI, Jap, KE, KEA, KED, KEF, KEM, KEN, KER, KES, KET, KOI, Kan, Ken, Key, LBE, LBI, LE, LEA, LEB, LEC, LEF, LEJ, LEM, LEP, LER, LES, LETI, LFI, LHI, LI, LNI, LOI, LPI, LRI, LTI, Lab, Las, Le, Les, Li, Lie, M3I, MBI, MCI, ME, MEA, MEB, MEC, MED, MEF, MEG, MEL, MEN, MEP, MER, MES, MET, MFI, MGI, MI, MMI, MOI, MPI, Maj, Man, May, Me, NE, NED, NEE, NEH, NEM, NEN, NER, NI, NMI, NNI, NOI, NPI, NUI, Nan, Ne, Neb, Neo, Ni, OAI, OBE, OCI, ODI, OEA, OEB, OEC, OED, OEP, OHI, OMI, OPI, ORI, OUI, PBI, PDI, PE, PEA, PEB, PEC, PED, PEE, PEF, PEK, PEL, PEM, PEN, PEP, PER, PES, PEV, PGI, PHI, PNI, POI, PTI, PVI, PWI, PXI, Pan, Peg, Pen, QBE, QE2, QEV, QI, RAI, RDI, RE, REA, REC, RED, REG, REL, REP, RER, RES, REV, REX, RFI, RI, RLI, ROI, RRI, RVI, Rae, Re, SAI, SCI, SDEI, SE, SEA, SEB, SEC, SEF, SEG, SEJ, SEK, SEL, SEM, SEO, SEP, SER, SES, SET, SEU, SEV, SEX, SEY, SEZ, SFI, SI, SII, SKI, SLI, SNI, SOI, SPI, SRI, SSI, STI, SVI, Sal, Se, Sen, Sep, Set, Shi, Si, Sui, TAI, TCI, TEB, TEC, TED, TEE, TEM, TEP, TEQ, TER, TES, TEV, TGI, TI, TMI, TPI, TRI, TSI, TTI, Tad, Te, Tet, Tex, Ti, UAI, UEC, UED, UEM, UER, UES, UET, UFI, UGI, UI, UMI, UNI, URI, USI, VBE, VCI, VDI, VE, VES, VFI, VI, VMI, VPI, Val, Van, WDI, WEC, WEG, WET, WI, WRI, Wac, XEU, Xe, Xi, YEC, YEM, ZED, ZEE, ZEF, ZEP, ZI, ZTI, aah, b, baps, bawd, bedim, berk, bf, bis, biz, bk, bl, bob, bpm, bu, bx, bxs, cay, chi, dag, deb, def, dpi, e, eV, ebb, ed, em, en, er, fab, fag, fer, fey, fez, gad, haj, haw, hes, hie, i, ii, iii, ken, lain, lam, lav, lea, lee, lii, lvi, lxi, mac, mas, meg, meh, mes, mi, nah, oi, pee, poi, re, rehi, res, rev, sen, sex, ti, uni, ve, veg, vi, vii, wad, wain, xci, xi, xii, xvi, xxi, ye, yea, yer, yew, zed, ABI, ADI, AMI, Abi, B&amp;B, B&amp;Q, B&amp;W, BAC, BG, BH, BIC, BJ, BJs, BMIs, BOL, BSc, BTEC, BV, BYD, Bain, Baya, Beja, BenQ, Benji, Biel, CEDI, CZI, DEX, DNI, Dalí, Dani, Demi, Desi, EB, EF, EOI, EP, EQ, EV, EY, El, FECI, GNI, HEIF, HEO, INI, JCI, Jair, Jeb, Jem, Jen, KPI, Kat, LEQ, LEV, Leia, Lexi, Léa, Léo, MBTI, MFi, MYI, MeV, NCI, NRI, NTI, PEO, RET, RKI, RTI, Raj, Ravi, Remi, Rey, Rémi, Sami, Sri, TEU, TOI, TUI, UCI, UEFI, UTI, Uber, Xavi, Yi, ai, baba, bc, bhp, dep, dev, fav, hah, mCi, nav, naïf, né, padi, pax, req, yay] (948) [lt:en:MORFOLOGIK_RULE_EN_US]">bei</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Kane, gainer, keener, seiner, saner, Mainer, vainer, caner, fainer, Heiner, Keizer, Kenner] (952) [lt:en:MORFOLOGIK_RULE_EN_US]">keiner</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Andersen, Andean] (959) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>anderen</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">Hochschule <span class="highlight-spelling" title="Possible spelling mistake found. (978) [lt:en:MORFOLOGIK_RULE_EN_US]">eingereicht</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [UND, and, end, fund, UNC, undo, CND, DND, HND, UNB, Ind, 2nd, AND, BND, END, GND, LND, ND, Nd, OND, PND, SND, TND, UAD, UCD, UED, UFD, UMD, UN, UNF, UNH, UNI, UNL, UNM, UNR, UNT, UNV, UPD, URD, USD, VND, ind, uni, DnD, IND, UHD, UID, UNDP, UNO, in, on, an, any, had, one, under, used, band, did, found, no, up, us, use, UK, land, led, old, round, run, June, find, hand, mid, red, runs, sound, unit, CD, DVD, God, add, aid, bad, bound, ends, funds, gun, guns, kind, mind, send, tend, wind, ad, bed, bid, bond, fed, fun, god, odd, punk, sand, sun, sung, tune, Andy, CNN, HD, Land, Ltd, PhD, RNA, aunt, hung, hunt, ink, lung, mud, pond, pound, rod, sued, sunk, ups, wound, Bond, Hunt, ID, LCD, LED, SD, TNA, TNT, Ted, UHF, USA, USB, UTC, UV, bend, bind, dad, fond, funk, hid, hind, id, inn, kid, lend, lid, mad, mod, mound, nun, nuns, nut, pad, pod, pun, punt, rid, sad, tuned, unto, Ana, BNP, BSD, Bud, CCD, CNS, DNS, INS, JD, Jun, Jung, Juno, LSD, Luna, NAD, NP, PD, Rand, SNL, SNP, Sand, UA, UDP, URL, USDA, Unix, Urdu, ant, cod, dune, fend, junk, nod, nu, quad, quid, tuna, urn, wed, ASD, CNC, CNG, CNR, CNT, FD, GNP, HUD, Hung, Indy, Judd, LD, LNG, PNC, PNG, PNP, PPD, PSD, Pound, RCD, Sung, Suns, UAW, UCL, UDF, UE, UPC, UPI, UPS, USF, USO, USP, WMD, bud, bun, bunk, dunk, fad, hound, lad, med, mend, puns, rune, rung, sod, um, undue, urns, wand, AD, ADD, APD, BCD, BMD, BPD, CFD, CKD, CNE, CSD, DNR, ESD, Enid, FWD, GED, HDD, HNL, Hurd, IED, Jed, Lind, MCD, MHD, MNA, MVD, NPD, NY, PNA, PNL, PNS, QED, RFD, SBD, SNA, SNC, SVD, TLD, TNG, Tod, UCA, UDC, UDR, UNIDO, UUP, WD, buns, bunt, curd, dud, jun, rind, suns, uh, ulna, unwed, ACD, BNC, CNH, CNI, CVD, DDD, DMD, DPD, DSD, FNC, Fundy, GMD, IUD, Ina, Kurd, MNP, MNS, NDB, PGD, PNM, SNF, TDD, UAR, UNEF, USG, Uzi, VN, WNW, Zuni, cued, hued, hunk, inf, mung, rad, rand, undid, DCD, MLD, RNG, SGD, UAM, URB, USNO, Wed, Zn, bung, cad, cud, duns, hod, nub, puny, rend, runt, tad, TBD, bod, gnu, gunk, nus, rued, ted, tuns, ugh, unfed, upend, vend, ETD, YTD, Ann, DNA, GNU, MD, Ned, TD, UFO, USC, USS, pend, UNDRO, cpd, 1D, 2D, 3D, 3rd, 4D, 8D, AAD, ABD, AED, AFD, AGD, AHD, AID, AKD, ALD, AMD, AN, ANB, ANC, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANQ, ANR, ANS, ANT, ANV, ANW, ANX, ANY, ANZ, AOD, ARD, ATD, AUD, AUN, AWD, AXD, AYD, AZD, BBD, BD, BDD, BED, BFD, BGD, BHD, BID, BKD, BLD, BNA, BNB, BNE, BNF, BNG, BNI, BNJ, BNK, BNL, BNM, BNN, BNO, BNQ, BNR, BNS, BNT, BNU, BNV, BNW, BNY, BNZ, BOD, BRD, BUD, BVD, BWD, BXD, BZD, Bend, CAD, CBD, CDD, CED, CHD, CID, CLD, CMD, CN, CNA, CNB, CNDP, CNDS, CNED, CNF, CNJ, CNK, CNL, CNM, CNO, CNP, CNQ, CNU, CNV, CNW, CNX, CNY, CNZ, COD, CPD, CRD, CUD, Cd, Cid, Cod, D, D2D, DBD, DD, DGD, DID, DN, DNB, DNF, DNK, DNL, DNT, DOD, DUD, DZD, Dunn, EAD, EDD, EED, EGD, EID, ELD, EMD, EN, ENC, ENE, ENF, ENG, ENH, ENI, ENL, ENM, ENP, ENS, ERD, EUN, Ed, Eng, FCD, FED, FFD, FID, FJD, FN, FNA, FNB, FNE, FNG, FNO, FNQ, FNR, FNS, FNT, FOD, FPD, FRD, FSD, FUD, Fed, GAD, GDD, GN, GNC, GNF, GNT, GNV, GRD, GYD, Gd, HKD, HMD, HNE, HNF, HNR, HNS, HPD, Hun, Huns, IAD, ICD, IFD, IMD, IN, INA, INB, INC, INE, INED, INF, ING, INH, INM, INP, INR, INT, IPD, IRD, ISD, IVD, In, Inc, JED, JLD, JMD, JNA, JNE, JNI, JNT, JWD, KED, KN, KNA, KNB, KNK, KNM, KNO, KNU, KNX, KOD, KPD, KTD, KWD, LBD, LDD, LLD, LMD, LN, LN2, LNA, LNB, LNE, LNH, LNI, LNO, LNR, LNT, LPD, LRD, LTD, LVD, LYD, Ln, MAD, MDD, MED, MEND, MGD, MID, MKD, MN, MN4, MNB, MNE, MNG, MNK, MNL, MNT, MNW, MOND, MPD, MSD, MTD, Md, Mn, N, NB, NC, NDF, NDH, NDK, NDR, NDS, NDT, NE, NED, NF, NG, NH, NI, NID, NJ, NK, NL, NLD, NM, NNI, NNN, NO, NR, NS, NT, NTD, NU, NV, NW, NZ, NZD, Na, Nb, Ne, Ni, No, Np, OAD, OCD, OD, ODD, OED, OGD, ON, ONB, ONE, ONF, ONG, ONM, ONP, ONS, ONT, ONU, ORD, OSD, Ono, Ont, PCD, PDD, PED, PFD, PHD, PID, PKD, PLD, PNB, PNE, PNF, PNH, PNI, PNK, PNN, PNR, PNT, PNUD, PNV, PNW, PRD, PUN, PUPD, PVD, Pd, QDD, QNE, QNH, QNJ, RAD, RBD, RD, RED, RHD, RJD, RKD, RLD, RN, RN6, RNB, RNE, RNF, RNN, RNO, RNR, RNS, RNT, RNU, RPD, RTD, RUD, RUN, RWD, Rd, Rn, Rod, SAD, SCD, SDD, SHD, SID, SN, SNB, SNE, SNG, SNI, SNR, SNS, SNU, SNV, SOD, SRD, STD, SUD, SYD, Sid, Sn, Sun, TAD, TCD, TED, TGD, THD, TID, TKD, TMD, TN, TNB, TNC, TNE, TNF, TNK, TNL, TNM, TNN, TNO, TNP, TNR, TNS, TSD, TTD, TUN, TWD, Tad, U, U10, U11, U12, U13, U14, U15, U16, U17, U18, U19, U20, U21, U22, U23, U30, U31, U32, U37, U38, U39, U40, U41, U42, U43, U44, U45, U46, U47, U49, U50, U51, U54, U55, U56, U57, U58, U59, U60, U61, U62, U63, U64, U65, U66, U67, U68, U69, U70, U71, U72, U73, U74, U75, U76, U77, U78, U79, U80, U83, U87, U96, U99, UAA, UAB, UAC, UAE, UAG, UAH, UAI, UAL, UAP, UAS, UAU, UB4, UBB, UBC, UBS, UBV, UBX, UC, UCAD, UCB, UCC, UCF, UCM, UCU, UDB, UDDM, UDL, UDM, UDS, UEC, UEM, UER, UES, UET, UFA, UFC, UFE, UFF, UFG, UFI, UFP, UFR, UFS, UFW, UG, UGA, UGB, UGC, UGE, UGG, UGI, UGM, UGS, UGT, UGU, UGX, UHCD, UI, UIA, UIC, UIG, UIL, UIM, UINL, UIO, UIP, UIR, UIT, UIZ, UJA, UJM, UKR, UL, ULC, ULG, ULK, ULM, ULR, ULT, ULX, UMA, UMB, UMF, UMH, UMI, UML, UMM, UMP, UMR, UMS, UMT, UNAC, UNAF, UNIL, UNIS, UNIT, UNIX, UNODC, UNPO, UNSA, UO, UO2, UOB, UP, UP1, UPA, UPB, UPE, UPF, UPK, UPN, UPP, UPR, UPU, UPV, UQC, UQO, UQR, URA, URE, URG, URI, URN, URS, URT, URV, URY, US, USE, USI, USJ, USN, USR, UT, UT1, UTA, UTF, UTG, UTL, UTP, UTV, UUA, UUB, UUC, UUG, UUID, UUO, UUS, UUU, UVA, UVB, UVF, UVM, UVT, UWA, UWC, UWP, UWW, UY, UYF, UZ, UZB, Ufa, Ut, Ute, VD, VDD, VGD, VHD, VNC, VNE, VNM, VNU, VRD, VSD, WAD, WNS, WNT, WSD, X2D, XCD, XNA, YHD, ZAD, ZED, ZNP, ang, ans, d, dd, dun, dung, ed, en, enc, ens, fwd, gad, inc, ins, int, kn, ltd, mun, n, nude, pd, pud, rd, std, tn, tun, turd, u, uhf, ult, ump, unbid, unis, univ, usu, wad, wend, yd, yid, zed, 'n', A&amp;D, BYD, Bundy, D&amp;D, DNC, DNI, DNP, DTD, DoD, ECD, ENA, ENT, EOD, FHD, FMD, FNL, FTD, GID, GNI, GTD, GUID, HRD, HWD, IBD, INI, JVD, KD, L&amp;D, LOD, Lundy, MNC, NAND, NCD, NDA, NDC, NDP, NHD, NWD, ONR, PMD, PN, Pune, QLD, Quid, R&amp;D, RNC, RNZ, Rudd, SPD, SSD, Syd, TNW, UAN, UAT, UAV, UBA, UCI, UCP, UCR, UCS, UCSD, UDA, UHC, UIs, ULB, UMC, UNAM, UNSC, UNSW, UOM, UOS, UPT, UPnP, USDT, UTI, UTM, UVC, UW, UX, Udo, Ulm, Urs, VVD, VoD, XD, XSD, ZD, Zuñi, bundy, cmd, env, gtd, kN, nm, né, uhh, uhm, umm, undos, vid] (990) [lt:en:MORFOLOGIK_RULE_EN_US]">und</span> hat <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Noah, notch, Enoch, Foch, nosh, Koch, och, n och, no ch, Boch, NOC] (998) [lt:en:MORFOLOGIK_RULE_EN_US]">noch</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Kane, Karen] (1003) [lt:en:MORFOLOGIK_RULE_EN_US]">keinen</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [antigen, onstage, solstice, solstices, consign, Rontgen, röntgen] (1010) [lt:en:MORFOLOGIK_RULE_EN_US]">sonstigen</span> <span class="highlight-spelling" title="Possible spelling mistake found. (1020) [lt:en:MORFOLOGIK_RULE_EN_US]">Prüfungszwecken</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [gradient, sediment, lenient, obedient, pediment, adient] (1036) [lt:en:MORFOLOGIK_RULE_EN_US]">gedient</span>. \newline</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">\noindent Experimentelle Daten wurden ordnungsgemäß</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Und] (1056) [lt:en:UPPERCASE_SENTENCE_START]">und</span> <span class="highlight-spelling" title="Possible spelling mistake found. (1060) [lt:en:MORFOLOGIK_RULE_EN_US]">nachvollziehbar</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Escher] (1076) [lt:en:MORFOLOGIK_RULE_EN_US]">gesichert</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [UND, and, end, fund, UNC, undo, CND, DND, HND, UNB, Ind, 2nd, AND, BND, END, GND, LND, ND, Nd, OND, PND, SND, TND, UAD, UCD, UED, UFD, UMD, UN, UNF, UNH, UNI, UNL, UNM, UNR, UNT, UNV, UPD, URD, USD, VND, ind, uni, DnD, IND, UHD, UID, UNDP, UNO, in, on, an, any, had, one, under, used, band, did, found, no, up, us, use, UK, land, led, old, round, run, June, find, hand, mid, red, runs, sound, unit, CD, DVD, God, add, aid, bad, bound, ends, funds, gun, guns, kind, mind, send, tend, wind, ad, bed, bid, bond, fed, fun, god, odd, punk, sand, sun, sung, tune, Andy, CNN, HD, Land, Ltd, PhD, RNA, aunt, hung, hunt, ink, lung, mud, pond, pound, rod, sued, sunk, ups, wound, Bond, Hunt, ID, LCD, LED, SD, TNA, TNT, Ted, UHF, USA, USB, UTC, UV, bend, bind, dad, fond, funk, hid, hind, id, inn, kid, lend, lid, mad, mod, mound, nun, nuns, nut, pad, pod, pun, punt, rid, sad, tuned, unto, Ana, BNP, BSD, Bud, CCD, CNS, DNS, INS, JD, Jun, Jung, Juno, LSD, Luna, NAD, NP, PD, Rand, SNL, SNP, Sand, UA, UDP, URL, USDA, Unix, Urdu, ant, cod, dune, fend, junk, nod, nu, quad, quid, tuna, urn, wed, ASD, CNC, CNG, CNR, CNT, FD, GNP, HUD, Hung, Indy, Judd, LD, LNG, PNC, PNG, PNP, PPD, PSD, Pound, RCD, Sung, Suns, UAW, UCL, UDF, UE, UPC, UPI, UPS, USF, USO, USP, WMD, bud, bun, bunk, dunk, fad, hound, lad, med, mend, puns, rune, rung, sod, um, undue, urns, wand, AD, ADD, APD, BCD, BMD, BPD, CFD, CKD, CNE, CSD, DNR, ESD, Enid, FWD, GED, HDD, HNL, Hurd, IED, Jed, Lind, MCD, MHD, MNA, MVD, NPD, NY, PNA, PNL, PNS, QED, RFD, SBD, SNA, SNC, SVD, TLD, TNG, Tod, UCA, UDC, UDR, UNIDO, UUP, WD, buns, bunt, curd, dud, jun, rind, suns, uh, ulna, unwed, ACD, BNC, CNH, CNI, CVD, DDD, DMD, DPD, DSD, FNC, Fundy, GMD, IUD, Ina, Kurd, MNP, MNS, NDB, PGD, PNM, SNF, TDD, UAR, UNEF, USG, Uzi, VN, WNW, Zuni, cued, hued, hunk, inf, mung, rad, rand, undid, DCD, MLD, RNG, SGD, UAM, URB, USNO, Wed, Zn, bung, cad, cud, duns, hod, nub, puny, rend, runt, tad, TBD, bod, gnu, gunk, nus, rued, ted, tuns, ugh, unfed, upend, vend, ETD, YTD, Ann, DNA, GNU, MD, Ned, TD, UFO, USC, USS, pend, UNDRO, cpd, 1D, 2D, 3D, 3rd, 4D, 8D, AAD, ABD, AED, AFD, AGD, AHD, AID, AKD, ALD, AMD, AN, ANB, ANC, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANQ, ANR, ANS, ANT, ANV, ANW, ANX, ANY, ANZ, AOD, ARD, ATD, AUD, AUN, AWD, AXD, AYD, AZD, BBD, BD, BDD, BED, BFD, BGD, BHD, BID, BKD, BLD, BNA, BNB, BNE, BNF, BNG, BNI, BNJ, BNK, BNL, BNM, BNN, BNO, BNQ, BNR, BNS, BNT, BNU, BNV, BNW, BNY, BNZ, BOD, BRD, BUD, BVD, BWD, BXD, BZD, Bend, CAD, CBD, CDD, CED, CHD, CID, CLD, CMD, CN, CNA, CNB, CNDP, CNDS, CNED, CNF, CNJ, CNK, CNL, CNM, CNO, CNP, CNQ, CNU, CNV, CNW, CNX, CNY, CNZ, COD, CPD, CRD, CUD, Cd, Cid, Cod, D, D2D, DBD, DD, DGD, DID, DN, DNB, DNF, DNK, DNL, DNT, DOD, DUD, DZD, Dunn, EAD, EDD, EED, EGD, EID, ELD, EMD, EN, ENC, ENE, ENF, ENG, ENH, ENI, ENL, ENM, ENP, ENS, ERD, EUN, Ed, Eng, FCD, FED, FFD, FID, FJD, FN, FNA, FNB, FNE, FNG, FNO, FNQ, FNR, FNS, FNT, FOD, FPD, FRD, FSD, FUD, Fed, GAD, GDD, GN, GNC, GNF, GNT, GNV, GRD, GYD, Gd, HKD, HMD, HNE, HNF, HNR, HNS, HPD, Hun, Huns, IAD, ICD, IFD, IMD, IN, INA, INB, INC, INE, INED, INF, ING, INH, INM, INP, INR, INT, IPD, IRD, ISD, IVD, In, Inc, JED, JLD, JMD, JNA, JNE, JNI, JNT, JWD, KED, KN, KNA, KNB, KNK, KNM, KNO, KNU, KNX, KOD, KPD, KTD, KWD, LBD, LDD, LLD, LMD, LN, LN2, LNA, LNB, LNE, LNH, LNI, LNO, LNR, LNT, LPD, LRD, LTD, LVD, LYD, Ln, MAD, MDD, MED, MEND, MGD, MID, MKD, MN, MN4, MNB, MNE, MNG, MNK, MNL, MNT, MNW, MOND, MPD, MSD, MTD, Md, Mn, N, NB, NC, NDF, NDH, NDK, NDR, NDS, NDT, NE, NED, NF, NG, NH, NI, NID, NJ, NK, NL, NLD, NM, NNI, NNN, NO, NR, NS, NT, NTD, NU, NV, NW, NZ, NZD, Na, Nb, Ne, Ni, No, Np, OAD, OCD, OD, ODD, OED, OGD, ON, ONB, ONE, ONF, ONG, ONM, ONP, ONS, ONT, ONU, ORD, OSD, Ono, Ont, PCD, PDD, PED, PFD, PHD, PID, PKD, PLD, PNB, PNE, PNF, PNH, PNI, PNK, PNN, PNR, PNT, PNUD, PNV, PNW, PRD, PUN, PUPD, PVD, Pd, QDD, QNE, QNH, QNJ, RAD, RBD, RD, RED, RHD, RJD, RKD, RLD, RN, RN6, RNB, RNE, RNF, RNN, RNO, RNR, RNS, RNT, RNU, RPD, RTD, RUD, RUN, RWD, Rd, Rn, Rod, SAD, SCD, SDD, SHD, SID, SN, SNB, SNE, SNG, SNI, SNR, SNS, SNU, SNV, SOD, SRD, STD, SUD, SYD, Sid, Sn, Sun, TAD, TCD, TED, TGD, THD, TID, TKD, TMD, TN, TNB, TNC, TNE, TNF, TNK, TNL, TNM, TNN, TNO, TNP, TNR, TNS, TSD, TTD, TUN, TWD, Tad, U, U10, U11, U12, U13, U14, U15, U16, U17, U18, U19, U20, U21, U22, U23, U30, U31, U32, U37, U38, U39, U40, U41, U42, U43, U44, U45, U46, U47, U49, U50, U51, U54, U55, U56, U57, U58, U59, U60, U61, U62, U63, U64, U65, U66, U67, U68, U69, U70, U71, U72, U73, U74, U75, U76, U77, U78, U79, U80, U83, U87, U96, U99, UAA, UAB, UAC, UAE, UAG, UAH, UAI, UAL, UAP, UAS, UAU, UB4, UBB, UBC, UBS, UBV, UBX, UC, UCAD, UCB, UCC, UCF, UCM, UCU, UDB, UDDM, UDL, UDM, UDS, UEC, UEM, UER, UES, UET, UFA, UFC, UFE, UFF, UFG, UFI, UFP, UFR, UFS, UFW, UG, UGA, UGB, UGC, UGE, UGG, UGI, UGM, UGS, UGT, UGU, UGX, UHCD, UI, UIA, UIC, UIG, UIL, UIM, UINL, UIO, UIP, UIR, UIT, UIZ, UJA, UJM, UKR, UL, ULC, ULG, ULK, ULM, ULR, ULT, ULX, UMA, UMB, UMF, UMH, UMI, UML, UMM, UMP, UMR, UMS, UMT, UNAC, UNAF, UNIL, UNIS, UNIT, UNIX, UNODC, UNPO, UNSA, UO, UO2, UOB, UP, UP1, UPA, UPB, UPE, UPF, UPK, UPN, UPP, UPR, UPU, UPV, UQC, UQO, UQR, URA, URE, URG, URI, URN, URS, URT, URV, URY, US, USE, USI, USJ, USN, USR, UT, UT1, UTA, UTF, UTG, UTL, UTP, UTV, UUA, UUB, UUC, UUG, UUID, UUO, UUS, UUU, UVA, UVB, UVF, UVM, UVT, UWA, UWC, UWP, UWW, UY, UYF, UZ, UZB, Ufa, Ut, Ute, VD, VDD, VGD, VHD, VNC, VNE, VNM, VNU, VRD, VSD, WAD, WNS, WNT, WSD, X2D, XCD, XNA, YHD, ZAD, ZED, ZNP, ang, ans, d, dd, dun, dung, ed, en, enc, ens, fwd, gad, inc, ins, int, kn, ltd, mun, n, nude, pd, pud, rd, std, tn, tun, turd, u, uhf, ult, ump, unbid, unis, univ, usu, wad, wend, yd, yid, zed, 'n', A&amp;D, BYD, Bundy, D&amp;D, DNC, DNI, DNP, DTD, DoD, ECD, ENA, ENT, EOD, FHD, FMD, FNL, FTD, GID, GNI, GTD, GUID, HRD, HWD, IBD, INI, JVD, KD, L&amp;D, LOD, Lundy, MNC, NAND, NCD, NDA, NDC, NDP, NHD, NWD, ONR, PMD, PN, Pune, QLD, Quid, R&amp;D, RNC, RNZ, Rudd, SPD, SSD, Syd, TNW, UAN, UAT, UAV, UBA, UCI, UCP, UCR, UCS, UCSD, UDA, UHC, UIs, ULB, UMC, UNAM, UNSC, UNSW, UOM, UOS, UPT, UPnP, USDT, UTI, UTM, UVC, UW, UX, Udo, Ulm, Urs, VVD, VoD, XD, XSD, ZD, Zuñi, bundy, cmd, env, gtd, kN, nm, né, uhh, uhm, umm, undos, vid] (1086) [lt:en:MORFOLOGIK_RULE_EN_US]">und</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [them, DEM, Dem, demo, Del, dam, den, DEC, DM, DRM, DSM, Dec, Dee, OEM, dim, gem, DTM, REM, deem, dew, DCM, DWM, Diem, deg, hem, rem, DLM, fem, idem, AEM, BEM, CEM, DDM, DE, DEA, DEB, DEG, DEI, DEL, DEP, DEQ, DES, DET, DEU, DFM, DHM, DLEM, DMM, DOM, DXM, EEM, EM, FEM, GEM, IEM, KEM, LEM, NEM, ODEM, PEM, SEM, TEM, UEM, YEM, deb, def, em, d em, DEX, DVM, Demi, Dems, Jem, dBm, dep, dev, dm, he, we, be, get, her, him, me, new, day, did, due, set, team, yes, died, do, does, few, km, led, men, see, term, deal, idea, let, red, sea, ten, yet, DVD, Dr, FM, am, arm, cm, com, dead, deep, die, dog, dry, ex, hey, key, leg, mm, poem, semi, web, EU, IBM, Jim, Lee, Tom, beam, bed, debt, deck, dies, diet, dream, drew, drum, duo, fed, fee, item, jet, met, net, seem, stem, sum, tea, wet, Adam, Ben, Dan, Dean, PM, Sam, deer, demon, deny, desk, dome, duel, duet, gym, pen, per, pet, pm, rim, rpm, Dame, Drew, LED, RAM, Rev, Ted, Web, Wei, bee, bet, dad, dams, deaf, dean, deed, demos, dig, dot, dub, dug, dump, dye, gum, helm, memo, ram, ref, ACM, ATM, DA, DDR, DDT, DL, DNS, DSO, DSP, DT, Devi, Eden, Feb, Ham, IEC, Jew, Mel, NEC, PET, RPM, Rex, SAM, Zen, beg, cam, dB, damp, dear, din, dip, doom, dorm, dues, dumb, dyed, dyes, edema, eds, elm, gel, gems, germ, ham, hemp, modem, mom, ppm, rum, seam, sec, wed, yen, AEC, AEG, Aden, CEA, DAC, DAT, DCC, DCI, DG, DLR, DMC, DMS, DMZ, DRC, DSC, DSL, DSS, DST, DTS, DUI, DUP, Dell, Deng, Doe, Dow, Dyer, EEA, EEC, EMT, Geo, KLM, KTM, Lea, Len, Lew, Meg, NEA, Nam, Oder, PCM, Pam, Rep, TeX, UE, VM, deems, defy, denim, dens, dime, eel, fen, gen, hen, hex, hum, med, mum, nee, ode, pea, peg, pep, rep, seq, tee, um, vet, wee, yam, ABM, AEF, AFM, APM, BPM, BRM, CBM, CEP, CPM, CTM, Clem, DBA, DBMS, DCE, DCL, DDS, DKK, DLA, DLL, DME, DMG, DMK, DNR, DOA, DPI, DRG, DRI, DSA, DSB, DSE, DTC, DTI, DVR, Dir, EAM, EMB, FSM, GED, ICM, IEA, IED, IEP, IOM, JDM, JM, JVM, Jed, LSM, Mme, NEP, OE, PSM, Perm, QED, RCM, RSM, SRM, SSM, Sec, TCM, TPM, berm, bum, dame, deft, deism, deli, dell, dent, doc, doe, dud, eh, emo, emu, keg, odes, ohm, pew, rec, sew, tam, temp, Adm, BIM, DCB, DCF, DDA, DDB, DDC, DDD, DDE, DMD, DMP, DMV, DND, DOB, DPD, DPT, DSD, DSR, DTA, DTMF, DTP, DWI, DWT, Debs, EMR, Eu, FAM, Fez, GAM, GBM, GEF, KVM, LCM, LPM, MDM, Mex, Odom, PNM, Qom, SDM, SGM, VEB, aim, dept, doer, dram, dz, emf, gee, gm, meme, perm, reg, rel, tel, tum, yum, zen, DBC, DCD, DLI, DPN, DPs, DRV, DVP, DVS, DWG, Dena, GPM, Nev, Orem, RMM, TEF, UAM, WLM, Wed, dab, duh, dyer, hems, hep, hew, mew, ream, wpm, EEO, Edam, demur, dewy, dims, fum, heme, ides, mam, neg, ted, teem, vex, vim, ahem, debs, eek, elem, ems, rems, yep, CEO, DJ, DNA, FEMS, GM, Kim, Leo, Ned, ROM, Tim, chm, dbl, dob, doz, dpt, jam, née, REMs, Xes, dds, 2RM, A6M, AAM, ADEME, ADM, AE, AEA, AEB, AED, AEE, AEI, AEJ, AEK, AEL, AEMG, AEMO, AEN, AEO, AEP, AEQ, AER, AES, AET, AEU, AEV, AEW, AEX, AEZ, AGM, AHM, AIM, AKM, ALM, AM, AMM, ANM, AOM, AQM, ARM, ASEM, ASM, AUM, AWM, AXM, Am, B2M, BAM, BBM, BCM, BDE, BDM, BDSM, BEA, BEB, BED, BEE, BEF, BEH, BEI, BEK, BEO, BEP, BEQ, BER, BES, BET, BEU, BEV, BEW, BEX, BFM, BGM, BHM, BJM, BKM, BLM, BM, BMM, BNM, BOM, BSM, BTM, BUM, BVM, BWM, BXM, BYM, Be, CAM, CCM, CDE, CDM, CE, CEB, CEC, CED, CEE, CEF, CEG, CEI, CEJ, CEL, CEN, CEQ, CER, CES, CET, CEU, CEV, CFM, CHM, CIEM, CIM, CKM, CLM, CM, CMM, CNM, COM, CQM, CREM, CRM, CSM, CUM, CVM, CXM, CYM, CZM, Ce, Cm, Com, D, D10, D11, D12, D13, D14, D15, D16, D17, D18, D19, D20, D21, D22, D23, D24, D25, D26, D27, D28, D29, D2D, D30, D31, D32, D33, D34, D35, D36, D37, D38, D39, D40, D41, D42, D45, D46, D48, D50, D51, D52, D55, D56, D57, D58, D59, D60, D61, D62, D63, D64, D65, D66, D67, D68, D70, D71, D72, D73, D74, D76, D78, D79, D8, D80, D82, D83, D86, D89, D90, D91, D96, D97, D99, DAA, DAB, DAE, DAEU, DAF, DAI, DAJ, DAK, DAL, DAR, DAU, DAV, DAX, DB7, DB9, DBB, DBD, DBF, DBL, DBO, DBP, DBQ, DBR, DBU, DBZ, DC, DCA, DCIM, DCK, DCN, DCO, DCP, DCR, DCS, DCT, DCU, DD, DDEA, DDH, DDI, DDL, DDO, DDV, DECT, DECs, DEEE, DEET, DELE, DELF, DESE, DEUG, DEXA, DFA, DFB, DFE, DFF, DFG, DFI, DFL, DFN, DFO, DFP, DFS, DFT, DFW, DGA, DGD, DGE, DGF, DGH, DGI, DGS, DGT, DH, DHA, DHC, DHEA, DHH, DHI, DHP, DHR, DHS, DHT, DI, DIC, DID, DIF, DIL, DIMM, DIN, DIP, DIR, DIV, DIY, DJF, DJG, DJI, DJO, DJT, DJU, DK, DKB, DKP, DKR, DKZ, DLC, DLF, DLG, DLH, DLP, DLT, DMF, DMI, DMO, DMR, DN, DNB, DNF, DNK, DNL, DNT, DO, DOC, DOD, DOE, DOF, DOI, DOJ, DON, DOP, DOS, DOT, DP, DPA, DPB, DPC, DPE, DPH, DPMI, DPO, DPS, DPU, DPW, DR, DR1, DRAM, DRH, DRP, DRS, DRT, DS, DSF, DSG, DSI, DSK, DSN, DSQ, DSU, DSV, DTU, DTV, DTW, DUD, DUS, DVA, DVB, DVI, DWH, DWV, DXB, DXF, DXO, DZA, DZD, DZI, DZO, Day, Deon, Depp, Di, Dis, Dix, Don, Dot, Du, Dy, E, EC, ECM, EDM, EE, EED, EEE, EEG, EEK, EEP, EER, EG, EI, EK, EMA, EMC, EMD, EME, EMF, EMI, EML, EMM, EMN, EMP, EMU, EN, ENM, EO, EPM, EQM, ER, ES, ESM, ET, ETM, EVM, EW, EX, EZ, Ed, Er, Es, FCM, FDES, FDM, FEB, FEC, FED, FEF, FEI, FEN, FEP, FER, FES, FET, FFM, FGM, FHM, FIM, FKM, FLM, FMM, FPM, FRM, FTM, Fe, Fed, Fm, GCM, GDE, GDM, GE, GEA, GEH, GEI, GEQ, GES, GET, GEV, GHM, GLM, GQM, GSM, GTM, GUM, Ge, Gen, Ger, HBM, HDE, HDM, HEB, HEC, HEF, HEL, HER, HES, HEV, HLM, HM, HOM, HRM, HSM, He, Heb, IDE, IDES, IDM, IE, IEF, IEG, IEMN, IEOM, IES, IFM, IGM, IIM, IJM, ILM, INM, IPM, IREM, IRM, ISM, IUEM, JDE, JE, JED, JEF, JEV, JIM, JLM, JMM, KE, KEA, KED, KEF, KEN, KER, KES, KET, KFM, KGM, KHM, KM, KNM, KSM, Kemp, Ken, Key, LBM, LDE, LE, LEA, LEB, LEC, LEF, LEJ, LEP, LER, LES, LFM, LGM, LIM, LKM, LRM, Le, Les, M, M2M, MAM, MCM, MDE, ME, MEA, MEB, MEC, MED, MEF, MEG, MEL, MEN, MEP, MER, MES, MET, MFM, MGM, MIM, MKM, MLM, MM, MMM, MODEM, MOM, MPM, MSM, MZM, Me, NAM, NCM, NE, NED, NEE, NEH, NEN, NER, NKM, NM, NMM, NRM, NSM, NVM, Ne, Neb, Neo, OAM, ODM, OEA, OEB, OEC, OED, OEP, OGM, OHM, OIM, OM, OMM, ONEM, ONM, OOM, OPM, ORM, OSM, PAM, PE, PEA, PEB, PEC, PED, PEE, PEF, PEI, PEK, PEL, PEN, PEP, PER, PES, PEV, PGM, PIM, PLM, POM, PPM, PRM, PTM, PUM, PVM, Peg, Pei, Pen, Pm, Pym, QCM, QE2, QEV, QM, QPM, RBM, RDM, RE, REA, REC, RED, REG, REI, REL, REMA, REP, RER, RES, REV, REX, RFM, RIM, RM, RTM, RVM, Re, Rom, SBM, SCM, SDEI, SE, SEA, SEB, SEC, SEF, SEG, SEJ, SEK, SEL, SEO, SEP, SER, SES, SET, SEU, SEV, SEX, SEY, SEZ, SHM, SIM, SJM, SM, SMM, SPM, STM, SVM, SXM, Se, Sen, Sep, Set, Sm, TAM, TBM, TDE, TDM, TEB, TEC, TED, TEE, TEI, TEP, TEQ, TER, TES, TEV, THM, TKM, TLM, TM, TNM, TOM, TQM, TRM, TSM, TVM, Te, Tet, Tex, Tm, UCM, UDDM, UDM, UEC, UED, UER, UES, UET, UGM, UIM, UJM, ULM, UMM, UNM, UVM, VAM, VCM, VDM, VE, VEI, VES, VGM, VLM, VMM, VNM, VSM, WAM, WDM, WEC, WEG, WET, WIM, WSM, Wm, XEU, XPM, Xe, YEC, YM, YQM, YUM, ZAM, ZED, ZEE, ZEF, ZEP, ZIM, bdrm, bey, bpm, chem, cum, d, dag, damn, db, dd, dded, demob, derv, diam, dis, div, don, dos, dpi, dun, e, eV, ea, ed, en, er, fer, fey, fez, geom, hes, hmm, ism, ken, lam, lea, lee, lei, m, meg, meh, mes, om, pee, pom, re, res, rev, rm, sen, sex, sim, tom, ve, veg, ye, yea, yer, yew, zed, 3M, A&amp;M, AVM, Adel, BEC, CDEC, CEMS, CGM, CWM, D&amp;D, DAP, DAW, DB, DBS, DECA, DF, DFC, DFV, DGP, DHL, DJed, DJs, DKA, DKIM, DLK, DLS, DMA, DMT, DMX, DMed, DMs, DNC, DNI, DNP, DOH, DOL, DQ, DRA, DRMs, DTD, DTT, DV, DVC, DVMs, DVT, DW, DWC, DX, Desi, DnD, DoD, DoJ, DoS, Doom, EB, EF, EMG, EMS, EOM, EP, EQ, ERM, EV, EY, El, FBM, FEMA, H&amp;M, HEO, IAM, IDEs, IDed, IM, Jeb, Jen, KDE, LEQ, LEV, LM, LVM, Léa, Léo, M&amp;M, MeV, NPM, Nemo, O&amp;M, OEMs, PEO, PWM, QEMU, RET, Remi, Remy, Rey, Rémi, Rémy, SDE, SFM, TEU, UOM, UTM, Udemy, Ulm, VDE, WCM, WFM, WHM, WebM, bam, deps, devs, dmed, dms, dox, drey, hm, mmm, nm, né, req, semé, uhm, umm, vm, µm] (1090) [lt:en:MORFOLOGIK_RULE_EN_US]">dem</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Betrayer, Breuer] (1094) [lt:en:MORFOLOGIK_RULE_EN_US]">Betreuer</span>/<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [her, Dr, deer, per, Del, den, DDR, DEC, Dec, Dee, dear, DLR, Dyer, Oder, dew, DNR, DVR, Dir, deg, DSR, doer, dyer, AER, BER, CER, DAR, DBR, DCR, DE, DEA, DEB, DEG, DEI, DEL, DEM, DEP, DEQ, DES, DET, DEU, DHR, DIR, DKR, DMR, DR, Dem, EER, ER, Er, FER, Ger, HER, KER, LER, MER, NER, PER, RER, SER, TER, UER, deb, def, derv, er, fer, yer, DEX, dep, dev] (1103) [lt:en:MORFOLOGIK_RULE_EN_US]">der</span> <span class="highlight-spelling" title="Possible spelling mistake found. (1107) [lt:en:MORFOLOGIK_RULE_EN_US]">Betreuerin</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Aberdeen, Bergen, bargemen, Bergeron, bergère, bergères] (1118) [lt:en:MORFOLOGIK_RULE_EN_US]">übergeben</span>. \newline \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (1139) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [big skip] (1149) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>bigskip</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">Marburg, den \<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [my format] (1171) [lt:en:MORFOLOGIK_RULE_EN_US]">myformat</span>\today \hspace{4cm} Johannes Gille</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found. (1189) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>cleardoublepage</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">\pagenumbering{Roman}</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found. (1217) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>tableofcontents</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">\thispagestyle{empty}</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [misconfigures] (1237) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>listoffigures</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline">\<span class="highlight-spelling" title="Possible spelling mistake found. (1252) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>listoftables</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline"><span class="highlight-sh" title="If you are writing a research paper, do not force page breaks. [sh:nonp]"></span>\newpage</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">\pagenumbering{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Arabic, Arabia, arabica, Arab, tragic, Arabs, Arctic, Arabian, arable, arctic, Aramaic, aerobic, Rabin, rabid, Alaric, Arabist, Araby, ataxic, ARIC, Ardabil, CRASIC, Adamic, Arabize] (1266) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>arabic}</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">\include{01_introduction.tex}</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">\include{02_methods.tex}</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">\include{03_results.tex}</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">\include{04_discussion.tex}</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">\include{05_appendix.tex}</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">\include{06_notes.tex}</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline">\bibliography{bib/library.bib}</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline"><span class="keyword2">\end{document}</span></div><div class="clear"></div>
</div>
<h2 class="filename">02_methods.tex</h2>

<p>Found 165 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 3 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{Methods}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span>{The dendritic error model}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">This section will go into detail about the dendritic error network \citep{sacramento2018dendritic}. The model contains a</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">somewhat complex and strongly recurrent connectivity, which poses one of the major criticisms aimed at it</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">\citep{whittington2019theories}. Much like traditional machine learning networks, it can be functionally separated into</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">layers. Yet in this particular model, input- hidden- and output layers are quite distinct in both neuron populations and</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">connectivity.</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline"><span class="keyword1">\subsection</span>{Network architecture}</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h!]</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">  <span class="keyword2">\begin{minipage}</span>{0.5\textwidth}</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{network_a}</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">  <span class="keyword2">\end{minipage}</span>\hfill</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">  <span class="keyword2">\begin{minipage}</span>{0.4\textwidth}</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{network_b}</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">  <span class="keyword2">\end{minipage}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">  <span class="keyword1">\caption</span>[Structure of the dendritic error network]{Structure of the dendritic error network, from \citep{Haider2021}.</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">    <span class="keyword1">\textbf</span>{a:} pyramidal- (red) and interneurons (blue) in a network of three layers. Note the fact that the number</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">    interneurons in a layer is equal to the number of pyramidal neurons in the subsequent layer\protect\footnotemark.</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">    <span class="keyword1">\textbf</span>{b:} connectivity within the highlighted section. Feedback pyramidal-to-interneuron connections (displayed</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">    with rectangular synapse) transmit pyramidal somatic potential directly and connect to a single interneuron. This</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">    enables these interneurons to learn to match their corresponding next-layer pyramidal neurons. All other synapses</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">    (circles) transmit the neuron's somatic activation $\phi (u^{som})$ and fully connect their origin and target</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">    populations.}</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">  <span class="keyword1">\label</span>{fig-network}</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">\footnotetext{{Note that the input layer is displayed as having interneurons here. This appears to be a mistake in the</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">      Figure. Within the implementation, interneurons are only modelled in hidden layers.}}</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">The basic connectivity scheme of the Model is shown in Fig. \ref{fig-network}. Neurons at the input layer receive no</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">feedback signals and serve primarily to apply a temporal low-pass filter to the stimulus which is injected directly</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline">into their membrane.  Hidden layers consist of a pyramidal- and an interneuron population, which are fully connected to</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">each other reciprocally. Both types of neurons are represented by multi-compartment neuron models with leaky membrane</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">dynamics. Interneurons contain one somatic and one dendritic compartment, while pyramidal neurons are modeled with both</div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">a basal and an apical dendrite.  Feedforward connections between layers are facilitated by all-to-all connections</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">between their respective pyramidal neurons and innervate basal compartments. Feedback connections from superficial</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">pyramidal neurons, as well as lateral interneuron connections arrive at the apical compartments of pyramidal neurons.</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">Thus, a hidden layer pyramidal neuron forms two reciprocal loops, one with all interneurons in the same layer, and one</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">with all pyramidal neurons in the next layer.</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">Interneurons receive feedback information from superficial pyramidal neurons in addition to their lateral connections.</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">These feedback connections are unique in this model, as they connect one pyramidal neuron to exactly one interneuron.</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">Instead of transmitting a neuronal activation as all other connections do, these connections relay somatic voltage</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">directly. This one-to-one connectivity puts a strict constraint on the number of interneurons in a hidden layer, as it</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">must be equal to the number of subsequent pyramidal neurons. These pairs of inter- and pyramidal neurons will henceforth</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">be called <span class="keyword1">\textit</span>{sister neurons}. The top-down signal serves to <span class="keyword1">\textit</span>{nudge} interneuron somatic activation towards</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">that of their pyramidal sisters. The purpose of an interneuron in this architecture is then, to predict the activity of</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">its sister neuron. Any failure to do so results in layer-specific errors which in turn are the driving force of learning</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">in this context, but more on this later.</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">Output layers have no interneurons, and are usually modeled as pyramidal neurons without an apical compartment. During</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">learning, the target for the network's activation is injected into their somatic compartment. Through the feedback</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">connections, it can propagate through the entire network. To understand what purpose this rather complex connectivity</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">scheme serves in our model, neuron models and plasticity rules require some elaboration.</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline"><span class="keyword1">\subsection</span>{Neuron models}<span class="keyword1">\label</span>{sec-neurons}</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">The network contains two types of multi-compartment neurons; Pyramidal neurons with three compartments each, and</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">interneurons with two compartments each. They integrate synaptic inputs into dendritic potentials, which in turn leak</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, Lomé, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, Sofía, Sonoma, Souza, TOML, Tomás, UOM, Zora, hola, momma, sRNA, semé, simp, socs, stomas, Škoda] (3446) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> with specific <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (3465) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span>. Note that vector notation will be used throughout this section, and $u_l^P$</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">and $u_l^I$ denote the column vectors of pyramidal- and interneuron somatic voltages at layer $l$, respectively.</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">Synaptic weights $W$ are likewise assumed matrices of size $n \times m$, which are the number of output and input</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">neurons of the connected populations respectively. The activation (or rate) $r_l^P$ of pyramidal neurons is given by</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">applying the neuronal transfer function $\phi$ to their somatic potentials $u_l^P$:</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">  r_l^P   &amp; = \phi(u_l^P)                                                                      \\</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">  \phi(x) &amp; = <span class="keyword2">\begin{cases}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">                0                                   &amp; \textrm{if } \ x &lt; -\epsilon               \\</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">                \gamma \ log(1+e^{\beta(x-\theta)}) &amp; \textrm{if } \ -\epsilon \leq x &lt; \epsilon \\</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">                \gamma \ x                          &amp; \textrm{otherwise}</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">              <span class="keyword2">\end{cases}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (3940) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $\phi$ acts component wise on $u$ and can be interpreted as a smoothed variant of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Rely, Rel, ELU, REL, Re Lu, ReL u] (4021) [lt:en:MORFOLOGIK_RULE_EN_US]">ReLu</span> (sometimes called</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline"><span class="keyword1">\textit</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Soft plus] (4044) [lt:en:MORFOLOGIK_RULE_EN_US]">Softplus}</span>) with scaling factors $\gamma=1$, $\beta=1$, $\theta=0$. Splitting the computation with the threshold</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">parameter $\epsilon=15$ does not alter its output much, but instead serves to prevent overflow errors for large absolute</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">values of $x$.</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">As mentioned before, pyramidal and interneurons are modeled as rate neurons with leaky membrane dynamics and multiple</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">compartments. Where applicable, they will be differentiated with superscripts $P$ and $I$ respectively. The basal and</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">apical dendrites of pyramidal neurons are denoted with superscripts <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [beys, BAS, was, as, has, base, gas, bad, bar, bass, bus, ban, bars, bay, bag, bat, bats, bias, Bass, SAS, bags, bays, BBS, bans, pas, IAS, OAS, bras, BDS, BMS, Boas, bash, BRS, abs, bask, bast, boas, bah, bps, BKS, baa, baas, AAS, As, BA, BAA, BAE, BAES, BAF, BAH, BAM, BAP, BAU, BAV, BAW, BAX, BAZ, BCS, BES, BFS, BGS, BHS, BIS, BJS, BLS, BNS, BOS, BPS, BS, BSS, BTS, BVS, BWS, BXS, BYS, BZS, Ba, EAS, GAS, Las, MAS, NAS, PAS, RAS, UAS, bap, baps, bey, bis, bxs, mas, b as, BAC, BASF, BJs, CAS, FAS, PAs, TAS, bam] (4552) [lt:en:MORFOLOGIK_RULE_EN_US]">$bas$</span> and <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [API, APA, APC, CPI, ape, apt, pi, APG, APS, PPI, UPI, APD, APM, Apr, DPI, RPI, APB, APN, Apia, IPI, app, Ali, AAI, ACI, AEI, AFI, AFPI, AGI, AHI, AI, AII, AJI, ANI, AOI, AP, APE, APF, APH, APII, APJ, APK, APL, APO, APP, APQ, APR, APRI, APT, APU, APV, APW, APX, APY, APZ, AQI, ARI, ASPI, ATI, AUI, AVI, AYI, AZI, BPI, DAPI, EPI, LPI, MAPI, MPI, NPI, OPI, SPI, TAPI, TPI, VPI, dpi, a pi, ABI, ADI, AMI, APIs, APs, Abi, KPI, ai, a, and, I, as, at, an, any, are, all, age, up, air, art, April, act, anti, pay, acid, add, ago, aid, am, arm, eye, eyes, map, paid, pair, rapid, Asia, FBI, PA, PC, ad, ask, axis, cap, gap, hip, lap, maps, mph, pain, spin, tape, tip, AFC, CPU, Nazi, PM, ads, aka, arc, ash, caps, epic, laps, pan, pin, pit, pm, rap, rpm, ski, spy, taxi, ups, GPS, HP, PCI, PR, PT, RPG, Wei, ace, aft, akin, amid, apex, arid, ate, avid, cape, eyed, gaps, lip, opt, pH, pad, par, pie, pig, spa, tap, ACM, ACS, ADC, ADP, AF, ANSI, AOL, ATC, ATM, ATV, Alps, Ana, Audi, Axis, CGI, CPA, CPC, CPR, CSI, GI, GPA, GPL, GUI, IPA, IPO, JP, LAPD, MRI, Mali, NP, NPC, NPR, PD, PPP, PRI, PV, RPM, SAP, VP, WPA, Wii, ale, amp, ant, apes, apse, aria, aux, awe, dip, hi, pa, pas, ppm, psi, rip, sap, sci, spit, zip, AAU, ADR, AEC, AEG, AIF, AIG, AMX, APEC, ARL, ARP, ASB, ASD, Abe, Ada, Aug, Ava, BPA, Bali, CBI, CCI, DCI, DUI, EPP, FP, GPO, GPU, HPV, Hui, IPC, IPS, Kali, LPG, MPA, MPH, MPP, Mani, Mari, Maui, NPA, NPS, Napa, OSI, PJ, PKI, PPD, Pa, RPC, RPF, SGI, SPL, TDI, UPC, UPS, VPN, WWI, ah, amps, ark, avg, capo, maxi, moi, nape, ops, pal, paw, raps, taps, AAT, ABM, ACU, AD, ADD, ADT, AEF, AFM, AFN, AGC, AGS, AHS, AIT, ARF, ASN, Agni, Ara, BPD, BPM, CAI, CLI, CPE, CPM, CPO, CTI, Cali, Capri, DRI, DTI, EPG, FCI, FDI, FSI, GDI, GPC, Hopi, IAP, IPN, IPP, IPR, Kari, LSI, MDI, MPR, MPV, MSI, Magi, NPD, PMI, PW, RMI, RPA, RPO, RPR, RSI, SBI, SDI, SPF, TBI, TPA, TPC, TPM, WAP, aphid, asp, av, ax, aye, gape, mpg, nap, pat, phi, pic, pip, ppb, pt, sari, sip, wadi, ACD, ACG, ACH, AGR, AUT, AZT, Adm, Avis, BRI, BWI, CFI, CNI, CPF, DPD, DPT, DWI, Dali, EPL, FPA, HMI, Jami, LPM, LPO, LPP, PX, RCI, RPN, RPT, SMI, Saki, Tupi, Uzi, abs, aim, alps, amt, fps, kepi, kip, magi, mp, nip, obi, pail, pap, papa, tapir, vapid, AFO, DLI, DPN, DPs, EPF, FPO, FYI, Fri, GMI, GPM, GPV, HRI, LPN, MPE, OAP, TPL, TPN, Tami, VRI, adj, ado, aha, ain, alp, aping, aspic, awl, bani, bps, naps, okapi, pah, pk, rapt, saps, topi, wpm, zap, zaps, AHQ, Amie, Ariz, Lapp, MLI, adv, ail, aped, apps, carpi, hap, kph, opp, yip, APDU, Jpn, asps, lapin, tali, yap, yaps, AAA, ABC, AMC, ATP, Afr, Amy, Ann, IP, PS, RBI, apish, ave, cpl, dpt, jape, op, rps, cpd, ppr, A, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26, A27, A28, A29, A30, A31, A32, A33, A34, A35, A36, A37, A38, A39, A40, A41, A42, A43, A44, A45, A46, A47, A48, A49, A50, A51, A52, A53, A54, A55, A56, A57, A58, A59, A60, A61, A62, A63, A64, A65, A66, A67, A68, A69, A6M, A70, A71, A72, A73, A74, A75, A76, A77, A78, A79, A80, A81, A82, A83, A84, A85, A86, A87, A88, A89, A90, A91, A92, A93, A94, A95, A96, A97, A98, A99, A9C, AA, AAB, AAC, AAD, AAE, AAF, AAG, AAH, AAIB, AAJ, AAK, AAL, AAM, AAN, AAO, AAP, AAQ, AAR, AAS, AAV, AAW, AAX, AAY, AAZ, AB, ABA, ABB, ABD, ABF, ABG, ABH, ABIS, ABJ, ABK, ABL, ABN, ABP, ABQ, ABR, ABS, ABT, ABU, ABV, ABW, ABX, ABZ, AC, ACA, ACB, ACC, ACDI, ACF, ACJ, ACK, ACL, ACN, ACO, ACP, ACR, ACT, ACV, ACW, ACX, ACY, ACZ, ADA, ADB, ADF, ADG, ADH, ADIE, ADJ, ADK, ADL, ADM, ADN, ADQ, ADS, ADSI, ADV, ADX, ADY, ADZ, AE, AEA, AEB, AED, AEE, AEJ, AEK, AEL, AEM, AEN, AEO, AEP, AEQ, AER, AES, AET, AEU, AEV, AEW, AEX, AEZ, AFA, AFB, AFD, AFE, AFF, AFG, AFH, AFIA, AFIS, AFJ, AFK, AFL, AFPA, AFPS, AFQ, AFR, AFS, AFT, AFU, AFV, AFW, AFX, AFZ, AG, AGB, AGD, AGE, AGF, AGG, AGH, AGJ, AGK, AGL, AGM, AGN, AGO, AGP, AGPL, AGPM, AGQ, AGT, AGU, AGV, AGW, AGX, AGY, AGZ, AHA, AHB, AHC, AHD, AHE, AHF, AHG, AHH, AHIP, AHJ, AHK, AHL, AHM, AHN, AHO, AHR, AHSI, AHT, AHU, AHV, AHW, AHX, AHY, AHZ, AIB, AIC, AID, AIE, AIH, AIIB, AIIC, AIJ, AIK, AIM, AIO, AIP, AIPE, AIPP, AIQ, AIS, AIU, AIV, AIW, AIY, AIZ, AIs, AJ, AJB, AJC, AJF, AJG, AJH, AJJ, AJK, AJL, AJN, AJO, AJP, AJQ, AJR, AJS, AJT, AJU, AJV, AJW, AJY, AK, AKB, AKC, AKD, AKE, AKF, AKG, AKJ, AKK, AKL, AKM, AKN, AKO, AKP, AKQ, AKR, AKS, AKT, AKU, AKV, AKX, AKY, AL, ALB, ALC, ALD, ALE, ALG, ALH, ALJ, ALK, ALL, ALM, ALN, ALO, ALP, ALPA, ALQ, ALR, ALS, ALV, ALW, ALX, ALZ, AM, AMA, AMD, AME, AMF, AMG, AMH, AMIS, AMJ, AMK, AML, AMM, AMN, AMO, AMP, AMQ, AMR, AMS, AMT, AMU, AMV, AMW, AMY, AMZ, AN, ANB, ANC, AND, ANE, ANF, ANG, ANH, ANIA, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANPE, ANPN, ANPS, ANQ, ANR, ANS, ANT, ANV, ANW, ANX, ANY, ANZ, AO, AOA, AOB, AOC, AOD, AOE, AOF, AOG, AOH, AOJ, AOK, AOM, AON, AOO, AOP, AOQ, AOR, AOS, AOT, AOU, AOV, AOW, AOX, APDR, APPR, APRS, AQA, AQB, AQC, AQF, AQG, AQH, AQK, AQL, AQM, AQMI, AQP, AQPA, AQPC, AQR, AQS, AQT, AQY, AR, ARB, ARC, ARD, ARE, ARH, ARIA, ARIC, ARJ, ARK, ARM, ARN, ARPU, ARQ, ARR, ARSI, ART, ARU, ARV, ARW, ARX, ARY, ASA, ASC, ASE, ASF, ASFI, ASG, ASH, ASIN, ASJ, ASK, ASL, ASM, ASP, ASPA, ASQ, ASR, ASS, AST, ASU, ASV, ASW, ASX, ASY, ASZ, AT, AT3, ATA, ATAPI, ATB, ATD, ATE, ATF, ATG, ATH, ATK, ATL, ATN, ATO, ATPL, ATQ, ATR, ATS, ATT, ATU, ATW, ATX, ATY, ATZ, AU, AUA, AUC, AUD, AUE, AUF, AUG, AUJ, AUL, AUM, AUN, AUO, AUP, AUQ, AUR, AUS, AUU, AUW, AUX, AUY, AUZ, AV, AVA, AVB, AVC, AVE, AVF, AVG, AVH, AVK, AVL, AVN, AVO, AVP, AVR, AVS, AVU, AVV, AVW, AVX, AVY, AVZ, AWA, AWB, AWD, AWE, AWG, AWK, AWM, AWN, AWP, AWR, AWS, AWU, AWZ, AX, AXB, AXC, AXD, AXG, AXK, AXL, AXM, AXN, AXP, AXR, AXS, AXV, AXX, AYA, AYC, AYD, AYE, AYG, AYH, AYJ, AYL, AYN, AYO, AYP, AYQ, AYR, AYS, AYU, AYY, AYZ, AZ, AZA, AZB, AZD, AZE, AZF, AZN, AZO, AZS, AZW, AZZ, Ac, Ag, Al, Ala, Alpo, Am, Ar, Ark, Art, As, At, Ats, Au, Av, Ave, BAII, BAP, BAPR, BBI, BCI, BDI, BEI, BFI, BGI, BHI, BII, BJI, BKI, BLI, BMI, BNI, BOI, BP, BPB, BPC, BPF, BPG, BPH, BPK, BPL, BPN, BPO, BPP, BPR, BPS, BPT, BPU, BPY, BSI, BTI, BUI, BVI, BYI, Bi, CAP, CAPO, CAPOI, CAPV, CDI, CEI, CFPI, CHI, CI, CII, CMI, COI, CPD, CPG, CPH, CPIE, CPJ, CPL, CPS, CPV, CQI, CVI, Caph, Capt, Chi, Ci, Cpl, DAI, DDI, DEI, DFI, DGI, DHI, DI, DJI, DMI, DOI, DP, DPA, DPB, DPC, DPE, DPH, DPIC, DPMI, DPO, DPS, DPU, DPW, DSI, DVI, DZI, Di, EAI, EAP, EBI, EDI, EFI, EI, ELI, EMI, ENI, EPA, EPCI, EPIC, EPM, EPO, EPR, EPS, EPSI, EPT, EPU, EPV, ESI, Eli, FAI, FAP, FCPI, FEI, FFI, FGI, FI, FII, FJI, FMI, FPD, FPF, FPH, FPJ, FPM, FPP, FPR, FPS, FPT, FPU, FPV, FTI, FWI, GAFI, GAP, GCI, GEI, GFI, GHI, GOI, GP, GPE, GPG, GPIS, GPP, GPR, GPX, GPZ, GRI, GSI, GTI, Gap, HAI, HAP, HCI, HDI, HI, HKI, HOI, HPA, HPC, HPD, HPE, HPL, HPS, IAI, IASI, ICI, IFI, IGI, IOI, IPB, IPD, IPE, IPF, IPG, IPJ, IPM, IPNI, IPT, IPY, IRI, ISI, IVI, JAI, JAP, JNI, JPA, JPF, JPN, JPO, JPP, JPR, JPX, JPY, JRI, Jap, Japs, KOI, KP, KPD, KPF, KPL, KPN, KPU, KPW, Kip, LBI, LFI, LHI, LI, LNI, LOI, LP, LPC, LPD, LPE, LPF, LPJ, LPQ, LPR, LRI, LTI, Li, M3I, MAP, MAPA, MBI, MCI, MFI, MGI, MI, MMI, MOI, MP, MPC, MPD, MPL, MPM, MPN, MPS, MPSI, MPT, MPX, Mai, NAP, NI, NMI, NNI, NOI, NP0, NPAI, NPF, NPG, NPL, NPO, NPP, NPV, NUI, Ni, Np, OACI, OAI, OCI, ODAPI, ODI, OHI, OMI, OMPI, OPA, OPG, OPK, OPL, OPM, OPO, OPQ, OPT, ORI, OUI, P, PAP, PAPS, PB, PBI, PDI, PE, PEI, PG, PGI, PH, PHI, PIB, PIC, PID, PIE, PIF, PIJ, PIL, PIM, PIN, PIO, PIP, PIS, PIT, PIX, PIZ, PK, PL, PNI, PO, POI, PP, PP1, PP2, PP3, PP4, PP7, PPA, PPB, PPC, PPE, PPH, PPK, PPL, PPM, PPN, PPO, PPQ, PPR, PPRI, PPS, PPT, PPU, PPV, PQ, PTI, PU, PVI, PWI, PXI, Pb, Pd, Pei, Pl, Pm, Po, Pr, Pt, Pu, QI, QPC, QPM, QPO, QPV, RAI, RAP, RDI, REI, RFI, RI, RLI, ROI, RP, RPB, RPD, RPE, RPK, RPP, RPS, RPU, RPV, RPX, RRI, RVI, SAI, SCI, SCPI, SFI, SI, SII, SKI, SLI, SNI, SOI, SP, SP1, SPA, SPB, SPC, SPE, SPG, SPH, SPM, SPP, SPR, SPS, SPT, SPV, SPW, SRI, SSI, SSPI, STI, SVI, Shi, Si, Sp, Sui, TAI, TAP, TCI, TEI, TGI, TI, TMI, TP, TPE, TPF, TPG, TPJ, TPO, TPP, TPR, TPS, TPT, TPV, TRI, TSI, TTI, Ti, UAI, UAP, UFI, UGI, UI, UMI, UNI, UP, UP1, UPA, UPB, UPD, UPE, UPF, UPK, UPN, UPP, UPR, UPU, UPV, URI, USI, VAP, VCI, VDI, VEI, VFI, VI, VMI, VPC, VPE, VPH, VPP, VPS, WDI, WI, WP, WPC, WPF, WPS, WRI, XAP, XP, XPF, XPM, XPS, Xi, YP, ZAP, ZAPA, ZI, ZPE, ZPP, ZTI, aah, ab, ac, alb, alt, ang, ans, arr, ass, auk, aw, awn, bap, baps, bi, bpm, capt, chi, cps, hp, i, ii, iii, lei, lii, lvi, lxi, mi, oi, p, paps, pd, pf, pg, pis, pix, pj, pl, poi, pp, pr, rape, spic, spiv, ti, uni, vape, vi, vii, xci, xi, xii, xvi, xxi, A&amp;A, A&amp;D, A&amp;E, A&amp;M, A&amp;P, A&amp;R, AAPL, ACE, ACh, ADU, AFP, AGA, AGs, AHCI, AIX, ALF, ALU, AMPQ, AOPA, APAC, APFA, APFC, APFS, APKs, APNG, APNs, APRN, ARPA, ASIC, ASPR, AVIF, AVM, AVs, AWC, AWF, AXA, Abu, Ajië, Aldi, Amir, Amit, Anil, Aon, Arif, Asif, Aziz, CP, CPN, CPP, CPQ, CPT, CZI, DAP, DNI, Dalí, Dani, EOI, EP, EPC, EPs, FPC, FPs, GNI, GPF, GPT, HPR, IFPI, INI, IPs, JCI, JPG, KPIs, Kai, LPA, LPL, LPS, LPs, MFi, MP3, MP4, MPs, MYI, NCI, NPM, NPN, NRI, NTI, PN, Pia, PyPi, RAPL, RKI, RTI, Ravi, SPD, Sami, Sri, TOI, TPU, TPX, TUI, Tai, UCI, UPT, UTI, VPs, WPP, WaPo, XPC, Xavi, Yi, ahh, ais, amu, aww, axe, açai, mCi, padi, pax, pc, più, ppl, tai] (4560) [lt:en:MORFOLOGIK_RULE_EN_US]">$api$</span> respectively, while interneuron</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">dendrites are simply denoted <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [end, dead, send, tend, deny, bend, deed, den, lend, fend, Deng, dens, mend, dent, DND, Dena, rend, vend, pend, Bend, END, MEND, wend, d end, DnD] (4625) [lt:en:MORFOLOGIK_RULE_EN_US]">$dend$</span>.  The derivative somatic membrane potentials of layer $l$ pyramidal neurons is given</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">by:</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">  C_m \dot{u}_l^P &amp; = - g_l u_l^{P} + g^{bas} v_l^{bas} + g^{api} v_l^{api} <span class="keyword1">\label</span>{eq-pyr-dynamics-rate}</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (4719) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $g_l$ is the somatic leakage conductance, and $C_m$ is the somatic membrane capacitance which will be assumed to</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">be $1$ from here on out. $v_l^{bas}$ and $v_l^{api}$ are the membrane potentials of basal and apical dendrites</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">respectively, and $g^{bas}$ and $g^{api}$ their corresponding coupling <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (4974) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span>.  Dendritic compartments in this</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">model have no persistence between simulation steps. Thus, they are defined at every <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [time step] (5104) [lt:en:MORFOLOGIK_RULE_EN_US]">timestep</span> $t$ through incoming weight</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">matrices and presynaptic activities:</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">  v_l^{bas}(t) &amp; = W_l^{up} \ \phi(u_{l-1}^P(t)) <span class="keyword1">\label</span>{eq-v-bas-rate}                                     \\</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">  v_l^{api}(t) &amp; =  W_l^{pi} \ \phi(u_l^I(t)) \ + \  W_l^{down} \ \phi(u_{l+1}^P(t)) <span class="keyword1">\label</span>{eq-v-api-rate}</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">The nomenclature for weight matrices conforms to \citep{Haider2021}, where they are indexed by the layer in which their</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">target neurons lie, and belong to one of four populations: Feedforward and feedback pyramidal-to-pyramidal connections</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">arriving at layer $l$ are denoted $W_l^{up}$ and $W_l^{down}$ respectively. Lateral pyramidal-to-interneuron connections</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">are denoted with $W_l^{ip}$ and their corresponding feedback connections with $W_l^{pi}$.</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">Interneurons integrate synaptic information by largely the same principle, but instead of top-down signals from their</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">sister neurons arriving at an apical compartment, it is injected directly into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, Lomé, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, Sofía, Sonoma, Souza, TOML, Tomás, UOM, Zora, hola, momma, sRNA, semé, simp, socs, stomas, Škoda] (5784) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span>.</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">  C_m \dot{u}_l^I &amp; = - g_l u_l^{I} + g^{dend} v_l^{dend} + i^{nudge, I}<span class="keyword1">\label</span>{eq-intn-dynamics} \\</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">  i^{nudge, I}    &amp; = g^{nudge, I} u_{l+1}^P                                                     \\</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">  v_l^{dend}      &amp; = W_l^{ip} \ \phi(u_{l}^P)</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (5792) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $ g^{nudge, I}$ is the interneuron nudging conductance, and $u_{l+1}^P$ is the somatic voltage of pyramidal</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">neurons in the next layer.  Pyramidal neurons in the output layer $N$ effectively behave like interneurons, as they</div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">receive no input to their apical compartment. Instead, the target  activation $u^{tgt}$ is injected into their <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, Lomé, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, Sofía, Sonoma, Souza, TOML, Tomás, UOM, Zora, hola, momma, sRNA, semé, simp, socs, stomas, Škoda] (6099) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span>:</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">  C_m \dot{u}_N^P &amp; = - g_l u_N^{P} + g^{bas} v_N^{bas} + i^{nudge, tgt} \\</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">  i^{nudge, tgt}  &amp; = g^{nudge, tgt} u^{tgt}</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">These neuron dynamics correspond closely to those described in \citep{urbanczik2014learning}, including the extension to</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">more than two compartments which was proposed in the original paper. It should be noted however, that they are</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">simplified in some ways. These simplifications enabled the authors to prove analytically that this model approximates</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (6432) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>. Yet they do come at the cost of omitting neuroscientific insights from the model, which will be discussed</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">later.</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 50 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This section is very short (about 51 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span></span>{<span class="highlight-spelling" title="Possible spelling mistake found. (6556) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> Plasticity}<span class="keyword1">\label</span>{sec-urb-senn-plast}</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline">The synapses in the network are all modulated according to variations of the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span><span class="highlight-spelling" title="Possible spelling mistake found. (6661) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]"></span>"</span> plasticity rule</div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">\citep{urbanczik2014learning}, which will be discussed in this section. Note that as for the neuron model, the dendritic</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline">error model slightly simplifies some equations of the plasticity rule from its original implementation.</div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline"><span class="keyword1">\subsection</span>{Derivation}</div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">The plasticity rule is defined for postsynaptic neurons which have one somatic and at least one dendritic compartment,</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline">to the latter of which synapses of this type can connect. Functionally, synaptic weights are changed in such a way, as</div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline">to minimize discrepancies between the somatic activity and dendritic potential. This discrepancy is called the</div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline"><span class="keyword1">\textit</span>{dendritic prediction error}, and is computed from a hypothetical dendritic activation. The change in weight for</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline">a synapse from neuron $j$ to the basal compartment of a pyramidal neuron <span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (7436) [lt:en:I_LOWERCASE]">$i$</span> is given by:</div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">  \dot{w}_{ij}    &amp; = \eta \ ( \phi(u_i^{som}) - \phi(\hat{v}_i^{bas}) ) \ \phi(u_j^{som})^T \\</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">  \hat{v}_i^{bas} &amp; = \alpha \  v_i^{bas}</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [With] (7453) [lt:en:UPPERCASE_SENTENCE_START]">with</span> learning rate $\eta$, and $u^T$ denoting the transposition of the vector $u$ (which is by default assumed a column</div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline">vector). The dendritic prediction $\hat{v}_i^{bas}$ is a scaled version of the dendritic potential by the constant</div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline">factor $\alpha$, which is calculated from coupling and leakage <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [conductance] (7717) [lt:en:MORFOLOGIK_RULE_EN_US]">conductances</span>. As an example, basal dendrites of pyramidal</div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline">neurons in \citep{sacramento2018dendritic} are attenuated by $\alpha = \frac{g^{bas}}{g_l + g^{bas} + g^{api}}$. A key</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline">property of this value for $\alpha$ is, that dendritic error is $0$ when the only input to a neuron stems from the given</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline">dendrite. In other words, the dendrite predicts somatic activity perfectly, and no change in synaptic weights is</div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline">required. Neuron- and layer-specific differences in $\alpha$, as well as an analytical derivation are detailed in</div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline">\citep{sacramento2018dendritic}.</div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline">If a current is injected into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, Lomé, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, Sofía, Sonoma, Souza, TOML, Tomás, UOM, Zora, hola, momma, sRNA, semé, simp, socs, stomas, Škoda] (8189) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> (or in this case, into a different dendrite), a dendritic error arises, and</div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline">plasticity drives synaptic weights to minimize it. In addition to the learning rate $\eta$, the change in weight</div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline">$\dot{w}_{ij}$ is proportional to presynaptic activity $\phi(u_j^{som})$. Therefore, a dendritic error arising without</div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">presynaptic contribution does not elicit a change in that particular synapse. This ensures that only synapses are</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline">modified which recently influenced the postsynaptic neuron, providing a form of credit assignment. Updates for the</div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">weight matrices in a hidden layer $l$ of the dendritic error model are given by:</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">  \dot{w}_{l}^{up}   &amp; = \eta_l^{up} \ ( \phi(u_l^{P}) - \phi(\hat{v}_l^{bas}) ) \ \phi(u_{l-1}^{P})^T<span class="keyword1">\label</span>{eq-delta_w_up}         \\</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline">  \dot{w}_{l}^{ip}   &amp; = \eta_l^{ip} \ ( \phi(u_l^{I}) - \phi(\hat{v}_l^{dend}) ) \ \phi(u_{l}^{P})^T<span class="keyword1">\label</span>{eq-delta_w_ip}          \\</div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">  \dot{w}_{l}^{pi}   &amp; = \eta_l^{pi} \ - v_l^{api} \ \phi(u_l^{I})^T<span class="keyword1">\label</span>{eq-delta_w_pi}                                           \\</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">  \dot{w}_{l}^{down} &amp; = \eta_l^{down} \ ( \phi(u_l^{P}) - \phi(w_l^{down} r_{l+1}^P) )\ \phi(u_{l+1}^{P})^T<span class="keyword1">\label</span>{eq-delta_w_down}</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">Each set of connections is updated with a specific learning rate $\eta$ and a specific dendritic error term. The purpose</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">of these particular dendritic errors will be explained in Section \ref{sec-selfpred}. Note that pyramidal-to-pyramidal</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">feedback weights $w_l^{down}$ are not plastic in the present simulations and are only listed for completeness, see</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">Section \ref{sec-feedback-plast} for an investigation into this.</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline"><span class="keyword1">\section</span>{The self-predicting network state}<span class="keyword1">\label</span>{sec-selfpred}</div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline">In the dendritic error model neuron dynamics, plasticity rules and network architecture form an elegant interplay, which</div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline">will be explained in this section. Since each interneuron receives a somatic nudging signal from its corresponding</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline">sister neuron, incoming synapses from lateral pyramidal neurons adapt their weights to match feedforward</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline">pyramidal-to-pyramidal weights. In intuitive terms; Feedforward pyramidal-to-pyramidal weights elicit a certain</div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">activation in the subsequent layer, which is fed back into corresponding interneurons. Hence, in the absence of incoming</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline">connections, nudging from sister neurons causes interneurons to take on a proportional somatic potential. In order to</div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">minimize the dendritic error term in Equation \ref{eq-delta_w_ip}, pyramidal-to-interneuron weight matrices at every</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline">layer must match these forward weights ($w_l^{ip} \approx \rho w_l^{up}$) up to some scaling factor $\rho$. The exact</div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">value for $\rho$ is parameter-dependent and immaterial for now. As long as no feedback information arrives at the</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline">pyramidal neurons, plasticity drives synaptic weight to fulfill this constraint. Note, that this alignment of two</div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline">separate sets of outgoing weights is achieved with only local information. Therefore, this mechanism could plausibly</div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline">align the weights of biological synapses that are physically separated by long distances. \newline</div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline">Next, consider the special case for interneuron-to-pyramidal weights in Equation \ref{eq-delta_w_pi}, in which</div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline">plasticity does not serve to reduce discrepancies between dendritic and somatic potential. The error term is instead</div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline">defined solely by the apical compartment <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [voltage In] (10742) [lt:en:MORFOLOGIK_RULE_EN_US]">voltage\footnote{In</span> strict terms, it is defined by the deviation of the</div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline">dendritic potential from its specific reversal potential. Since that potential is zero throughout, $- v_l^{api}$ remains</div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline">as the error term<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (10930) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Thus, plasticity in these synapses works towards silencing the apical compartment. The apical</div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">compartments also receive feedback from superficial pyramidal neurons, whose synapses will be considered non-plastic for</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline">now. As shown above, interneurons each learn to match their respective sister neuron activity. Thus, silencing of apical</div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">compartments can only be achieved by mirroring the pyramidal-to-pyramidal feedback weights (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PW, cw, MW, CW, EW, KW, NW, SW, TW, W, WW, YW, aw, kW, kw, ow, w, DW, FW, GW, HW, UW, VW, mW] (11361) [lt:en:MORFOLOGIK_RULE_EN_US]">$w</span>_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [LPI] (11364) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{pi}</span> \approx</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">  -w_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [down, Downs, downs, downy, letdown, Downy, lowdown, letdowns, Ladonna] (11382) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{down}$</span>).\newline</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">When enabling plasticity in only these two synapse types, the network converges on the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"<span class="keyword1">\textbf</span>{</span>self-predicting state}<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]"></span>"</div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline">\citep{sacramento2018dendritic}. This state is defined by a minimization of four error metrics at each hidden layer $l$:</div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline"><span class="keyword2">\begin{itemize}</span></div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline">  <span class="keyword1">\item</span> The symmetries between feedforward ($w_l^{ip} \approx \rho w_l^{up}$) and feedback (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PW, cw, MW, CW, EW, KW, NW, SW, TW, W, WW, YW, aw, kW, kw, ow, w, DW, FW, GW, HW, UW, VW, mW] (11660) [lt:en:MORFOLOGIK_RULE_EN_US]">$w</span>_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [LPI] (11663) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{pi}</span> \approx</div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline">          -w_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [down, Downs, downs, downy, letdown, Downy, lowdown, letdowns, Ladonna] (11689) [lt:en:MORFOLOGIK_RULE_EN_US]">l^{down}$</span>) weights. <span class="keyword1">\textit</span>{Mean squared error (MSE)} between these pairs of matrices will be called <span class="keyword1">\textbf</span>{Feedforward - }</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">        and <span class="keyword1">\textbf</span>{Feedback weight error} respectively.</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline">  <span class="keyword1">\item</span> Silencing of pyramidal neuron apical compartments ($v_l^{api} \approx 0$). Mean absolute apical compartment</div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">        voltage within a layer is called the <span class="keyword1">\textbf</span>{Apical error}.</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline">  <span class="keyword1">\item</span> Equal activations in interneurons and their respective sister neurons ($\phi (u_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [LSI, LBI, LFI, LHI, LI, LNI, LOI, LPI, LRI, LTI, Li, lei, lii, lvi, lxi] (12073) [lt:en:MORFOLOGIK_RULE_EN_US]">l^I</span>) \approx \phi</div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">          (u_{l+1}^P)$). The mean squared error over these vectors is called the <span class="keyword1">\textbf</span>{Interneuron error}.</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline"><span class="keyword2">\end{itemize}</span></div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">The network does not ever reach a state in which all of these error terms are exactly zero. In the original</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline">implementation, these deviations are minute and can likely be explained with floating point conversions. Since it is</div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">impossible to replicate the timing of the original precisely within NEST, the NEST simulations deviate more strongly</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">from this ideal. The key insight here is that this state is not clearly defined by absolute error thresholds, but is</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline">rather flexible. Thus, networks are able to learn successfully even when their weights are initialized imperfectly.</div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">An analysis of the equations describing the network reveals that the idealized self-predicting state forms a stable</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline">point of minimal energy. When Interneuron error is zero, the nudging signal from sister neurons is predicted perfectly,</div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">thus disabling plasticity in incoming synapses. Likewise, a silenced apical compartment will disable plasticity in all</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline">incoming synapses from interneurons. Furthermore, the apical compartment is also the driving factor for the dendritic</div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">error of feedforward synapses (Equation \ref{eq-delta_w_up}), since any nonzero potential leaks into the</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [somatic, Mathis, Somalis] (13328) [lt:en:MORFOLOGIK_RULE_EN_US]">soma\footnote{This</span> property might actually be considered the purpose of the <span class="highlight-spelling" title="Possible spelling mistake found. (13394) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity. In the original</div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">  paper, currents were injected directly into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, Lomé, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, Sofía, Sonoma, Souza, TOML, Tomás, UOM, Zora, hola, momma, sRNA, semé, simp, socs, stomas, Škoda] (13487) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> to change the error term. In biological neurons, introducing a</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline">  second dendrite which performs that very task makes far more sense<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (13623) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Thus, in the self-predicting state all <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [plasticity] (13665) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>plasticiy</div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline">in the network is disabled, and the state is stable regardless of the kind of stimulus injected into the input layer.</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline">Next, notice how information flows backwards through the network; All feedback pathways between layers ultimately pass</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline">through the apical compartments of pyramidal neurons. Thus, successful silencing of all apical compartments implies that</div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">no information can travel backwards between layers. As a result, the network behaves strictly like a fully connected</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline">feedforward network consisting only of pyramidal neurons. The recurrence within this network is in balance, and</div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">completely cancels out its own effects. This holds true as long as the network only receives external stimulation at the</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline">input layer. One interpretation of this is, that the network has learned to predict its own top-down input. A failure by</div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline">interneurons to fully explain (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>cancel out) top-down input thus results in a prediction error, encoded in deviation</div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline">of apical dendrite potentials from their resting state. This prediction error in turn elicits a cascade of plasticity in</div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline">several synapses, which drives the network towards a self-predicting state that is congruent with the novel top-down</div><div class="clear"></div>
<div class="linenb">252</div><div class="codeline">signal. The authors show analytically that this intricate mechanism can be recast as a gradient descent optimization.</div><div class="clear"></div>
<div class="linenb">253</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">254</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">255</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">256</div><div class="codeline"><span class="keyword1">\section</span>{Training the network}</div><div class="clear"></div>
<div class="linenb">257</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">258</div><div class="codeline">Training the network then requires only the injection of a target activation into the network's output layer alongside</div><div class="clear"></div>
<div class="linenb">259</div><div class="codeline">with a stimulus at the input layer. Since output layer neurons have strong feedback connections, a prediction error</div><div class="clear"></div>
<div class="linenb">260</div><div class="codeline">arises in the previous layer. Synapses then drive to minimize this error by creating a new self-predicting state in</div><div class="clear"></div>
<div class="linenb">261</div><div class="codeline">which interneurons mirror the novel <span class="highlight-spelling" title="Possible spelling mistake. 'behaviour' is British English.. Suggestions: [behavior] (15392) [lt:en:MORFOLOGIK_RULE_EN_US]">behaviour</span> of their sisters. Note that this interaction is not exclusive to the last</div><div class="clear"></div>
<div class="linenb">262</div><div class="codeline">two layers. Any Apical errors elicit a change in somatic activity, which preceding interneurons will fail to predict.</div><div class="clear"></div>
<div class="linenb">263</div><div class="codeline">Thus, errors are propagated backwards through arbitrarily deep networks, causing error minimization at every layer. See</div><div class="clear"></div>
<div class="linenb">264</div><div class="codeline">the Supplementary analysis of \citep{sacramento2018dendritic} for a rigorous proof that this type of network does indeed</div><div class="clear"></div>
<div class="linenb">265</div><div class="codeline">approximate the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (15823) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> algorithm.</div><div class="clear"></div>
<div class="linenb">266</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">267</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">268</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">269</div><div class="codeline">Classical <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (15863) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> relies on a strict separation of a forward pass of some stimulus, and subsequent a backwards</div><div class="clear"></div>
<div class="linenb">270</div><div class="codeline">pass dependent on the arising loss at the output layer. Since the present network is time-continuous, stimulus and</div><div class="clear"></div>
<div class="linenb">271</div><div class="codeline">target activation are injected into the network simultaneously. These injections are maintained for a given presentation</div><div class="clear"></div>
<div class="linenb">272</div><div class="codeline">time $t_{pres}$, in order to allow the network to calculate errors through its recurrent connections before slowly</div><div class="clear"></div>
<div class="linenb">273</div><div class="codeline">adapting weights. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Particularly] (16325) [lt:en:MORFOLOGIK_RULE_EN_US]">Particluarly</span> for deep networks, signals travelling from both the input and output layer require some</div><div class="clear"></div>
<div class="linenb">274</div><div class="codeline">time to balance out and elicit the correct dendritic error terms. This property poses the most significant drawback of</div><div class="clear"></div>
<div class="linenb">275</div><div class="codeline">this type of time-continuous approximation of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (16591) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span>: The network tends to overshoot activations in some</div><div class="clear"></div>
<div class="linenb">276</div><div class="codeline">neurons, which in turn causes an imbalance between dendritic and somatic compartments. This effect causes the network to</div><div class="clear"></div>
<div class="linenb">277</div><div class="codeline">change synaptic weights away from the desired state during the first few milliseconds of a stimulus presentation. The</div><div class="clear"></div>
<div class="linenb">278</div><div class="codeline">solution Sacramento <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>found for this issue was to drastically reduce learning rates, while increasing stimulus</div><div class="clear"></div>
<div class="linenb">279</div><div class="codeline">presentation time. This solution is sufficient to prove that plasticity in this kind of network is able to perform error</div><div class="clear"></div>
<div class="linenb">280</div><div class="codeline">propagation, but still has some issues. Most notably, training is highly inefficient and computationally intensive. A</div><div class="clear"></div>
<div class="linenb">281</div><div class="codeline">closer investigation of the issue together with a different solution will be discussed in Section \ref{sec-latent-eq}.</div><div class="clear"></div>
<div class="linenb">282</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">283</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">284</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">285</div><div class="codeline"><span class="keyword1">\section</span>{The NEST simulator}</div><div class="clear"></div>
<div class="linenb">286</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">287</div><div class="codeline">One of the key research questions motivating this thesis is whether the network would be able to learn successfully when</div><div class="clear"></div>
<div class="linenb">288</div><div class="codeline">employing spike-based communication instead of the rate neurons for which it was developed. As a framework for the</div><div class="clear"></div>
<div class="linenb">289</div><div class="codeline">spike-based implementation two options were considered: The first one was to use the existing implementation of the</div><div class="clear"></div>
<div class="linenb">290</div><div class="codeline">network which employs the Python frameworks \texttt{PyTorch} and \texttt{NumPy}, and expand it to employ spiking</div><div class="clear"></div>
<div class="linenb">291</div><div class="codeline">neurons. PyTorch does in principle support spiking communication between layers, but is streamlined for implementing</div><div class="clear"></div>
<div class="linenb">292</div><div class="codeline">less recurrent and less complex network and neuron models. Another concern is efficiency; PyTorch is very well optimized</div><div class="clear"></div>
<div class="linenb">293</div><div class="codeline">for computing matrix operations on dedicated hardware. This makes it a good choice for simulating large networks of rate</div><div class="clear"></div>
<div class="linenb">294</div><div class="codeline">neurons, which transmit all of their activations between layers at every simulation step. Spiking communication between</div><div class="clear"></div>
<div class="linenb">295</div><div class="codeline">leaky neurons is almost antithetical to this design philosophy and thus can be expected to perform comparatively poorly</div><div class="clear"></div>
<div class="linenb">296</div><div class="codeline">when using this backend.</div><div class="clear"></div>
<div class="linenb">297</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">298</div><div class="codeline">The second option was to use the NEST simulator</div><div class="clear"></div>
<div class="linenb">299</div><div class="codeline">(\href{https://nest-simulator.readthedocs.io}{nest-simulator.readthedocs.io}, <span class="highlight-sh" title="Do not mix \cite with \citep or \citet in the same document. [sh:c:itemix]">\cite{</span>Gewaltig2007}), which was developed</div><div class="clear"></div>
<div class="linenb">300</div><div class="codeline">with highly parallel simulations of large spiking neural networks in mind. It is written in C++ and uses the</div><div class="clear"></div>
<div class="linenb">301</div><div class="codeline"><span class="keyword1">\textit</span>{Message Passing Interface} (\href{https://www.mpi-forum.org/}{MPI}) to efficiently communicate events between</div><div class="clear"></div>
<div class="linenb">302</div><div class="codeline">both threads and compute nodes. One design pillar of the simulator, which is particularly relevant for this project, is</div><div class="clear"></div>
<div class="linenb">303</div><div class="codeline">the event-based communication scheme that underpins all simulated nodes. It ensures that communication bandwidth at</div><div class="clear"></div>
<div class="linenb">304</div><div class="codeline">every simulation step is only used by the subset of nodes which transmit signals at that time step, which is</div><div class="clear"></div>
<div class="linenb">305</div><div class="codeline">particularly efficient for spiking communication. Another important advantage of the NEST simulator is, that an</div><div class="clear"></div>
<div class="linenb">306</div><div class="codeline">event-based implementation of the <span class="highlight-spelling" title="Possible spelling mistake found. (19222) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity alongside a corresponding neuron model had already been</div><div class="clear"></div>
<div class="linenb">307</div><div class="codeline">developed for it. Therefore, it was decided to implement the spiking neuron model in the NEST simulator.</div><div class="clear"></div>
<div class="linenb">308</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">309</div><div class="codeline">The simulator has one particular limitation which needs to be considered. As communication between physically separate</div><div class="clear"></div>
<div class="linenb">310</div><div class="codeline">compute nodes takes time, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Events, Eventual] (19555) [lt:en:MORFOLOGIK_RULE_EN_US]">Events\footnote{An</span> Event in NEST is an abstract C++ Class that is created by neurons, and</div><div class="clear"></div>
<div class="linenb">311</div><div class="codeline">  transmitted across threads and compute nodes by the Simulator. A Multitude of Event types are provided (i.e.\</div><div class="clear"></div>
<div class="linenb">312</div><div class="codeline">  \texttt{<span class="highlight-spelling" title="Possible spelling mistake found. (19749) [lt:en:MORFOLOGIK_RULE_EN_US]">SpikeEvent</span>, <span class="highlight-spelling" title="Possible spelling mistake found. (19761) [lt:en:MORFOLOGIK_RULE_EN_US]">CurrentEvent</span>, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Reinvent, Atrovent, Reinterment] (19775) [lt:en:MORFOLOGIK_RULE_EN_US]">RateEvent}</span>), each able to carry specific types of payload and being processed</div><div class="clear"></div>
<div class="linenb">313</div><div class="codeline">  differently by postsynaptic neurons.} <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [In] (19891) [lt:en:UPPERCASE_SENTENCE_START]">in</span> NEST can not be handled in the same simulation step in which they were sent.</div><div class="clear"></div>
<div class="linenb">314</div><div class="codeline">Thus, NEST enforces a synaptic transmission delay of at least one simulation step for all connections. This property is</div><div class="clear"></div>
<div class="linenb">315</div><div class="codeline">integral to other parallel simulation backends \citep{Hines1997} as well as <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [meromorphic, neurotrophic] (20153) [lt:en:MORFOLOGIK_RULE_EN_US]">neuromorphic</span> hardware</div><div class="clear"></div>
<div class="linenb">316</div><div class="codeline">\citep{davies2018loihi}. It may not even <span class="highlight" title="The modal verb 'may' requires the verb's base form.. Suggestions: [consider] (20196) [lt:en:MD_BASEFORM]">considered</span> a limitation by some, as synaptic transmission within biological</div><div class="clear"></div>
<div class="linenb">317</div><div class="codeline">neurons is never instantaneous either \citep{kandel2021principles}. Yet particularly with regard to the relaxation</div><div class="clear"></div>
<div class="linenb">318</div><div class="codeline">period issue of this model (cf. Section \ref{sec-latent-eq}), it can be expected to affect performance.</div><div class="clear"></div>
<div class="linenb">319</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">320</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">321</div><div class="codeline"><span class="keyword1">\section</span>{Transitioning to spiking communication}</div><div class="clear"></div>
<div class="linenb">322</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">323</div><div class="codeline">The spiking neuron models rely heavily on the NEST implementation from <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (20561) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> and colleagues \citep{Stapmanns2021},</div><div class="clear"></div>
<div class="linenb">324</div><div class="codeline">which was used show that spiking neurons are able to perform learning tasks that were designed for the rate neurons</div><div class="clear"></div>
<div class="linenb">325</div><div class="codeline">described in \citep{urbanczik2014learning}. The existing model is an exact replication of the <span class="highlight-spelling" title="Possible spelling mistake found. (20775) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> neuron in</div><div class="clear"></div>
<div class="linenb">326</div><div class="codeline">terms of membrane dynamics. The critical update of the NEST variant is that instead of transmitting their hypothetical</div><div class="clear"></div>
<div class="linenb">327</div><div class="codeline">rate $r = \phi(u)$ at every time step, these neurons emit spikes <span class="highlight" title="Consider replacing this phrase with the adverb 'similarly' to avoid wordiness.. Suggestions: [similarly] (20972) [lt:en:IN_A_X_MANNER]">in a similar way</span> to stochastic binary neurons</div><div class="clear"></div>
<div class="linenb">328</div><div class="codeline">\citep{Ginzburg1994}. The number of spikes to be generated during a simulation step $n$ is determined by drawing from a</div><div class="clear"></div>
<div class="linenb">329</div><div class="codeline">Poisson distribution, which takes $r$ as a parameter:</div><div class="clear"></div>
<div class="linenb">330</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">331</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">332</div><div class="codeline">  P\{<span class="keyword1">\textit</span>{n} \ \text{spikes during} \ \Delta t\} &amp; = e^{-r \Delta t} \frac{(r \ \Delta t) ^ n}{n!}<span class="keyword1">\label</span>{eq-pr-n-spikes} \\</div><div class="clear"></div>
<div class="linenb">333</div><div class="codeline">  \langle <span class="keyword1">\textit</span>{n} \rangle                        &amp; = r \ \Delta t <span class="keyword1">\label</span>{eq-n-spikes}</div><div class="clear"></div>
<div class="linenb">334</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">335</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">336</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Where] (21173) [lt:en:UPPERCASE_SENTENCE_START]">where</span> $\Delta t$ denotes the integration time step of the simulator, which will be assumed to be $0.1 ms$ from here on</div><div class="clear"></div>
<div class="linenb">337</div><div class="codeline">out.  $\langle <span class="keyword1">\textit</span>{n} \rangle$ denotes the expected number of spikes to be emitted in a simulation step. Note that</div><div class="clear"></div>
<div class="linenb">338</div><div class="codeline">this mechanism makes the assumption that more than one spike can occur per simulation step. NEST was developed with this</div><div class="clear"></div>
<div class="linenb">339</div><div class="codeline">possibility in mind and provides a <span class="keyword1">\textit</span>{multiplicity} parameter for <span class="highlight-spelling" title="Possible spelling mistake found. (21551) [lt:en:MORFOLOGIK_RULE_EN_US]">SpikeEvents</span>, which is processed at the</div><div class="clear"></div>
<div class="linenb">340</div><div class="codeline">postsynaptic neuron. As the high spike frequencies resulting from this could not occur in biological neurons, the model</div><div class="clear"></div>
<div class="linenb">341</div><div class="codeline">is also capable of simulating a refractory period. For this, the number of spikes per step is limited to $1$, and the</div><div class="clear"></div>
<div class="linenb">342</div><div class="codeline">spiking probability is set to 0 for the duration of the refractory period $t_{ref}$. The probability of at least one</div><div class="clear"></div>
<div class="linenb">343</div><div class="codeline">spike occurring within the next simulation step is given the inverse probability of no spike occurring. Thus, when</div><div class="clear"></div>
<div class="linenb">344</div><div class="codeline">inserting $n=0$ into Equation \ref{eq-pr-n-spikes}, the probability of eliciting at least one spike within the next</div><div class="clear"></div>
<div class="linenb">345</div><div class="codeline">simulation step can be derived as:</div><div class="clear"></div>
<div class="linenb">346</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">347</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">348</div><div class="codeline">  P\{ <span class="keyword1">\textit</span>{n} \geq 1\} &amp; = 1 - e^{-r \Delta t}</div><div class="clear"></div>
<div class="linenb">349</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">350</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">351</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">352</div><div class="codeline">Drawing from this probability then determines whether a spike is sent during that step, henceforth denoted with</div><div class="clear"></div>
<div class="linenb">353</div><div class="codeline">the function $s(t)$, which outputs $1$ if a spike is sent during the interval $[t, t+\Delta t]$, and $0$ otherwise.</div><div class="clear"></div>
<div class="linenb">354</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">355</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">356</div><div class="codeline">In order to implement the plasticity rule for spiking neurons, dendritic compartments need to be modeled with leaky</div><div class="clear"></div>
<div class="linenb">357</div><div class="codeline">dynamics. These dynamics are fundamentally the same as those described for the somatic compartment. Thus, the basal</div><div class="clear"></div>
<div class="linenb">358</div><div class="codeline">compartment of a pyramidal neuron $j$ evolves according to:</div><div class="clear"></div>
<div class="linenb">359</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">360</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">361</div><div class="codeline">  C_m^{bas} \dot{v}_j^{bas} &amp; = -g_l^{bas} \  v_j^{bas} + \sum_{i \in I} W_{ji} s_i(t)     <span class="keyword1">\label</span>{eq-spiking-basal-compartment}</div><div class="clear"></div>
<div class="linenb">362</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">363</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">364</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [With] (22686) [lt:en:UPPERCASE_SENTENCE_START]">with</span> presynaptic neurons $I$, and membrane capacitance $C_m^{bas}$ and leakage conductance $g_l^{bas}$ being specific to</div><div class="clear"></div>
<div class="linenb">365</div><div class="codeline">the basal dendrite. Note that these equations are calculated individually for each neuron and do not employ the matrix</div><div class="clear"></div>
<div class="linenb">366</div><div class="codeline">notation used for layers of rate neurons. Pyramidal apical and interneuron dendritic compartments evolve by the same</div><div class="clear"></div>
<div class="linenb">367</div><div class="codeline">principle and with largely the same parameters.</div><div class="clear"></div>
<div class="linenb">368</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">369</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">section</span>{Event-based <span class="highlight-spelling" title="Possible spelling mistake found. (23082) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity}<span class="keyword1">\label</span>{sec-event-urb}</div><div class="clear"></div>
<div class="linenb">370</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">371</div><div class="codeline">One major challenge in implementing this architecture with spiking neurons is the <span class="highlight-spelling" title="Possible spelling mistake found. (23191) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity introduced</div><div class="clear"></div>
<div class="linenb">372</div><div class="codeline">in Section \ref{sec-urb-senn-plast}. Since the plasticity rule is originally defined for rate neurons, computing the</div><div class="clear"></div>
<div class="linenb">373</div><div class="codeline">updates for spiking neurons requires some additional effort. Fortunately, this problem has already been solved in NEST</div><div class="clear"></div>
<div class="linenb">374</div><div class="codeline">for two-compartment neurons \citep{Stapmanns2021}. This Section will discuss its algorithm and its implementation.</div><div class="clear"></div>
<div class="linenb">375</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">376</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">377</div><div class="codeline">Since NEST is an event-based simulator, most of the plasticity mechanisms developed for it compute weight changes at the</div><div class="clear"></div>
<div class="linenb">378</div><div class="codeline">location (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>thread and compute node) of the postsynaptic neuron whenever an Event is received. This has several</div><div class="clear"></div>
<div class="linenb">379</div><div class="codeline">advantages; It allows the thread that created the Event to continue processing neuron updates instead of having to</div><div class="clear"></div>
<div class="linenb">380</div><div class="codeline">synchronize with all threads that manage recipient neurons.  More importantly, this feature mirrors the local properties</div><div class="clear"></div>
<div class="linenb">381</div><div class="codeline">of most biologically plausible synaptic plasticity models, as these are often considered to be primarily dependent on</div><div class="clear"></div>
<div class="linenb">382</div><div class="codeline">factors that are local to the synapse \citep{magee2020synaptic}. For a spiking implementation of the <span class="highlight-spelling" title="Possible spelling mistake found. (24217) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>Urbanczik-Senn</div><div class="clear"></div>
<div class="linenb">383</div><div class="codeline">plasticity, dendritic errors at every time step are required instead of just a scalar trace at the time of a spike, as</div><div class="clear"></div>
<div class="linenb">384</div><div class="codeline">would be the case for STDP. Thus, a mechanism for managing these errors was required, for which two basic possibilities</div><div class="clear"></div>
<div class="linenb">385</div><div class="codeline">were considered:</div><div class="clear"></div>
<div class="linenb">386</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">387</div><div class="codeline">In a <span class="keyword1">\textbf</span>{Time-driven scheme}, dendritic errors are made available to synapses at every <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [time step] (24571) [lt:en:MORFOLOGIK_RULE_EN_US]">timestep</span>, and weight changes</div><div class="clear"></div>
<div class="linenb">388</div><div class="codeline">are applied instantaneously. This approach is in principle an adaptation of the original computations for <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike trains] (24706) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrains</span>.</div><div class="clear"></div>
<div class="linenb">389</div><div class="codeline">Its main drawback is that calls to the synaptic update function are as frequent as neuron updates - for all synapses.</div><div class="clear"></div>
<div class="linenb">390</div><div class="codeline">Particularly for large numbers of incoming synapses, as is common for simulations of cortical pyramidal neurons</div><div class="clear"></div>
<div class="linenb">391</div><div class="codeline">\citep{potjans2014cell,vezoli2004quantitative}, this requires numerous function calls per time step.  Therefore,</div><div class="clear"></div>
<div class="linenb">392</div><div class="codeline">this approach proved costly in terms of computational resources.</div><div class="clear"></div>
<div class="linenb">393</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">394</div><div class="codeline">An <span class="keyword1">\textbf</span>{Event-driven scheme} on the other hand, updates synaptic weights only when a spike is sent through the</div><div class="clear"></div>
<div class="linenb">395</div><div class="codeline">synapse. A history of the dendritic error is stored at the postsynaptic neuron, which is read by each synapse when a</div><div class="clear"></div>
<div class="linenb">396</div><div class="codeline">spike is transmitted in order to compute weight changes. As the history of dendritic error applies equally to all</div><div class="clear"></div>
<div class="linenb">397</div><div class="codeline">incoming synapses, it only needs to be recorded once at the neuron. Alongside each entry in the history, a counter is</div><div class="clear"></div>
<div class="linenb">398</div><div class="codeline">stored and incremented whenever a synapse has read the history at that time step. Once all synapses have read out an</div><div class="clear"></div>
<div class="linenb">399</div><div class="codeline">entry, it is deleted. Thus, the history dynamically grows and shrinks during simulation and is only ever as long as the</div><div class="clear"></div>
<div class="linenb">400</div><div class="codeline">largest inter-spike interval (ISI) of all presynaptic neurons. This approach proves to be more efficient in terms of</div><div class="clear"></div>
<div class="linenb">401</div><div class="codeline">computation time, since fewer calls to the update function are required per synapse. It does come at the cost of memory</div><div class="clear"></div>
<div class="linenb">402</div><div class="codeline">consumption, as the history can grow particularly large for simulations with low in-degrees or large <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [ISI It] (26114) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>ISI\footnote{It</div><div class="clear"></div>
<div class="linenb">403</div><div class="codeline">  should also be noted that in this approach requires redundant integration of the history by every synapse. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (26229) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> et</div><div class="clear"></div>
<div class="linenb">404</div><div class="codeline">  al. propose a third solution, in which this integration is performed once whenever a spike is transmitted through any</div><div class="clear"></div>
<div class="linenb">405</div><div class="codeline">  incoming connection, with the resulting weight change being applied to all synapses immediately. This approach proved to</div><div class="clear"></div>
<div class="linenb">406</div><div class="codeline">  be even more efficient for some network configurations, but is incompatible with simulations where incoming synapses</div><div class="clear"></div>
<div class="linenb">407</div><div class="codeline">  have heterogeneous synaptic delays due to the way that these delays are processed by the NEST simulator. See Section</div><div class="clear"></div>
<div class="linenb">408</div><div class="codeline">  3.1.3 in \citep{Stapmanns2021} for a detailed explanation<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (26764) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> During testing, the Event-based schemed proved substantially</div><div class="clear"></div>
<div class="linenb">409</div><div class="codeline">more efficient for many network types. This did however introduce the challenge of retroactively computing weight</div><div class="clear"></div>
<div class="linenb">410</div><div class="codeline">changes from the time of the last spike upon arrival of a new spike. \newline</div><div class="clear"></div>
<div class="linenb">411</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">412</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">413</div><div class="codeline"><span class="keyword1">\subsection</span>{Integrating weight changes}</div><div class="clear"></div>
<div class="linenb">414</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">415</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">416</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (27051) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>describe the <span class="highlight-spelling" title="Possible spelling mistake found. (27081) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity rule based on the general equation for weight changes, while</div><div class="clear"></div>
<div class="linenb">417</div><div class="codeline">omitting obsolete parameters:</div><div class="clear"></div>
<div class="linenb">418</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">419</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">420</div><div class="codeline">  \dot{w}_{ij}(t) &amp; = F(s_j^\ast (t), V_i^\ast (t)) <span class="keyword1">\label</span>{eq-delta-w-spiking}</div><div class="clear"></div>
<div class="linenb">421</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">422</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">423</div><div class="codeline">where the change in weight $\dot{w}_{ij}$ of a synapse from neuron $j$ to neuron $i$ at time $t$ is given by a function</div><div class="clear"></div>
<div class="linenb">424</div><div class="codeline">$F$ that depends on the postsynaptic membrane potential $V_i^\ast$ and the presynaptic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (27377) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> $s_j^\ast$.</div><div class="clear"></div>
<div class="linenb">425</div><div class="codeline">The $\ast$ operator denotes a causal function, indicating that a value $V_i^\ast(t)$ potentially depends on all previous</div><div class="clear"></div>
<div class="linenb">426</div><div class="codeline">values of $V_i(t' &lt; t)$. One can formally integrate Equation \ref{eq-delta-w-spiking} in order to obtain the weight</div><div class="clear"></div>
<div class="linenb">427</div><div class="codeline">change between two arbitrary time points $t$ and $T$:</div><div class="clear"></div>
<div class="linenb">428</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">429</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">430</div><div class="codeline">  \Delta w_{ij}(t,T) &amp; = \int_t^T dt' F[s_j^\ast, V_i^\ast](t') <span class="keyword1">\label</span>{eq-delta-w-t-T}</div><div class="clear"></div>
<div class="linenb">431</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">432</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">433</div><div class="codeline">This integral forms the basis of computing the change in weight between two arriving spikes. Thus, at the</div><div class="clear"></div>
<div class="linenb">434</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [implementation, implementations] (27734) [lt:en:MORFOLOGIK_RULE_EN_US]">implementational</span> level, $t$ is usually the time of the last spike that traversed the synapse, and $T$ is the current</div><div class="clear"></div>
<div class="linenb">435</div><div class="codeline">\texttt{biological\_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [times, Timothy, timeless, timers, tithes, Mathis, Tibetans, bioethics, mimetic, timeouts, Tethys, timothy, Epimethius, meths, teethes, timeous, Métis, Tetris, Timothée, dimethyl, kimchis, mimesis, timeshift, timeshifts, timewise] (27859) [lt:en:MORFOLOGIK_RULE_EN_US]">time}\footnote{This</span> term is adopted from the NEST convention, where it describes the time in $ms$</div><div class="clear"></div>
<div class="linenb">436</div><div class="codeline">  which the simulator has computed. In other words, it is the number of simulation steps times $\Delta t$, not to be</div><div class="clear"></div>
<div class="linenb">437</div><div class="codeline">  confused with a simulation's hardware-dependent runtime (sometimes also called <span class="keyword1">\textit</span>{wall clock time}</div><div class="clear"></div>
<div class="linenb">438</div><div class="codeline">  \citep{albada2018performance}). }<span class="highlight" title="Don't put a space before the full stop.. Suggestions: [.] (28157) [lt:en:COMMA_PARENTHESIS_WHITESPACE]">.</span> For spiking neurons, it is necessary to approximate the presynaptic rate</div><div class="clear"></div>
<div class="linenb">439</div><div class="codeline">($r_j=\phi(u_j)$). For this, a well established solution is to transform the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (28295) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> $s_j$ into a decaying trace</div><div class="clear"></div>
<div class="linenb">440</div><div class="codeline">using an exponential filter kernel $\kappa$:</div><div class="clear"></div>
<div class="linenb">441</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">442</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">443</div><div class="codeline">  \kappa(t)     &amp; = H(t) \frac{1}{t}e^{\frac{-t}{\tau_{\kappa}}}                        \\</div><div class="clear"></div>
<div class="linenb">444</div><div class="codeline">  H(t)          &amp; =</div><div class="clear"></div>
<div class="linenb">445</div><div class="codeline">  <span class="keyword2">\begin{cases}</span></div><div class="clear"></div>
<div class="linenb">446</div><div class="codeline">    1 &amp; \text{if $t &gt; 0$}    \\</div><div class="clear"></div>
<div class="linenb">447</div><div class="codeline">    0 &amp; \text{if $t \leq 0$} \\</div><div class="clear"></div>
<div class="linenb">448</div><div class="codeline">  <span class="keyword2">\end{cases}</span>                                                              \\</div><div class="clear"></div>
<div class="linenb">449</div><div class="codeline">  (f \ast g)(t) &amp; = \int_{- \infty }^{\infty} f(t') g(t-t') d t' <span class="keyword1">\label</span>{eq-convolution} \\</div><div class="clear"></div>
<div class="linenb">450</div><div class="codeline">  s_j^\ast      &amp; = \kappa_s \ast s_j. <span class="keyword1">\label</span>{eq-spike-trace}</div><div class="clear"></div>
<div class="linenb">451</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">452</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">453</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [With] (28370) [lt:en:UPPERCASE_SENTENCE_START]">with</span> filter time constant $\tau_\kappa$. The trace is computed by <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [coevolving] (28424) [lt:en:MORFOLOGIK_RULE_EN_US]">convolving</span> (Equation \ref{eq-convolution}) the</div><div class="clear"></div>
<div class="linenb">454</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (28452) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> with the exponential filter kernel $\kappa$. The filter uses the Heaviside step function $H(t)$, and is</div><div class="clear"></div>
<div class="linenb">455</div><div class="codeline">therefore only supported on positive values of $t$ (also called a one-sided exponential decay kernel). This property is</div><div class="clear"></div>
<div class="linenb">456</div><div class="codeline">important, as integration limits of the convolution can be truncated when $f$ and $g$ are both only supported on</div><div class="clear"></div>
<div class="linenb">457</div><div class="codeline">$[0,\infty)$:</div><div class="clear"></div>
<div class="linenb">458</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">459</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">460</div><div class="codeline">  (f \ast g)(t) &amp; = \int_{0}^{t} f(t') g(t-t') d t'</div><div class="clear"></div>
<div class="linenb">461</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">462</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">463</div><div class="codeline">Since spikes naturally only occur for $t&gt;0$, this simplified integral allows for a much more efficient computation of the</div><div class="clear"></div>
<div class="linenb">464</div><div class="codeline">convolution. The Function $F$ on the right-hand side of Equation \ref{eq-delta-w-spiking} can therefore</div><div class="clear"></div>
<div class="linenb">465</div><div class="codeline">be <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [rewrite, rewritten, rewrites, rewriter] (28987) [lt:en:MORFOLOGIK_RULE_EN_US]">rewriten</span> as:</div><div class="clear"></div>
<div class="linenb">466</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">467</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">468</div><div class="codeline">  F[s_j^\ast, V_i^\ast] &amp; = \eta \kappa \ast (V_i^\ast s_j^\ast)        \\</div><div class="clear"></div>
<div class="linenb">469</div><div class="codeline">  V_i^\ast              &amp; = (\phi(u_i^{som}) - \phi(\hat{v}_i^{dend}) )</div><div class="clear"></div>
<div class="linenb">470</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">471</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">472</div><div class="codeline">\what{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [This] (29002) [lt:en:UPPERCASE_SENTENCE_START]">this</span> notation seems slightly abusive, as neither side considers $t$, but it is taken precisely from</div><div class="clear"></div>
<div class="linenb">473</div><div class="codeline">  \cite{Stapmanns2021}} with learning rate $\eta$. $V_i^\ast$ then is the dendritic error of the dendrite that the synapse</div><div class="clear"></div>
<div class="linenb">474</div><div class="codeline">between $j$ and <span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (29205) [lt:en:I_LOWERCASE]">$i$</span> is located <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [the, battle, Battle, cattle, Matthew, anthem, ate, attire, attic, Attic, ante, bathe, lathe, matte, rattle, tithe, ACTH, Artie, Ashe, atone, Althea, Attlee, Mattie, ache, anther, attache, latte, lithe, Bethe, Lethe, attune, tattle, attar, withe, ACTE, AHE, ARTE, ATE, ATEE, ATH, ATT, ATTAC, Attn, FTTH, Hattie, LTTE, acte, attn, atty, tattie, wattle, Agathe, Authy, Sarthe, attaché, auth, authed, auths, pattie] (29218) [lt:en:MORFOLOGIK_RULE_EN_US]">at\footnote{The</span> dendritic error here is defined as the difference between two</div><div class="clear"></div>
<div class="linenb">475</div><div class="codeline">hypothetical rates based on the arbitrary function $\phi$. The original implementation uses the difference between the</div><div class="clear"></div>
<div class="linenb">476</div><div class="codeline">actual postsynaptic <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (29420) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> and this dendritic prediction ($V_i^\ast = (s_i - \phi(\hat{v}_i^{dend}) )$).</div><div class="clear"></div>
<div class="linenb">477</div><div class="codeline">Furthermore, <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Seaplanes, Shamans, Stamens, Sampans, Starman] (29479) [lt:en:MORFOLOGIK_RULE_EN_US]">Stapmanns</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>show that generating a <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [spike train] (29519) [lt:en:MORFOLOGIK_RULE_EN_US]">spiketrain</span> from the dendritic potential (<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [UV, CV, MV, PV, TV, AV, GV, IV, JV, KV, NV, RV, V, WV, eV, XV, BV, DV, EV, LV, SV] (29560) [lt:en:MORFOLOGIK_RULE_EN_US]">$V</span>_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [in, is, I, it, if, ID, IQ, id, IH, IP, IA, IE, IF, IJ, IL, IN, IR, IS, IT, IV, IZ, Ia, In, Io, Ir, It, i, ii, iv, ix, IB, IC, IG, IM, IU] (29563) [lt:en:MORFOLOGIK_RULE_EN_US]">i^</span>\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [AST, as, at, last, art, east, East, St, act, cast, past, fast, vast, ask, sat, ash, aft, mast, LST, ant, apt, ASB, ASD, CST, DST, MST, SST, st, AAT, ADT, AIT, ASN, PST, asp, AUT, AZT, amt, bast, asst, hast, 1st, ABT, ACT, AET, AFT, AGT, AHT, AJT, AKT, AMST, AMT, ANT, AOT, APT, AQT, ART, ASA, ASC, ASE, ASF, ASG, ASH, ASJ, ASK, ASL, ASM, ASP, ASQ, ASR, ASS, ASU, ASV, ASW, ASX, ASY, ASZ, AT, ATT, Art, As, At, Ats, BST, EST, HST, IST, NST, OST, RST, ST, VST, WST, alt, ass, est, ACST, AEST, ASTM, AWST, FST, GST, a, and, is, was, an, any, are, but, has, it, not, all, get, most, must, out, part, age, its, set, so, us, use, air, best, got, just, lost, wait, west, West, also, anti, base, case, coast, cost, cut, eight, fact, gas, hit, host, least, let, list, mass, pass, post, put, rest, saw, want, yet, Best, acts, add, ago, aid, am, arm, arts, bass, bit, easy, eye, eyes, hot, lot, say, task, test, Asia, NASA, Post, ad, asks, cash, cat, eat, fit, ft, hat, jet, met, net, salt, sit, taste, vs, waste, wet, AFC, Matt, NSW, ads, aka, arc, asset, aunt, auto, bat, bats, blast, caste, cats, dust, ease, fat, feast, halt, kit, lasts, mask, nest, pet, pit, pot, rats, Acts, Bass, CSS, Case, Hart, ICT, Lt, Mass, Nash, PT, Pat, SAS, SD, SF, SG, SK, SL, SSR, TNT, TT, USA, USB, Walt, ace, ants, ate, beast, bet, bust, cart, casts, dash, dot, eats, eyed, hash, hats, hut, nut, opt, pact, paste, rat, sad, waist, wash, wit, yeast, ACM, ACS, ADC, ADP, AF, ANSI, AOL, APA, APC, ATC, ATM, ATV, Ana, Aston, BMT, BSA, BSD, BT, Bart, CRT, CSA, CSI, CSU, CSX, Cash, DDT, DSM, DSO, DSP, DT, ESA, ESP, GMT, IIT, ISP, LSD, MRT, MSC, MSN, NSA, Oct, PET, PS2, PSA, PSP, PSV, RSA, RSS, Sgt, Taft, Watt, ale, alto, amp, ape, apse, aux, awe, bait, boast, dart, fats, fist, gait, gut, lest, lust, malt, masts, mat, mats, mist, oft, pas, pasta, pest, psi, raft, rash, roast, rot, rust, sac, sap, sax, toast, vase, vest, watt, AAU, ADR, ADSL, AEC, AEG, AIF, AIG, AMX, APG, APS, ARL, ARP, Abe, Ada, Alta, Astor, Aug, Ava, CNT, CSC, CSF, CSL, CSP, CSR, DAT, DSC, DSL, DSS, EMT, ESC, ESL, FSA, FSU, Faust, Host, IAS, ITT, JSA, Kant, MSF, NSF, Nat, OAS, OSI, OSU, PSD, PSL, PSU, RSC, SSA, SSE, SSL, SSP, TSA, USF, USO, USP, WSU, Wash, ah, angst, ante, ark, avg, baht, cask, cyst, fest, gust, hadst, haste, hasty, kart, mash, ms, oats, oust, sash, sh, vat, vet, wasp, ABM, ACTH, ACU, AD, ADD, AEF, AFM, AFN, AGC, AGS, AHS, APD, APM, ARF, ASAP, ASSR, Apr, Ara, Ashe, BLT, CBT, CDT, CSD, CSG, CSV, CVT, DSA, DSB, DSE, ESB, ESD, ESO, ESR, ESS, Esq, FSH, FSI, FSK, FSM, GATT, HSE, HSL, HSP, HSS, IMT, KSC, LCT, LSA, LSC, LSI, LSM, LSO, LSP, Lat, MBT, MSI, MSW, NWT, ORT, OSB, OSF, PDT, PSB, PSE, PSF, PSG, PSM, PSO, PSS, PTT, RSI, RSM, RSV, SALT, SBT, SRT, SSM, SSW, STP, Sat, TSS, USTA, WASP, av, ax, aye, bash, bot, cant, cit, cot, esp, fasts, gash, gasp, jest, lash, lat, mart, oat, pasts, pat, pt, rant, rut, sag, sf, tact, tart, tasty, tat, taut, tot, ts, vats, zest, ACD, ACG, ACH, AGR, APB, APN, Adm, CFT, CLT, DPT, DSD, DSR, DWT, ELT, ESE, Easts, HSA, KSS, LSB, MASH, MCT, MDT, MKT, MTT, Pabst, RPT, RSP, RSX, RTT, SLT, SSO, USG, WSW, abs, abut, aim, arty, aster, bask, dist, gist, hart, lass, pant, pasty, pats, rasp, wart, AFO, CSH, CTT, Catt, EFT, ESN, Tut, WSJ, abet, adj, ado, aha, ain, alp, ashy, awl, dost, ext, haft, jot, jut, masc, nit, rapt, rt, sass, sate, ssh, sty, tut, AHQ, Inst, SASE, Tass, VDT, Zest, adv, ail, app, ascot, canst, fut, gt, inst, lase, pct, pvt, sot, tsp, waft, zit, asps, baste, hasp, tats, AAA, ABC, AMC, ATP, Afr, Ali, Amy, Ann, MIT, PS, USC, USS, alts, assn, astir, ave, dpt, jct, lit, wist, Hts, psst, A, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26, A27, A28, A29, A30, A31, A32, A33, A34, A35, A36, A37, A38, A39, A40, A41, A42, A43, A44, A45, A46, A47, A48, A49, A50, A51, A52, A53, A54, A55, A56, A57, A58, A59, A60, A61, A62, A63, A64, A65, A66, A67, A68, A69, A6M, A70, A71, A72, A73, A74, A75, A76, A77, A78, A79, A80, A81, A82, A83, A84, A85, A86, A87, A88, A89, A90, A91, A92, A93, A94, A95, A96, A97, A98, A99, A9C, AA, AAB, AAC, AAD, AAE, AAF, AAG, AAH, AAI, AAJ, AAK, AAL, AAM, AAN, AAO, AAP, AAQ, AAR, AAS, AAV, AAW, AAX, AAY, AAZ, AB, ABA, ABB, ABD, ABF, ABG, ABH, ABJ, ABK, ABL, ABN, ABP, ABQ, ABR, ABS, ABU, ABV, ABVT, ABW, ABX, ABZ, AC, ACA, ACAT, ACB, ACC, ACF, ACI, ACJ, ACK, ACL, ACN, ACO, ACP, ACR, ACTA, ACTE, ACTP, ACV, ACW, ACX, ACY, ACZ, ADA, ADB, ADF, ADG, ADH, ADJ, ADK, ADL, ADM, ADN, ADQ, ADS, ADSI, ADV, ADX, ADY, ADZ, AE, AEA, AEB, AED, AEE, AEI, AEJ, AEK, AEL, AEM, AEN, AEO, AEP, AEQ, AER, AES, AESA, AESV, AEU, AEV, AEW, AEX, AEZ, AFA, AFAT, AFB, AFD, AFE, AFF, AFG, AFH, AFI, AFJ, AFK, AFL, AFQ, AFR, AFS, AFU, AFV, AFW, AFX, AFZ, AG, AGB, AGD, AGE, AGF, AGG, AGH, AGI, AGJ, AGK, AGL, AGM, AGN, AGO, AGP, AGQ, AGU, AGV, AGW, AGX, AGY, AGZ, AHA, AHB, AHC, AHD, AHE, AHF, AHG, AHH, AHI, AHJ, AHK, AHL, AHM, AHN, AHO, AHR, AHSI, AHTV, AHU, AHV, AHW, AHX, AHY, AHZ, AI, AIB, AIC, AID, AIE, AIH, AII, AIJ, AIK, AIM, AIO, AIP, AIQ, AIS, AITA, AITF, AIU, AIV, AIW, AIY, AIZ, AIs, AJ, AJB, AJC, AJF, AJG, AJH, AJI, AJJ, AJK, AJL, AJN, AJO, AJP, AJQ, AJR, AJS, AJU, AJV, AJW, AJY, AK, AKB, AKC, AKD, AKE, AKF, AKG, AKJ, AKK, AKL, AKM, AKN, AKO, AKP, AKQ, AKR, AKS, AKU, AKV, AKX, AKY, AL, ALAT, ALB, ALC, ALD, ALE, ALG, ALH, ALJ, ALK, ALL, ALM, ALN, ALO, ALP, ALQ, ALR, ALS, ALV, ALW, ALX, ALZ, AM, AMA, AMD, AME, AMF, AMG, AMH, AMJ, AMK, AML, AMM, AMN, AMO, AMP, AMQ, AMR, AMS, AMSL, AMU, AMV, AMW, AMY, AMZ, AN, ANB, ANC, AND, ANE, ANF, ANG, ANH, ANI, ANJ, ANK, ANL, ANM, ANN, ANO, ANP, ANQ, ANR, ANS, ANSM, ANTS, ANV, ANW, ANX, ANY, ANZ, AO, AOA, AOB, AOC, AOD, AOE, AOF, AOG, AOH, AOI, AOJ, AOK, AOM, AON, AOO, AOP, AOQ, AOR, AOS, AOU, AOV, AOW, AOX, AP, APE, APF, APH, API, APJ, APK, APL, APO, APP, APQ, APR, APU, APV, APW, APX, APY, APZ, AQA, AQB, AQC, AQF, AQG, AQH, AQI, AQK, AQL, AQM, AQP, AQR, AQS, AQY, AR, ARB, ARC, ARD, ARE, ARH, ARI, ARJ, ARK, ARM, ARN, ARQ, ARR, ARSI, ARTE, ARTS, ARU, ARV, ARW, ARX, ARY, ASBL, ASEM, ASFI, ASGS, ASIN, ASLM, ASMP, ASMR, ASNL, ASPA, ASPI, ASSE, ASVP, AT&amp;T, AT3, ATA, ATB, ATD, ATE, ATF, ATG, ATH, ATI, ATK, ATL, ATN, ATO, ATQ, ATR, ATS, ATU, ATW, ATX, ATY, ATZ, AU, AUA, AUC, AUD, AUE, AUF, AUG, AUI, AUJ, AUL, AUM, AUN, AUO, AUP, AUQ, AUR, AUS, AUU, AUW, AUX, AUY, AUZ, AV, AVA, AVB, AVC, AVE, AVF, AVG, AVH, AVI, AVK, AVL, AVN, AVO, AVP, AVR, AVS, AVSF, AVU, AVV, AVW, AVX, AVY, AVZ, AWA, AWB, AWD, AWE, AWG, AWK, AWM, AWN, AWP, AWR, AWS, AWU, AWZ, AX, AXB, AXC, AXD, AXG, AXK, AXL, AXM, AXN, AXP, AXR, AXS, AXV, AXX, AYA, AYC, AYD, AYE, AYG, AYH, AYI, AYJ, AYL, AYN, AYO, AYP, AYQ, AYR, AYS, AYU, AYY, AYZ, AZ, AZA, AZB, AZD, AZE, AZF, AZI, AZN, AZO, AZS, AZW, AZZ, Ac, Ag, Al, Ala, Am, Ar, Ark, Attn, Au, Av, Ave, BAS, BBT, BCST, BCT, BDT, BEST, BET, BFT, BGT, BHT, BKT, BNT, BPT, BQT, BRT, BS, BSB, BSC, BSE, BSF, BSG, BSH, BSI, BSK, BSL, BSM, BSN, BSO, BSP, BSQ, BSR, BSS, BSU, BSV, BSW, BSX, BTT, BUT, BVT, BWT, BZT, CALT, CAMT, CASA, CASE, CASM, CAT, CCT, CEST, CET, CGT, CHT, CIT, CMT, CQT, CS, CSE, CSM, CSST, CT, CUT, CWT, CYT, CZT, Capt, Cs, Ct, DCT, DET, DFT, DGT, DHT, DJT, DLT, DNT, DOT, DRT, DS, DSF, DSG, DSI, DSK, DSN, DSQ, DSTN, DSU, DSV, Dot, EAS, EAT, EDT, EGT, EIT, ENST, EOST, EPT, ERT, ES, ESF, ESH, ESI, ESM, ESTP, ET, ETT, Es, Esc, FASM, FAT, FET, FFT, FIT, FJT, FMT, FNT, FPT, FRT, FS, FSB, FSC, FSD, FSF, FSG, FSP, FSR, FSS, FSW, GAS, GBT, GDT, GET, GHT, GLT, GNT, GSA, GSC, GSF, GSH, GSI, GSIT, GSK, GSL, GSM, GSO, GSP, GSR, GSW, GT, GUT, GVT, HFT, HGT, HIT, HRT, HS, HSC, HSF, HSM, HSV, HT, IAMT, IASB, IASI, IDT, IHT, INT, IOT, IPT, IRT, IS, ISA, ISB, ISC, ISD, ISF, ISG, ISGT, ISI, ISK, ISL, ISM, ISN, ISO, ISR, ISS, ISTC, ISV, IT, It, JIT, JNT, JS2, JSB, JSF, JSL, JSP, JSU, JSX, JT, KET, KS, KSA, KSM, KSR, KT, KWT, Kit, Ks, LAT, LDT, LFST, LFT, LGT, LNT, LS, LS1, LS2, LS3, LS4, LSAT, LSE, LSF, LSJ, LSL, LSN, LSQ, LSW, LSX, LT, LTT, LUT, Las, Lot, MAS, MAT, MET, MFT, MLT, MMT, MNT, MOT, MPT, MQT, MS, MSA, MSB, MSD, MSE, MSFT, MSG, MSH, MSIT, MSK, MSM, MSO, MSR, MSS, MSTS, MSU, MT, Ms, Mt, Myst, NAS, NBT, NDT, NFT, NMT, NRT, NS, NSB, NSC, NSM, NSS, NSU, NT, NUT, NVT, OAT, OBT, OFT, OIT, OLT, OMT, ONT, OPT, OS, OSA, OSC, OSD, OSG, OSL, OSM, OSQ, OSR, OSS, OSV, OT, OXT, Ont, Os, PAS, PBT, PCT, PGT, PIT, PLT, PMT, PNT, POST, PPT, PRT, PS1, PSC, PSK, PSN, PSQ, PSR, PSTN, PSX, PYT, Pt, Pvt, QAT, QFT, QIT, QSE, QSR, QSV, QVT, RAS, RDT, RNT, ROT, RRT, RS, RS7, RSE, RSF, RSH, RSO, RSR, RSW, RT, RVT, Rasta, S, SA, SADT, SART, SAT, SB, SC, SE, SET, SFT, SGT, SH, SI, SIT, SJ, SM, SMT, SN, SO, SP, SPT, SS, SS7, SSB, SSF, SSG, SSH, SSI, SSK, SSS, SSV, STA, STB, STC, STD, STE, STF, STG, STI, STL, STM, STN, STO, STR, STS, STT, STV, STX, SU, SVT, SW, SWT, SY, SYT, SZ, Sask, Sb, Sc, Se, Set, Si, Sm, Sn, Sp, Sq, Sr, Sta, Ste, Stu, T, TASS, TAT, TBT, TDT, TFT, THT, TKT, TLT, TMT, TOT, TPT, TRT, TS, TSB, TSC, TSD, TSE, TSG, TSH, TSI, TSK, TSL, TSM, TSN, TSO, TSP, TSR, TSV, TSX, TTT, Tet, UAS, UET, UGT, UIT, ULT, UMT, UNT, URT, US, USD, USE, USI, USJ, USN, USR, USTB, USTL, USTM, UT, UVT, Ut, VAT, VCT, VOST, VSB, VSC, VSD, VSF, VSG, VSL, VSM, VSN, VSOT, VSP, VSR, VSS, VSTI, VSV, VT, VTT, VUT, VVT, Vt, WCT, WET, WHT, WKT, WLT, WNT, WRT, WSA, WSC, WSD, WSF, WSM, WSP, WSS, XFT, XRT, XS, XSLT, XSN, YT, ZSE, ZSL, ZSP, Zs, aah, ab, ac, acct, acte, advt, alb, ang, ans, arr, asap, attn, atty, auk, avast, aw, awn, capt, cs, ct, cwt, daft, fart, flt, git, gs, hasn, hgt, hist, ht, int, isl, ism, isn, ks, kt, lats, ls, mas, mayst, mot, nasty, pkt, qt, qts, rs, s, sq, std, t, tit, ult, usu, vasts, wasn, wot, wt, yest, A&amp;A, A&amp;D, A&amp;E, A&amp;M, A&amp;P, A&amp;R, AAWT, ABI, ACE, ACTs, ACh, ADI, ADU, AFP, AGA, AGs, AIX, ALF, ALU, AMI, AOSP, APs, ASIC, ASML, ASOS, ASPR, ASUS, AVM, AVs, AWC, AWF, AXA, Aalst, Abi, Abu, Amit, Aon, Asha, Asif, Astra, Asus, BASF, BSc, CAS, CPT, CSAT, CSET, CSO, DMT, DSAT, DTT, DVT, EASA, EASO, ECT, ENT, ES5, ES6, ESET, ESG, ESTA, ESV, ESY, FAS, FASD, FSE, FSN, FT, GCT, GPT, GSE, GSTs, HCT, HSK, IAT, IGST, ISTA, ISU, IoT, JS, JSC, JWT, Kat, LMT, LSTM, LSU, M&amp;T, MSc, NCT, NIST, NLT, NSO, NTT, NYT, ODT, OSX, PAs, PS3, PS4, PS5, PSAT, PSTG, QS, RET, RSU, SGST, SHT, SSC, SSD, SSN, STW, SV, TAS, TXT, UAT, UPT, USDT, WS, WSB, WSL, WSN, Wasm, XSD, XSL, XSS, YSL, ahh, ai, ais, amu, auth, aww, axe, cts, msg, pts, tase] (29566) [lt:en:MORFOLOGIK_RULE_EN_US]">ast</span> = (s_i -</div><div class="clear"></div>
<div class="linenb">478</div><div class="codeline">  s_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [indeed, intend, indent, addend, impend, IDed, Indeed] (29583) [lt:en:MORFOLOGIK_RULE_EN_US]">i^{dend}</span>)$) also results in successful learning, although at the cost of additional training time. The rate-based</div><div class="clear"></div>
<div class="linenb">479</div><div class="codeline">variant was chosen in order to not hinder learning performance any more than necessary<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (29781) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Writing out the convolutions</div><div class="clear"></div>
<div class="linenb">480</div><div class="codeline">in Equation \ref{eq-delta-w-t-T} explicitly, we obtain</div><div class="clear"></div>
<div class="linenb">481</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">482</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">483</div><div class="codeline">  \Delta w_{ij}(t,T) &amp; = \int_t^T dt' F[s_j^\ast, V_i^\ast](t')                                                                           \\</div><div class="clear"></div>
<div class="linenb">484</div><div class="codeline">                     &amp; =  \int_t^T dt' \  \eta\int_0^{t'} dt'' \ \kappa(t'-t'') V_i^\ast (t'') s_j^\ast (t'') <span class="keyword1">\label</span>{eq-delta-w-t-T-long}</div><div class="clear"></div>
<div class="linenb">485</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">486</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">487</div><div class="codeline">Computing this Equation directly is inefficient due to the nested integrals. Yet, it is possible to break up the</div><div class="clear"></div>
<div class="linenb">488</div><div class="codeline">integrals into two simpler computations and rewrite the weight change as:</div><div class="clear"></div>
<div class="linenb">489</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">490</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">491</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">492</div><div class="codeline">  \Delta W_{ij}(t, T) &amp; = \eta \left[ I_1 (t, T) - I_2(t,T) + I_2(0,t)\left( 1- e^{-\frac{T-t}{\tau_\kappa}} \right) \right] \\</div><div class="clear"></div>
<div class="linenb">493</div><div class="codeline">  I_1(a, b)           &amp; = \int_{a}^{b} dt \ V_i^\ast (t) s_j^\ast (t)                                                        \\</div><div class="clear"></div>
<div class="linenb">494</div><div class="codeline">  I_2(a, b)           &amp; = \int_{a}^{b} dt \ e^{-\frac{b-t}{\tau_\kappa}} V_i^\ast (t) s_j^\ast (t)                           \\</div><div class="clear"></div>
<div class="linenb">495</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">496</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">497</div><div class="codeline">See Section \todo{ref} for a rigorous proof that this is in fact the desired integral. \what{That proof is currently</div><div class="clear"></div>
<div class="linenb">498</div><div class="codeline">  part of the appendix, as the original proof \citep{Stapmanns2021} has errors. Should I keep it or refer to the original</div><div class="clear"></div>
<div class="linenb">499</div><div class="codeline">  paper?} The resulting equations allow for a rather efficient computation of weight changes compared to the complex</div><div class="clear"></div>
<div class="linenb">500</div><div class="codeline">integral described in Equation \ref{eq-delta-w-t-T-long}. This integration is performed whenever a spike traverses a</div><div class="clear"></div>
<div class="linenb">501</div><div class="codeline">synapse. It generalizes to all special cases in Equations \ref{eq-delta_w_up}-\ref{eq-delta_w_down}, as long as the</div><div class="clear"></div>
<div class="linenb">502</div><div class="codeline">appropriate dendritic error is stored by the postsynaptic neuron.</div><div class="clear"></div>
<div class="linenb">503</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">504</div><div class="codeline"><span class="keyword1">\section</span>{Latent Equilibrium}<span class="keyword1">\label</span>{sec-latent-eq}</div><div class="clear"></div>
<div class="linenb">505</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">506</div><div class="codeline">The most significant drawback of the Sacramento model is the previously mentioned requirement for long stimulus</div><div class="clear"></div>
<div class="linenb">507</div><div class="codeline">presentation times and appropriately low learning rates. This makes the network prohibitively inefficient for the large</div><div class="clear"></div>
<div class="linenb">508</div><div class="codeline">networks required for complex learning tasks. Sacramento <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>developed a steady-state approximation of their network</div><div class="clear"></div>
<div class="linenb">509</div><div class="codeline">which models the state of the network after it has balanced out in response to a stimulus-target pair. It does not</div><div class="clear"></div>
<div class="linenb">510</div><div class="codeline">suffer from these issues and shows that their model can in principle solve more demanding learning tasks such as <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MOIST, MIST, MONIST, MN IST, NIST] (31203) [lt:en:MORFOLOGIK_RULE_EN_US]">MNIST</span>.</div><div class="clear"></div>
<div class="linenb">511</div><div class="codeline">Yet these types of approximation are much further detached from biological neurons than the original model and thus do</div><div class="clear"></div>
<div class="linenb">512</div><div class="codeline">not lend themselves well to an investigation of biological plausibility \citep{Gerstner2009}. Furthermore, the</div><div class="clear"></div>
<div class="linenb">513</div><div class="codeline">approximation is unsuitable for and investigation of spike-based communication, since the steady state of both network</div><div class="clear"></div>
<div class="linenb">514</div><div class="codeline">ideally would be the same. Thus, neither the fully modeled neuron dynamics nor the steady-state approximation are suited</div><div class="clear"></div>
<div class="linenb">515</div><div class="codeline">for complex learning tasks. A substantial improvement to rate neurons which promises to solve this dilemma was developed</div><div class="clear"></div>
<div class="linenb">516</div><div class="codeline">by \citep{Haider2021}, and will be discussed here.</div><div class="clear"></div>
<div class="linenb">517</div><div class="codeline">\newline</div><div class="clear"></div>
<div class="linenb">518</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">519</div><div class="codeline">The requirement for long stimulus presentation times of the dendritic error network is caused by the slow development of</div><div class="clear"></div>
<div class="linenb">520</div><div class="codeline">leaky neuron dynamics, and is therefore not unique to this model. When a stimulus-target pair is presented to the</div><div class="clear"></div>
<div class="linenb">521</div><div class="codeline">network, membrane potentials in all neurons slowly evolve until a steady state is reached. The time until a network of</div><div class="clear"></div>
<div class="linenb">522</div><div class="codeline">has reached this state after a change in input is called the <span class="keyword1">\textit</span>{relaxation period} following \citep{Haider2021}.</div><div class="clear"></div>
<div class="linenb">523</div><div class="codeline">Given a membrane time constant $\tau_m$, a feedforward network with $N$ layers of leaky neurons thus has a relaxation</div><div class="clear"></div>
<div class="linenb">524</div><div class="codeline">time constant of $N \tau_m$. Yet in our case, a target activation simultaneously injected into the output neurons slowly</div><div class="clear"></div>
<div class="linenb">525</div><div class="codeline">propagates backwards through the highly recurrent network. Neurons at early layers require all subsequent layers to be</div><div class="clear"></div>
<div class="linenb">526</div><div class="codeline">fully relaxed in order to correctly compute their dendritic error terms, effectively being dependent on two network</div><div class="clear"></div>
<div class="linenb">527</div><div class="codeline">passes. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (32742) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>state that this kind of network therefore requires $2N\tau_m$ to relax in response to a given</div><div class="clear"></div>
<div class="linenb">528</div><div class="codeline">input-output pairing. This prediction proved to be slightly optimistic in experiments, as shown in Fig.</div><div class="clear"></div>
<div class="linenb">529</div><div class="codeline">\ref{fig-error-comp-le}.</div><div class="clear"></div>
<div class="linenb">530</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">531</div><div class="codeline">This is a major issue, as it implies that plasticity during the first few <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [milliseconds] (33023) [lt:en:MORFOLOGIK_RULE_EN_US]">miliseconds</span> of a stimulus presentation is</div><div class="clear"></div>
<div class="linenb">532</div><div class="codeline">driven by faulty error terms. The network thus tends to 'overshoot', and needs to undo the synaptic weight changes made</div><div class="clear"></div>
<div class="linenb">533</div><div class="codeline">during the relaxation period in the later phase of a stimulus presentation, in order to make tangible progress on the</div><div class="clear"></div>
<div class="linenb">534</div><div class="codeline">learning task. <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (33318) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>call this issue the <span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span>relaxation problem<span class="highlight-sh" title="Use `` and '' instead of ''. [sh:d:008]">"</span> and suggest that it might be inherent to most</div><div class="clear"></div>
<div class="linenb">535</div><div class="codeline">established attempts at biologically plausible <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Back propagation] (33466) [lt:en:MORFOLOGIK_RULE_EN_US]">Backpropagation</span> algorithms</div><div class="clear"></div>
<div class="linenb">536</div><div class="codeline">\citep{Whittington2017,guerguiev2017towards,sacramento2018dendritic,millidge2020activation}.</div><div class="clear"></div>
<div class="linenb">537</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">538</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">539</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">540</div><div class="codeline">The choice to simply increase presentation time to compensate for the relaxation period is therefore somewhat</div><div class="clear"></div>
<div class="linenb">541</div><div class="codeline">problematic. It implicitly tolerates adverse synaptic plasticity in all synapses, which are counteracted by enforcing</div><div class="clear"></div>
<div class="linenb">542</div><div class="codeline">the desired plasticity for a longer time. Physiological changes that are meant to immediately be undone are of course an</div><div class="clear"></div>
<div class="linenb">543</div><div class="codeline">inefficient use of a brain's resources, which can be considered highly untypical for a biological system. One possible</div><div class="clear"></div>
<div class="linenb">544</div><div class="codeline">solution to this is to decrease synaptic time constants and remove the temporal filtering of stimulus injections. Yet</div><div class="clear"></div>
<div class="linenb">545</div><div class="codeline">this does not solve the fundamental issue that during a substantial portion of stimulus presentations, the network is</div><div class="clear"></div>
<div class="linenb">546</div><div class="codeline">driven by erroneous plasticity. Removing temporal filtering does decrease the length of the relaxation period, but</div><div class="clear"></div>
<div class="linenb">547</div><div class="codeline">causes a drastic increase in dendritic error values during that period. Therefore, while improving response time, this</div><div class="clear"></div>
<div class="linenb">548</div><div class="codeline">change effectively impedes learning. Another possible solution is to disable plasticity for the first few milliseconds</div><div class="clear"></div>
<div class="linenb">549</div><div class="codeline">of stimulus presentation. After the network has relaxed, the plasticity rules produce useful weight changes and learning</div><div class="clear"></div>
<div class="linenb">550</div><div class="codeline">rates can consequently be safely increased. Yet a mechanism by which neurons could implement this style of phased</div><div class="clear"></div>
<div class="linenb">551</div><div class="codeline">plasticity is yet to be found, making this approach questionable in terms of biological plausibility. Furthermore, it</div><div class="clear"></div>
<div class="linenb">552</div><div class="codeline">introduces a requirement for external control to the network, a trait that is considered highly undesirable for</div><div class="clear"></div>
<div class="linenb">553</div><div class="codeline">approximations of <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Backdrop, Back prop] (35041) [lt:en:MORFOLOGIK_RULE_EN_US]">Backprop</span> \citep{whittington2019theories}. Ideally, the relaxation period would be skipped or</div><div class="clear"></div>
<div class="linenb">554</div><div class="codeline">shortened, in order to reduce the erroneous plasticity. This would allow for a loosening of the constraints put on</div><div class="clear"></div>
<div class="linenb">555</div><div class="codeline">presentation time and learning rates, thus increasing computational efficiency. \newline</div><div class="clear"></div>
<div class="linenb">556</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">557</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">558</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">559</div><div class="codeline">The approach proposed by <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (35338) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>is to change the parameter of the activation function $\phi$, a mechanism called</div><div class="clear"></div>
<div class="linenb">560</div><div class="codeline"><span class="keyword1">\textit</span>{Latent Equilibrium} (LE). Neurons in the original dendritic error network (henceforth called <span class="keyword1">\textit</span>{Sacramento</div><div class="clear"></div>
<div class="linenb">561</div><div class="codeline">  neurons}) transmit a function of their somatic potential $u_i$, which is updated through Euler integration at every</div><div class="clear"></div>
<div class="linenb">562</div><div class="codeline">simulation step (Equation \ref{eq-r-t-sacramento}). In contrast, neurons using Latent Equilibrium (henceforth called</div><div class="clear"></div>
<div class="linenb">563</div><div class="codeline"><span class="keyword1">\textit</span>{LE neurons}) transmit a function of what the somatic potential is expected to be in the future. To calculate</div><div class="clear"></div>
<div class="linenb">564</div><div class="codeline">this expected future somatic potential $\breve{u}$, the integration is performed with a larger Euler step:</div><div class="clear"></div>
<div class="linenb">565</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">566</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">567</div><div class="codeline">  u_i(t+ \Delta t)          &amp; = u_i(t) + \dot{u}_i(t) \ \Delta t <span class="keyword1">\label</span>{eq-r-t-sacramento} \\</div><div class="clear"></div>
<div class="linenb">568</div><div class="codeline">  \breve{u}_i(t + \Delta t) &amp; = u_i(t) + \dot{u}_i(t) \ \tau_{eff} <span class="keyword1">\label</span>{eq-r-t-haider}</div><div class="clear"></div>
<div class="linenb">569</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">570</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">571</div><div class="codeline">Instead of broadcasting their rate based on the current somatic potential ($r_i(t) = \phi(u_i(t))$), LE neurons send</div><div class="clear"></div>
<div class="linenb">572</div><div class="codeline">their predicted future activation, denoted as $\breve{r}_i(t) = \phi(\breve{u}_i(t))$. The degree to <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [with, which, rich, wish, Rich, witch, wick, winch, Mich, ICH, WIC, Wish] (36104) [lt:en:MORFOLOGIK_RULE_EN_US]">wich</span> LE neurons</div><div class="clear"></div>
<div class="linenb">573</div><div class="codeline">look ahead is determined by the <span class="keyword1">\textit</span>{effective membrane time constant} <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Jeff, eff] (36185) [lt:en:MORFOLOGIK_RULE_EN_US]">$\tau_{eff}</span> = \frac{C_m}{g_l + <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [gas, grabs, Gibbs, Abbas, GBA, galas, tubas, gobs, gibes, gabs, BAS, GAS, GBS, Goiás, babas] (36201) [lt:en:MORFOLOGIK_RULE_EN_US]">g^{bas}</span> +</div><div class="clear"></div>
<div class="linenb">574</div><div class="codeline">    <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [gain, rapid, gap, grain, graphic, grape, graph, gaps, grapes, graphs, Gavin, gait, goalie, Gaia, Gracie, Grail, gape, gaping, Apia, grail, gratis, tapir, vapid, grappa, gravid, okapi, Gail, gaped, gapes, gratin, gamin, lapin, okapis, gappy, API, APII, ATAPI, DAPI, GAFI, GAIN, GANIL, GAP, GPIS, Gap, MAPI, ODAPI, TAPI, APIs, GAAP, Ghazi, Gmail] (36213) [lt:en:MORFOLOGIK_RULE_EN_US]">g^{api}}$</span>. This time constant takes into account the conductance with which dendritic compartments leak into the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, Lomé, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, Sofía, Sonoma, Souza, TOML, Tomás, UOM, Zora, hola, momma, sRNA, semé, simp, socs, stomas, Škoda] (36323) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span>,</div><div class="clear"></div>
<div class="linenb">575</div><div class="codeline">which is a key driving factor for the speed at which the network relaxes. Any computations that employ or relate to this</div><div class="clear"></div>
<div class="linenb">576</div><div class="codeline">prediction of future network states will henceforth be referred to as <span class="keyword1">\textit</span>{prospective} and denoted with a breve</div><div class="clear"></div>
<div class="linenb">577</div><div class="codeline">($\breve{x}$).</div><div class="clear"></div>
<div class="linenb">578</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">579</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">580</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h!]</div><div class="clear"></div>
<div class="linenb">581</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">582</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_le_response}</div><div class="clear"></div>
<div class="linenb">583</div><div class="codeline">  <span class="keyword1">\caption</span>[Signal transmission of LE neurons]{Signal transmission of LE neurons. Shown is a connection between an input</div><div class="clear"></div>
<div class="linenb">584</div><div class="codeline">    neuron $i$ and a hidden layer pyramidal neuron $j$. Activations for the original Sacramento model (blue) and</div><div class="clear"></div>
<div class="linenb">585</div><div class="codeline">    prospective activation using LE (orange) are compared. <span class="keyword1">\textbf</span>{A:} Current injected into the input neuron. Membrane</div><div class="clear"></div>
<div class="linenb">586</div><div class="codeline">    potential slowly adapts to match this (not shown) <span class="keyword1">\textbf</span>{B:} Activation of the input neuron using instantaneous-</div><div class="clear"></div>
<div class="linenb">587</div><div class="codeline">    $\phi(u_i)$ (blue), and prospective activation $\phi(\breve{u}_i)$ (orange). Note how strongly prospective</div><div class="clear"></div>
<div class="linenb">588</div><div class="codeline">    activation reacts to changes in somatic voltage, leading to 'bursts' in neuron output. After the input neuron has</div><div class="clear"></div>
<div class="linenb">589</div><div class="codeline">    reached its relaxed state ($\dot{u}_i = 0$), both mechanisms evoke the same activation. <span class="keyword1">\textbf</span>{C:} Somatic</div><div class="clear"></div>
<div class="linenb">590</div><div class="codeline">    potential $u_j$ of the pyramidal neuron responding to signals sent from the input neuron (color scheme as in B).}</div><div class="clear"></div>
<div class="linenb">591</div><div class="codeline">  <span class="keyword1">\label</span>{fig-comparison-le}</div><div class="clear"></div>
<div class="linenb">592</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">593</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">594</div><div class="codeline">When employing the default parametrization given by <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Harder, Raider, Hider, Aider] (36617) [lt:en:MORFOLOGIK_RULE_EN_US]">Haider</span> <span class="highlight-sh" title="Use a backslash, comma, or tilde after the period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:010]">et al. </span>(Table \ref{tab-params}), $\tau_{eff}$ is</div><div class="clear"></div>
<div class="linenb">595</div><div class="codeline">slightly lower than reported pyramidal neuron time constants \citep{McCormick1985} at approximately $5.26ms$. When</div><div class="clear"></div>
<div class="linenb">596</div><div class="codeline">presynaptic neurons employ prospective dynamics, postsynaptic neurons approach their steady state much more quickly, as</div><div class="clear"></div>
<div class="linenb">597</div><div class="codeline">depicted in Fig. \ref{fig-comparison-le}. In intuitive terms, prospective activation is more strongly dependent on the</div><div class="clear"></div>
<div class="linenb">598</div><div class="codeline">derivative membrane potential compared to the instantaneous activation. This results in drastic changes in activation</div><div class="clear"></div>
<div class="linenb">599</div><div class="codeline">in response to changes in the somatic membrane potential. While this can lead to an overshoot of postsynaptic activity,</div><div class="clear"></div>
<div class="linenb">600</div><div class="codeline">under careful parametrization it strongly decreases response time.</div><div class="clear"></div>
<div class="linenb">601</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">602</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">603</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[h!]</div><div class="clear"></div>
<div class="linenb">604</div><div class="codeline">  \centering</div><div class="clear"></div>
<div class="linenb">605</div><div class="codeline">  <span class="keyword1">\includegraphics</span>[width=0.9\textwidth]{fig_le_dendritic_errors}</div><div class="clear"></div>
<div class="linenb">606</div><div class="codeline">  <span class="keyword1">\caption</span>[Effects of LE dynamics on dendritic error]{Effects of LE dynamics on the dendritic error terms from Equations</div><div class="clear"></div>
<div class="linenb">607</div><div class="codeline">    \ref{eq-delta_w_up}-\ref{eq-delta_w_down}. Depicted are error terms for individual spiking neurons in a network with</div><div class="clear"></div>
<div class="linenb">608</div><div class="codeline">    one hidden layer (N=3). The network was fully trained on the Bars dataset (cf. Section \ref{sec-le-tpres}), so</div><div class="clear"></div>
<div class="linenb">609</div><div class="codeline">    errors should ideally relax to zero. Note, that this does not happen here due to the fluctuations inherent to the</div><div class="clear"></div>
<div class="linenb">610</div><div class="codeline">    spiking network variant which was employed. In the original dendritic error network (orange), dendritic errors</div><div class="clear"></div>
<div class="linenb">611</div><div class="codeline">    exhibit longer and more intense deviations, while errors in an identical LE network (blue) relax much sooner.</div><div class="clear"></div>
<div class="linenb">612</div><div class="codeline">    <span class="keyword1">\textbf</span>{A:} Basal dendritic error for a pyramidal neuron at the output layer. <span class="keyword1">\textbf</span>{B:} Dendritic error for a</div><div class="clear"></div>
<div class="linenb">613</div><div class="codeline">    hidden layer Interneuron. <span class="keyword1">\textbf</span>{C:} Proximal apical error for a hidden layer Pyramidal neuron. <span class="keyword1">\textbf</span>{D:} Distal</div><div class="clear"></div>
<div class="linenb">614</div><div class="codeline">    apical error for the same pyramidal neuron. Note that this error term does not converge to zero for either</div><div class="clear"></div>
<div class="linenb">615</div><div class="codeline">    network, an issue that will be discussed in Section \ref{sec-feedback-plast}.}</div><div class="clear"></div>
<div class="linenb">616</div><div class="codeline">  <span class="keyword1">\label</span>{fig-error-comp-le}</div><div class="clear"></div>
<div class="linenb">617</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">618</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">619</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">620</div><div class="codeline">When employing prospective dynamics in the dendritic error networks, local error terms of pyramidal- and interneurons</div><div class="clear"></div>
<div class="linenb">621</div><div class="codeline">relax much faster, as shown in Fig. \ref{fig-error-comp-le}. These results highlight the superiority of LE for</div><div class="clear"></div>
<div class="linenb">622</div><div class="codeline">learning in this network, as the relaxation period is almost instantaneous. In contrast, the error terms in the original</div><div class="clear"></div>
<div class="linenb">623</div><div class="codeline">dendritic error network drive random synaptic plasticity even when the network is fully trained on a given dataset and</div><div class="clear"></div>
<div class="linenb">624</div><div class="codeline">is able to make accurate predictions. Thus, both the issue of redundant weight changes, <span class="highlight" title="Probable usage error. Use 'and' after 'both'.. Suggestions: [and] (37803) [lt:en:BOTH_AS_WELL_AS]">as well as</span> the concern over</div><div class="clear"></div>
<div class="linenb">625</div><div class="codeline">response and learning speed can be solved by LE. The authors furthermore show, that learning with this mechanism is</div><div class="clear"></div>
<div class="linenb">626</div><div class="codeline">indifferent to presentation times or effective time constant for rate neurons.</div><div class="clear"></div>
<div class="linenb">627</div><div class="codeline">In addition to using the prospective somatic potential for the neuronal transfer function, it is also used in the</div><div class="clear"></div>
<div class="linenb">628</div><div class="codeline">plasticity rule of LE neurons. The <span class="highlight-spelling" title="Possible spelling mistake found. (38175) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity is therefore updated to compute dendritic error from</div><div class="clear"></div>
<div class="linenb">629</div><div class="codeline">prospective somatic activations and a non-prospective dendritic potential <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [PW, cw, MW, CW, EW, KW, NW, SW, TW, W, WW, YW, aw, kW, kw, ow, w, DW, FW, GW, HW, UW, VW, mW] (38328) [lt:en:MORFOLOGIK_RULE_EN_US]">$\dot{w}</span>_{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [IJ, in, is, I, it, if, ID, IQ, id, PJ, RJ, IH, DJ, IP, AIJ, AJ, CIJ, CJ, FIJ, FJ, GJ, IA, IE, IF, IJF, IJM, IL, IN, IPJ, IR, IS, IT, IV, IZ, Ia, In, Io, Ir, It, J, KJ, MJ, NJ, OJ, PIJ, SJ, TJ, VJ, i, ii, iv, ix, j, pj, BJ, IB, IC, ICJ, IG, IM, IU, JJ, kJ] (38331) [lt:en:MORFOLOGIK_RULE_EN_US]">ij}</span>= \eta \ <span class="highlight" title="Don't put a space after the opening parenthesis.. Suggestions: [(] (38342) [lt:en:COMMA_PARENTHESIS_WHITESPACE]">(</span></div><div class="clear"></div>
<div class="linenb">630</div><div class="codeline">  \phi(\breve{u}_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [idiom, Epsom, IOM, bosom, besom, IEOM, ISAM, ISCM, ISM, ISO, ISOC, ism, ISOs, ITSM] (38353) [lt:en:MORFOLOGIK_RULE_EN_US]">i^{som}</span>) - \phi(\hat{v}_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [ideas, Abbas, IAS, ICBMs, Iqbal, ibis, IRAs, Incas, tubas, iotas, BAS, IBA, IBAN, IFAS, IGAS, INBS, IRAS, ISAS, Iowas, IBANs, IBS, IPAs, ISBNs, ITAs, IaaS, babas] (38369) [lt:en:MORFOLOGIK_RULE_EN_US]">i^{bas}</span>)<span class="highlight" title="Don't put a space before the closing parenthesis.. Suggestions: [)] (38375) [lt:en:COMMA_PARENTHESIS_WHITESPACE]"> )</span> \ \phi(\breve{u}_<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Jason, Epsom, bosom, besom, JSON, JOSM] (38387) [lt:en:MORFOLOGIK_RULE_EN_US]">j^{som}</span>)<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [TO, AT, IT, ITS, ST, ETC, MTV, FT, LTD, BTW, CTV, LT, PT, TT, UTC, ATE, ATC, ATM, ATV, BT, DT, ITF, ITU, MTA, MTR, RTL, RTS, TTC, TV, WTA, WTO, TA, CTA, CTC, DTM, DTS, ETA, FTA, FTC, GTA, GTE, ITC, ITT, KTM, MTS, OTC, PTC, TTL, WTC, CTF, CTI, CTM, CTO, CTP, DTC, DTI, MTB, MTC, MTU, NTP, PTH, PTO, PTT, PTV, RTA, STP, FTP, TS, CTH, DTA, DTP, ITE, KTP, MTO, MTT, RTT, RTU, TTA, NTH, CTT, MTG, OTB, TTF, RT, STY, GT, ETD, YTD, ATP, ITV, LTE, TD, QTY, HTS, 0TH, 1TH, 2TH, 3TH, 4TH, 5TH, 6TH, 7TH, 8TH, 9TH, AT3, ATA, ATB, ATD, ATF, ATG, ATH, ATI, ATK, ATL, ATN, ATO, ATQ, ATR, ATS, ATT, ATU, ATW, ATX, ATY, ATZ, BTA, BTB, BTC, BTF, BTG, BTH, BTI, BTK, BTL, BTM, BTN, BTO, BTP, BTQ, BTR, BTS, BTT, BTU, BTV, BTX, CT, CTB, CTE, CTK, CTR, DTU, DTV, DTW, ET, ETB, ETF, ETL, ETM, ETP, ETS, ETT, ETV, ETZ, FTF, FTG, FTI, FTM, FTO, FTQ, FTV, FTW, GT3, GT4, GT6, GTB, GTG, GTI, GTK, GTM, GTO, GTP, GTQ, GTR, GTS, HT, HTA, HTB, HTF, HTL, HTO, HTP, HTV, ITA, ITB, ITK, ITL, ITP, ITO, JT, JTA, JTC, JTL, KT, KTD, KTS, LTA, LTF, LTI, LTL, LTN, LTR, LTT, LTU, MT, MTD, MTF, MTL, MTN, MTP, MTQ, MTW, MTZ, NT, NTC, NTD, NTL, NTR, NTV, NTW, OT, OTG, OTN, OTO, OTP, OTU, PTA, PTB, PTE, PTF, PTG, PTI, PTM, PTP, PTR, PTU, PTY, PTZ, QT8, RTB, RTC, RTD, RTE, RTF, RTG, RTM, RTN, RTO, RTP, RTR, RTV, RTW, STA, STB, STC, STD, STE, STF, STG, STI, STL, STM, STN, STO, STR, STS, STT, STV, STX, STU, T, TB, TC, TF, TG, TH, TI, TJ, TK, TL, TM, TN, TP, TR, TTD, TTI, TTO, TTP, TTR, TTS, TTT, TTU, TTX, TTY, TU, TW, TX, TY, TZ, TE, UT, UT1, UTA, UTF, UTG, UTL, UTP, UTV, UTE, VT, VTA, VTC, VTP, VTT, WTF, WTH, WTN, WTP, XTC, XTP, YT, YTL, YTO, YTV, YTZ, ZTI, CTN, QT, QTS, WT, ÉTP, CTG, CTS, CTU, CTs, DTD, DTT, ETH, ETR, FTD, FTE, FTX, GTC, GTD, GTX, HTC, ITN, JTF, LTC, LTG, LTS, LTV, MTX, NTI, NTS, NTT, OTA, RTI, RTX, RTs, STW, TQ, TTG, TTs, UTI, UTM, ZTE, PTS] (38393) [lt:en:MORFOLOGIK_RULE_EN_US]">^T$</span>. Much like for the transfer function,</div><div class="clear"></div>
<div class="linenb">631</div><div class="codeline">this change serves to increase the responsiveness of the network to input changes.</div><div class="clear"></div>
<div class="linenb">632</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">633</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">section</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Implementation, Implementations] (38519) [lt:en:MORFOLOGIK_RULE_EN_US]">Implementational</span> details}</div><div class="clear"></div>
<div class="linenb">634</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">635</div><div class="codeline">Building on the neuron and plasticity model from \citep{Stapmanns2021}, a replicate model of the pyramidal neuron with</div><div class="clear"></div>
<div class="linenb">636</div><div class="codeline">spike-based communication was developed in NEST. The existing neuron model was expanded to three compartments, and</div><div class="clear"></div>
<div class="linenb">637</div><div class="codeline">storage and readout of dendritic errors were updated  to allow for compartment-specific plasticity rules. Interneurons</div><div class="clear"></div>
<div class="linenb">638</div><div class="codeline">were chosen to be modeled as pyramidal neurons with slightly updated parameters and apical conductance $g^{api}=0$.</div><div class="clear"></div>
<div class="linenb">639</div><div class="codeline">Since membrane dynamics of both neurons follow the same principles and additional compartments have minor impact on</div><div class="clear"></div>
<div class="linenb">640</div><div class="codeline">performance, this was deemed sufficient.</div><div class="clear"></div>
<div class="linenb">641</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">642</div><div class="codeline">After facing some setbacks when attempting to train the first spiking variant of the network, the decision was made to</div><div class="clear"></div>
<div class="linenb">643</div><div class="codeline">also implement a rate-based variant of the neuron in NEST.  While the additional effort required for another</div><div class="clear"></div>
<div class="linenb">644</div><div class="codeline">implementation might be questionable, this model turned out to be <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [indispensable] (39438) [lt:en:MORFOLOGIK_RULE_EN_US]">indispensible</span>. It enabled the identification of both</div><div class="clear"></div>
<div class="linenb">645</div><div class="codeline">errors in the model, <span class="highlight" title="Probable usage error. Use 'and' after 'both'.. Suggestions: [and] (39512) [lt:en:BOTH_AS_WELL_AS]">as well as</span> training mechanisms and parameters that required changes to enable spike-compatible</div><div class="clear"></div>
<div class="linenb">646</div><div class="codeline">learning. The rate version in NEST additionally served to distinguish discrepancies that are due to the novel simulation</div><div class="clear"></div>
<div class="linenb">647</div><div class="codeline">backend from those that were introduced by the spike-based communication scheme.</div><div class="clear"></div>
<div class="linenb">648</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">649</div><div class="codeline">Following NEST convention, the spiking and rate-based neuron models were named \texttt{pp\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (39894) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [cold, bond, cone, cord, pond, Bond, con, fond, cod, Cong, coed, condo, CND, coned, cont, conk, conj, COD, CON, Cod, Conn, MOND, OND, cons, contd, cony, COPD, Conda] (39905) [lt:en:MORFOLOGIK_RULE_EN_US]">cond</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (39912) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>allowbreak</div><div class="clear"></div>
<div class="linenb">650</div><div class="codeline">  exp\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (39931) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [MC, me, my, BC, Mr, cm, mm, PC, MA, Mac, JC, MSC, MV, Mk, QC, ma, ml, BMC, DMC, MBC, MCG, MCP, MDC, MF, MHC, MX, WC, mic, ms, FMC, HMC, IMC, MCB, MCD, MCL, MTC, MVC, MWC, PMC, RMC, MCE, MCF, MCO, MCR, MCT, MCU, mp, AMC, MCA, MD, MW, mg, AC, Ac, C, CC, CMC, DC, EC, EMC, FC, KC, LC, LMC, M, MB, MCH, MCI, MCJ, MCM, MCS, MCZ, ME, MEC, MFC, MG, MH, MI, MJ, MJC, MK, ML, MLC, MM, MMC, MN, MO, MOC, MP, MPC, MR, MRC, MS, MT, MU, MUC, MY, MZ, Mb, Md, Me, Mg, Mn, Mo, Ms, Mt, NC, OMC, RC, SC, SMC, Sc, TC, TMC, Tc, UC, VC, VMC, ZC, ac, c, cc, kc, m, mac, mi, mo, mu, °C, GC, GMC, HC, IC, MCC, MCK, MCQ, MCX, MCs, MNC, MSc, NMC, OC, UMC, YC, bc, mCi, mL, mW, mcg, pc] (39942) [lt:en:MORFOLOGIK_RULE_EN_US]">mc</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (39947) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [despite, respite] (39958) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr}\footnote{Despite</span> being somewhat cryptic, the name does actually make sense, as it</div><div class="clear"></div>
<div class="linenb">651</div><div class="codeline">  describes some key features of the model: It is a <span class="keyword1">\textbf</span>{point process} for <span class="keyword1">\textbf</span>{cond}uctance based synapses and has</div><div class="clear"></div>
<div class="linenb">652</div><div class="codeline">  an <span class="keyword1">\textbf</span>{exp}onentially decaying membrane in <span class="keyword1">\textbf</span>{multiple compartments}.} <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [And] (40202) [lt:en:UPPERCASE_SENTENCE_START]">and</span> \texttt{rate\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40213) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>allowbreak</div><div class="clear"></div>
<div class="linenb">653</div><div class="codeline">  neuron\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40235) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [per, PR, par, pyre, PMR, PVR, pry, ppr, AYR, BYR, MYR, PAR, PBR, PCR, PER, PFR, PHR, PLR, PNR, POR, PPR, PQR, PRR, PSR, PTR, PUR, PWR, PYF, PYG, PYT, Pr, Pym, RYR, SYR, pr, pyx, yr, p yr, PDR] (40246) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr}</span> respectively. Furthermore, the \texttt{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [per, PR, par, pyre, PMR, PVR, pry, ppr, AYR, BYR, MYR, PAR, PBR, PCR, PER, PFR, PHR, PLR, PNR, POR, PPR, PQR, PRR, PSR, PTR, PUR, PWR, PYF, PYG, PYT, Pr, Pym, RYR, SYR, pr, pyx, yr, p yr, PDR] (40281) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40287) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> synapse} class was defined for spike</div><div class="clear"></div>
<div class="linenb">654</div><div class="codeline">events, and implements the event-based variant of the <span class="highlight-spelling" title="Possible spelling mistake found. (40388) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-Senn</span> plasticity described in Section</div><div class="clear"></div>
<div class="linenb">655</div><div class="codeline">\ref{sec-event-urb}. The \texttt{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [per, PR, par, pyre, PMR, PVR, pry, ppr, AYR, BYR, MYR, PAR, PBR, PCR, PER, PFR, PHR, PLR, PNR, POR, PPR, PQR, PRR, PSR, PTR, PUR, PWR, PYF, PYG, PYT, Pr, Pym, RYR, SYR, pr, pyx, yr, p yr, PDR] (40442) [lt:en:MORFOLOGIK_RULE_EN_US]">pyr</span>\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40448) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> synapse\_\<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [allow break] (40469) [lt:en:MORFOLOGIK_RULE_EN_US]">allowbreak</span> rate} model on the other hand transmits rate</div><div class="clear"></div>
<div class="linenb">656</div><div class="codeline">events and updates its weight according to the original plasticity rule.</div><div class="clear"></div>
<div class="linenb">657</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">658</div><div class="codeline">Simulations were managed using the python API \texttt{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Priest, Finest, Nest, Ernest, Honest, Pest, Pines, Panes, Purest, Pyres, Palest, Sanest, Pones, Openest, Piniest, Pinkest, Puniest, Punkest, Pyxes, Yest] (40644) [lt:en:MORFOLOGIK_RULE_EN_US]">PyNEST}</span> \citep{Eppler2009}, which is much more convenient than the</div><div class="clear"></div>
<div class="linenb">659</div><div class="codeline">SLI interface that lies at the core of NEST. An additional advantage of using this language is, that the LE network  is</div><div class="clear"></div>
<div class="linenb">660</div><div class="codeline">also implemented in python. Thus, by including a slightly modified version of that code in my project, it was possible</div><div class="clear"></div>
<div class="linenb">661</div><div class="codeline">to unify all three variants in a single network class and accompanying interface. This allowed for exact alignment of</div><div class="clear"></div>
<div class="linenb">662</div><div class="codeline">network stimulation and readout and enabled in-depth comparative analyses. In <span class="highlight" title="If the text is a generality, 'of the' is not necessary.. Suggestions: [some] (41130) [lt:en:SOME_OF_THE]">some of the</span> upcoming Results, three</div><div class="clear"></div>
<div class="linenb">663</div><div class="codeline">variants of the same network architecture will therefore be compared; The modified python implementation from</div><div class="clear"></div>
<div class="linenb">664</div><div class="codeline">\citep{Haider2021} is termed \texttt{NumPy} based on the framework that is used to compute neuron dynamics and synaptic</div><div class="clear"></div>
<div class="linenb">665</div><div class="codeline">plasticity through matrix multiplication. The two NEST variants will be referred to as <span class="keyword1">\textit</span>{NEST spiking} and</div><div class="clear"></div>
<div class="linenb">666</div><div class="codeline"><span class="keyword1">\textit</span>{NEST rate}.</div><div class="clear"></div>
<div class="linenb">667</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">668</div><div class="codeline"><span class="keyword1">\subsection</span>{Neuron model Adaptations}</div><div class="clear"></div>
<div class="linenb">669</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">670</div><div class="codeline">The neuron model from \citep{Stapmanns2021} was modified in several ways in order to match the pyramidal neuron</div><div class="clear"></div>
<div class="linenb">671</div><div class="codeline">implementation more closely. Both the inclusion of nonzero reversal potentials and the flow of currents from the <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [SOMA, some, coma, soda, sofa, OMA, Sosa, SMA, SOA, SOPA, MoMA, Roma, soca, stoma, homey, samey, come, home, same, so, sold, son, sea, soon, woman, Roman, com, semi, soil, solo, song, sons, Rome, Tom, bomb, soap, soft, solar, sole, sort, soul, sum, tomb, Iowa, MA, Nova, Sam, dome, Emma, Rosa, Sony, foam, saga, slam, soup, sour, spa, sums, yoga, Alma, Goa, Lima, NOAA, Omar, Rama, SAM, SMS, SOS, Samoa, Sara, Sega, Sofia, Sol, Somali, aroma, coda, comb, comma, dogma, ma, mom, nova, roam, roman, samba, scam, seam, ska, sonar, sore, soy, spam, sumo, swam, womb, CMA, Como, Dora, JMA, LMA, Lola, Mona, Nora, Norma, Oman, SDA, SLA, SOE, SSA, Selma, Shea, Sims, Soho, Somme, Sonia, Soto, coca, cola, comp, loam, pomp, sham, sigma, smog, sock, sod, soot, sous, sow, sown, sump, BMA, Cora, DOA, Doha, Goya, IMA, IOM, Irma, Kama, Nome, PMA, RMA, SAA, SBA, SGA, SGML, SME, SMG, SNA, SOP, SRA, SRM, SSM, Samar, Siva, Somoza, Sonja, Sonya, Sousa, Suva, VMA, VOA, Yuma, Zola, boa, lama, mama, nomad, ova, romp, soak, soar, sows, stomp, toga, tome, toms, Dona, FMA, Lora, Nola, Nona, Ora, Qom, SDM, SGM, SMF, SMI, SNMP, SPCA, Sana, Spam, Yoda, hora, iota, kola, moms, puma, rota, sobs, sodas, sofas, sols, sumac, Asoka, COLA, CSMA, Elma, Erma, Lome, SOB, bola, comas, dona, skua, sob, soc, sop, sou, Asama, FNMA, dopa, korma, sods, sorta, sot, Zomba, sops, ROM, SOs, oms, sots, ACMA, ALMA, AMA, AOA, AOM, BOM, CCMA, COM, COMP, Com, DOM, DOPA, EMA, FOA, FOIA, FOTA, FSMA, Gama, HOM, Houma, IFMA, INMA, ISIMA, KMA, LAMA, MMA, MOA, MOM, NMA, NOA, NUMA, OEA, OHA, OLA, OM, OMB, OMC, OMG, OMI, OMM, OMN, OMO, OMP, OMR, OMS, OMT, OOM, OPA, ORMA, OSA, OUA, Ola, POA, POM, RAMA, REMA, RIMA, ROA, ROME, Rom, S3A, SA, SAMOA, SAMU, SARA, SATA, SBM, SCA, SCM, SEA, SEM, SEPA, SESA, SFMC, SGMAP, SGMP, SHA, SHM, SIA, SIDA, SIM, SIMC, SIMCA, SIMM, SISA, SJA, SJM, SKA, SKEMA, SM, SMAC, SMB, SMC, SMH, SMK, SML, SMM, SMMI, SMN, SMR, SMT, SO, SOAD, SOAP, SOC, SOD, SOHO, SOI, SON, SOPK, SOTU, SOUD, SOX, SPA, SPM, SRBA, STA, STCA, STM, SUA, SUMC, SVA, SVGA, SVM, SXM, SYMPA, Siam, Sm, Soc, Son, Sta, TMA, TOM, TSMA, Tomas, UMA, WOSA, Zosma, boga, comm, homeys, homo, om, pom, poms, sim, sims, sol, soph, souk, tom, AOPA, CDMA, CGMA, COA, DMA, EOM, EOMs, FEMA, FOMC, FOMO, GMA, GSMA, Gomal, HOA, LOA, LZMA, Lomé, MDMA, OA, OTA, OVA, OWA, Osama, Puma, ROMs, SAML, SCMP, SCRA, SFA, SFM, SIMs, SMP, SMU, SOCS, SOEs, SOPs, Sami, Shia, Simba, Skoda, SoC, SoCs, Sofía, Sonoma, Souza, TOML, Tomás, UOM, Zora, hola, momma, sRNA, semé, simp, socs, stomas, Škoda] (41721) [lt:en:MORFOLOGIK_RULE_EN_US]">soma</span> to</div><div class="clear"></div>
<div class="linenb">672</div><div class="codeline">the dendrites were omitted in my model. Furthermore, the present network requires synapses to be able</div><div class="clear"></div>
<div class="linenb">673</div><div class="codeline">change the sign of their weight at runtime, which is not permitted in the original synapse model. For this reason, the</div><div class="clear"></div>
<div class="linenb">674</div><div class="codeline">strict separation of excitatory and inhibitory synapses had to be removed from the synapse model. In order to compare</div><div class="clear"></div>
<div class="linenb">675</div><div class="codeline">the different implementations exactly, the ODE solver with variable <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [step size] (42136) [lt:en:MORFOLOGIK_RULE_EN_US]">stepsize</span> was replaced with Euler</div><div class="clear"></div>
<div class="linenb">676</div><div class="codeline"><span class="highlight-spelling" title="Possible spelling mistake found. (42169) [lt:en:MORFOLOGIK_RULE_EN_US]">integrations\footnote{This</span> change initially served debugging purposes, but turned out to have no negative effect on</div><div class="clear"></div>
<div class="linenb">677</div><div class="codeline">  performance and was therefore kept} with step size $\Delta t$. For the spiking neuron model, dendritic compartments are</div><div class="clear"></div>
<div class="linenb">678</div><div class="codeline">modeled with leaky membrane dynamics in contrast to the rate variant. The choice of dendritic leakage conductance</div><div class="clear"></div>
<div class="linenb">679</div><div class="codeline">$g_l^{dend}=\Delta t=0.1$ is motivated in Section \ref{sec-gl-dend}.</div><div class="clear"></div>
<div class="linenb">680</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">681</div><div class="codeline">A major issue of the spiking network is the fact that under the default parametrization, spikes are too infrequent for</div><div class="clear"></div>
<div class="linenb">682</div><div class="codeline">the network to accurately compute the dendritic error terms. Initial experiments showed that the network is rather</div><div class="clear"></div>
<div class="linenb">683</div><div class="codeline">sensitive to changes in parametrization, which meant that it was desirable to change as few existing parameters as</div><div class="clear"></div>
<div class="linenb">684</div><div class="codeline">possible. Therefore, a novel parameter $\psi$ was introduced. In a spiking neuron $i$, the probability of eliciting a</div><div class="clear"></div>
<div class="linenb">685</div><div class="codeline">spike is linearly increased by this factor ($r_i = \psi \phi(u_i)$). Likewise, all synaptic weights $W$ in a spiking</div><div class="clear"></div>
<div class="linenb">686</div><div class="codeline">network are attenuated by the same factor ($W \leftarrow \frac{W}{\psi}$). These changes cancel each other out, as an</div><div class="clear"></div>
<div class="linenb">687</div><div class="codeline">increased value for $\psi$ elicits no change in absolute compartment voltages of a network. Instead, it serves to</div><div class="clear"></div>
<div class="linenb">688</div><div class="codeline">stabilize these voltages over time, which drastically improves learning performance. One mechanism in which this</div><div class="clear"></div>
<div class="linenb">689</div><div class="codeline">parameter also needs to be considered is the plasticity rule. Weight changes are affected by $\psi$ in three distinct</div><div class="clear"></div>
<div class="linenb">690</div><div class="codeline">ways: Since $\psi$ <span class="highlight" title="The verb form seems incorrect.. Suggestions: [is linearly scaling, it linearly scales, linearly scales] (43524) [lt:en:IS_VBZ]">is linearly scales</span> the activation (spiking or hypothetical), it also increases dendritic error</div><div class="clear"></div>
<div class="linenb">691</div><div class="codeline">linearly, as it does the presynaptic activation. Additionally, since the frequency of weight changes is determined by</div><div class="clear"></div>
<div class="linenb">692</div><div class="codeline">the presynaptic spike rate, $\psi$ increases the strength of plasticity three times. As these influences are</div><div class="clear"></div>
<div class="linenb">693</div><div class="codeline">multiplicative, learning rates are attenuated by $\eta \leftarrow \frac{\eta}{\psi^3}$. The exception to this are the</div><div class="clear"></div>
<div class="linenb">694</div><div class="codeline">weights from interneurons to pyramidal neurons, as these do not depend on dendritic predictions, but on absolute</div><div class="clear"></div>
<div class="linenb">695</div><div class="codeline">dendritic voltage. Hence, in this case $\eta^{pi} \leftarrow\frac{\eta^{pi}}{\psi^2}$. On close investigation of the</div><div class="clear"></div>
<div class="linenb">696</div><div class="codeline">spiking neuron model, one can observe that for $\psi \rightarrow \infty$, it approximates the rate-based implementation</div><div class="clear"></div>
<div class="linenb">697</div><div class="codeline">exactly at the steady state. Unsurprisingly therefore, increasing $\psi$ caused the spiking network to learn</div><div class="clear"></div>
<div class="linenb">698</div><div class="codeline">successfully with fewer samples and to a lower test loss. Yet, the argument against increasing $\psi$ is twofold:</div><div class="clear"></div>
<div class="linenb">699</div><div class="codeline">Initial experiments showed that only for $\psi &lt; 0.1$ did pyramidal and interneurons exhibit spike frequencies in</div><div class="clear"></div>
<div class="linenb">700</div><div class="codeline">biologically plausible range of less than $55Hz$ \citep{Kawaguchi2001,Eyal2018}. Additionally, each transmitted</div><div class="clear"></div>
<div class="linenb">701</div><div class="codeline">\texttt{<span class="highlight-spelling" title="Possible spelling mistake found. (44603) [lt:en:MORFOLOGIK_RULE_EN_US]">SpikeEvent}</span> is computationally costly, which increases training time (cf. Fig. \ref{fig-benchmark-threads-psi})</div><div class="clear"></div>
<div class="linenb">702</div><div class="codeline">and therefore further makes high spike frequencies undesirable. As a middle ground, $\psi = 100$ proved useful during</div><div class="clear"></div>
<div class="linenb">703</div><div class="codeline">initial tests and will be assumed the default from here on out. Note that this parametrization was chosen primarily with</div><div class="clear"></div>
<div class="linenb">704</div><div class="codeline">efficiency in mind, and is far removed from biologically plausible spike frequencies. With these adaptations, the</div><div class="clear"></div>
<div class="linenb">705</div><div class="codeline">network was able to perform supervised learning with spiking neurons, as will be discussed in the upcoming sections.</div><div class="clear"></div>
<div class="linenb">706</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">707</div><div class="codeline"><span class="keyword1">\section</span>{Error metrics and nomenclature}</div><div class="clear"></div>
<div class="linenb">708</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">709</div><div class="codeline">In this thesis, the word 'error' is used frequently, which might understandably lead to confusion. While stylistically</div><div class="clear"></div>
<div class="linenb">710</div><div class="codeline">questionable, this choice was made deliberately to conform to the main underlying works</div><div class="clear"></div>
<div class="linenb">711</div><div class="codeline">\citep{urbanczik2014learning,sacramento2018dendritic,whittington2019theories,Haider2021}. This section will provide a</div><div class="clear"></div>
<div class="linenb">712</div><div class="codeline">brief disambiguation. Firstly, there are four error metrics describing the network's deviation from the self-predicting</div><div class="clear"></div>
<div class="linenb">713</div><div class="codeline">state: <span class="keyword1">\textit</span>{Feedforward weight error (FF error), Feedback weight error (FB error), Apical error and Interneuron</div><div class="clear"></div>
<div class="linenb">714</div><div class="codeline">  error}. These were introduced in Section <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [Observation] (45685) [lt:en:MORFOLOGIK_RULE_EN_US]">\ref{sec-selfpred}\footnote{Observation</span> of the network dynamics reveals that pairs of</div><div class="clear"></div>
<div class="linenb">715</div><div class="codeline">  them are closely related: FF error drives interneuron error, and FB error drives apical</div><div class="clear"></div>
<div class="linenb">716</div><div class="codeline">  error as soon as interneuron error is minimal. An analytical upgrade to this model might include a way for unifying</div><div class="clear"></div>
<div class="linenb">717</div><div class="codeline">  these pairs of metrics<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (45976) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Furthermore, three terms require elaboration:\newline</div><div class="clear"></div>
<div class="linenb">718</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">719</div><div class="codeline"><span class="keyword1">\textbf</span>{Dendritic error}: Any value which drives the Dendritic plasticity rules. Classically, this refers to a failure</div><div class="clear"></div>
<div class="linenb">720</div><div class="codeline">of a dendrite to predict somatic activity \citep{urbanczik2014learning}. In this context, due to the changes to the</div><div class="clear"></div>
<div class="linenb">721</div><div class="codeline">plasticity rule, it may also refer to absolute voltage of pyramidal neuron apical compartments (<span class="highlight-sh" title="Use a backslash or comma after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>Apical error). \newline</div><div class="clear"></div>
<div class="linenb">722</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">723</div><div class="codeline"><span class="keyword1">\textbf</span>{Train error}: Failure rate of a network to correctly classify inputs during testing. In the upcoming</div><div class="clear"></div>
<div class="linenb">724</div><div class="codeline">simulations, all targets are encoded with one-hot vectors. Thus, accuracy is defined as:</div><div class="clear"></div>
<div class="linenb">725</div><div class="codeline"><span class="keyword2">\begin{align*}</span></div><div class="clear"></div>
<div class="linenb">726</div><div class="codeline">  accuracy &amp; = \frac{1}{N} \sum_{i=1}^N \  \delta \left(argmax(y^{target}_i),\ argmax(y^{pred}_i) \right)</div><div class="clear"></div>
<div class="linenb">727</div><div class="codeline"><span class="keyword2">\end{align*}</span></div><div class="clear"></div>
<div class="linenb">728</div><div class="codeline">for a test run over $N$ samples, with $\delta$ being the Kronecker delta function. Train error is defined as inverse</div><div class="clear"></div>
<div class="linenb">729</div><div class="codeline">accuracy.\newline</div><div class="clear"></div>
<div class="linenb">730</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">731</div><div class="codeline"><span class="keyword1">\textbf</span>{Loss}: Unless specified otherwise, train- and test loss <span class="highlight" title="Possible typo: you repeated a word. Suggestions: [are] (46731) [lt:en:ENGLISH_WORD_REPEAT_RULE]">are are</span> computed through MSE between</div><div class="clear"></div>
<div class="linenb">732</div><div class="codeline">predicted and target output:</div><div class="clear"></div>
<div class="linenb">733</div><div class="codeline"><span class="keyword2">\begin{align*}</span></div><div class="clear"></div>
<div class="linenb">734</div><div class="codeline">  MSE &amp; = \frac{1}{N} \sum_{i=1}^N \left( y^{target}_i-y^{pred}_i \right)^2</div><div class="clear"></div>
<div class="linenb">735</div><div class="codeline"><span class="keyword2">\end{align*}</span></div><div class="clear"></div>
<div class="linenb">736</div><div class="codeline">Due to the network's relaxation period, $y^{pred}$ can not be accurately computed <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [instantaneously Sacramento] (46870) [lt:en:MORFOLOGIK_RULE_EN_US]">instantaneously\footnote{Sacramento</span> et</div><div class="clear"></div>
<div class="linenb">737</div><div class="codeline">  al. actually do exactly this. They compute $y^{pred}$ without neuron dynamics, only from the input, activation function</div><div class="clear"></div>
<div class="linenb">738</div><div class="codeline">  $\phi$, and feedforward weights. This approach makes the assumption that the network is permanently in a perfect</div><div class="clear"></div>
<div class="linenb">739</div><div class="codeline">  self-predicting state, in which feedback connections do not change the output. Particularly for the spiking variant,</div><div class="clear"></div>
<div class="linenb">740</div><div class="codeline">  this assumption is likely erroneous, leading to artificially inflated performance. Hence, all tests are performed by</div><div class="clear"></div>
<div class="linenb">741</div><div class="codeline">  fully simulating networks with disabled plasticity<span class="highlight" title="Two consecutive dots. Suggestions: [., …] (47412) [lt:en:DOUBLE_PUNCTUATION]">.}.</span> Instead, the network needs to be presented with the stimulus for a</div><div class="clear"></div>
<div class="linenb">742</div><div class="codeline">given time $t_{pres}$. Particularly for the SNN, as well as networks injected with noise, output fluctuates. <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Therefore] (47582) [lt:en:UPPERCASE_SENTENCE_START]">therefore</span>,</div><div class="clear"></div>
<div class="linenb">743</div><div class="codeline">$y^{pred}$ is an average over recorded somatic potentials for each output neuron. This recording typically starts after</div><div class="clear"></div>
<div class="linenb">744</div><div class="codeline">roughly $70\%$ of $t_{pres}$.</div><div class="clear"></div>
</div>
<h2 class="filename">06_notes.tex</h2>

<p>Found 17 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;1</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 2 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span></span>{<span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Additional] (0) [lt:en:UPPERCASE_SENTENCE_START]">additional</span> notes and questions}</div><div class="clear"></div>
<div class="linenb">&nbsp;2</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;3</div><div class="codeline"><span class="highlight-sh" title="A section title should not be written in all caps. The LaTeX stylesheet takes care of rendering titles in caps if needed. [sh:003]"></span>\<span class="highlight-sh" title="This section is very short (about 121 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">section</span></span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [To-dos, DODOS, TO DOS, TO-DOS] (32) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>TODOS}</div><div class="clear"></div>
<div class="linenb">&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;5</div><div class="codeline"><span class="keyword2">\begin{itemize}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;6</div><div class="codeline">    <span class="keyword1">\item</span> Does the network still learn when neurons have a refractory period?</div><div class="clear"></div>
<div class="linenb">&nbsp;7</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Talk] (116) [lt:en:UPPERCASE_SENTENCE_START]">talk</span> about feedback plasticity</div><div class="clear"></div>
<div class="linenb">&nbsp;8</div><div class="codeline">    <span class="keyword1">\item</span> investigate <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [examine, exciting, etching, excise, exiting, exacting, excite, exiling, excising, excusing, exuding, exocrine] (163) [lt:en:MORFOLOGIK_RULE_EN_US]">exc-inh</span> split biologically</div><div class="clear"></div>
<div class="linenb">&nbsp;9</div><div class="codeline">    <span class="keyword1">\item</span> <span class="highlight-spelling" title="Possible spelling mistake found. (194) [lt:en:MORFOLOGIK_RULE_EN_US]">Urbanczik-senn</span> has little empirical background. <span class="highlight" title="This word is considered offensive. (242) [lt:en:PROFANITY]">shits</span> for the outlook</div><div class="clear"></div>
<div class="linenb">10</div><div class="codeline">    <span class="keyword1">\item</span> Spiking network <span class="highlight" title="Consider using 'yet' or 'as yet'.. Suggestions: [yet, as yet] (284) [lt:en:AS_OF_YET]">as of yet</span> can only process positive-valued input</div><div class="clear"></div>
<div class="linenb">11</div><div class="codeline">    <span class="keyword1">\item</span> test time can be reduced by introducing separate t\_pres</div><div class="clear"></div>
<div class="linenb">12</div><div class="codeline"><span class="keyword2">\end{itemize}</span></div><div class="clear"></div>
<div class="linenb">13</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">14</div><div class="codeline">squared error / variance of training output = explained variance</div><div class="clear"></div>
<div class="linenb">15</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">16</div><div class="codeline"><span class="keyword1">\subsection</span>{Interneurons and their jobs}</div><div class="clear"></div>
<div class="linenb">17</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">18</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">19</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">20</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">21</div><div class="codeline"><span class="highlight" title="This sentence does not start with an uppercase letter.. Suggestions: [Reciprocal] (494) [lt:en:UPPERCASE_SENTENCE_START]">reciprocal</span> inhibition of SST+ neurons. In agreement,</div><div class="clear"></div>
<div class="linenb">22</div><div class="codeline">recent experiments show that feedback input can gate</div><div class="clear"></div>
<div class="linenb">23</div><div class="codeline">plasticity of the feedforward synapses through VIP+</div><div class="clear"></div>
<div class="linenb">24</div><div class="codeline">neuron mediated disinhibition 176 and that <span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [pyramidal] (695) [lt:en:MORFOLOGIK_RULE_EN_US]">p</span>yrami</div><div class="clear"></div>
<div class="linenb">25</div><div class="codeline">dal neurons indeed recruit inhibitory populations to</div><div class="clear"></div>
<div class="linenb">26</div><div class="codeline">produce a predictive error177 \citep{Poirazi2020}</div><div class="clear"></div>
<div class="linenb">27</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">28</div><div class="codeline"><span class="highlight-sh" title="A section title should start with a capital letter. [sh:001]"></span>\<span class="highlight-sh" title="This section is very short (about 6 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsection</span>{<span class="highlight-spelling" title="Possible spelling mistake found.. Suggestions: [tires, Ypres, Pres, pres, tares] (790) [lt:en:MORFOLOGIK_RULE_EN_US]"></span>tpres}</div><div class="clear"></div>
<div class="linenb">29</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">30</div><div class="codeline">$t_{pres} 10 - 50 \tau$</div><div class="clear"></div>
</div>
<hr/>
Output produced by TeXtidote v0.8.2, &copy; 2018-2020 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/textidote">TeXtidote website</a> for more information.
</body>
</html>
